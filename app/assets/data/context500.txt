W03-0415==>P99-1016!=<context citStr="Caraballo (1999)" endWordPosition="593" position="3648" startWordPosition="592">. In Section 4, we show how correctly extracted relationships can be used as seed-cases to extract several more relationships, thus improving recall; this work shares some similarities with that of Caraballo (1999). In Section 5 we show that combining the techniques of Section 3 and Section 4 improves both precision and recall. Section 6 demonstrates that 1Another possible view is that hyponymy should only re</context>
W03-0415==>P99-1016!=<context citStr="Caraballo (1999)" endWordPosition="696" position="4278" startWordPosition="695">ingent relationships are an important part of world-knowledge (and are therefore worth learning), and because in practice we found the distinction difficult to enforce. Another definition is given by Caraballo (1999): ... a word A is said to be a hypernym of a word B if native speakers of English accept the sentence B is a (kind of) A.  linguistic tools such as lemmatization can be used to reliably put the ex</context>
W03-0415==>P99-1016!=<context citStr="Caraballo (1999)" endWordPosition="2852" position="17183" startWordPosition="2851">hat might be found in text are expressed overtly by the simple lexicosyntactic patterns used in Section 2, as was apparent in the results presented in that section. This problem has been addressed by Caraballo (1999), who describes a system that first builds an unlabelled hierarchy of noun clusters using agglomerative bottom-up clustering of vectors of noun coordination information. The leaves of this hierarchy (</context>
W03-0415==>P99-1016!=<context citStr="Caraballo, 1999" endWordPosition="5658" position="34437" startWordPosition="5657">is paper suggests many possibilities for future work. First of all, it would be interesting to apply LSA to a system for building an entire hypernym-labelled ontology in roughly the way described in (Caraballo, 1999), perhaps by using an LSA-weighted voting method to determine which hypernym would be used to label each node. We are considering how to extend our techniques to such a task. Also, systematic comparis</context>
W06-2404==>W03-0429!=<context citStr="Mayfield et al., 2003" endWordPosition="812" position="5645" startWordPosition="809">ning technique, which has been successfully applied to various natural language processing tasks including chunking tasks such as phrase chunking (Kudo and Matsumoto, 2001) and named entity chunking (Mayfield et al., 2003). In the preliminary experimental evaluation, we focus on 52 expressions that have balanced distribution of their usages in the newspaper text corpus and are among the most difficult ones in terms of </context>
C00-1068==>P99-1040!=<context citStr="Litman et al., 1999" endWordPosition="513" position="3471" startWordPosition="510">s (CMs) of every speech recognition output should be modeled as a criterion to control dialogue management. CMs have been calculated in previous works using transcripts and various knowledge sources (Litman et al., 1999) (Pao et al., 1998). For more flexible interaction, it is desirable that CMs are defined on each word rather than whole sentence, because the system can handle only unreliable portions of an utterance</context>
W06-3501==>J94-4002!=<context citStr="Lappin and Leass (1994)" endWordPosition="2538" position="15944" startWordPosition="2535">ocation A: Hes going to do a Thorpey after today. B: Why? What happened? A: [TV24] Won the year 7 freestyle, he did. Although the use of it was frequent in Table 1, the expletive use was rare (See Lappin and Leass (1994) for identifying the expletive use of it.) As mentioned, this is attributed to the type of corpora having specific topics of conversation. Function of ellipsis Number (FaCon) Number (TV) expletive 2</context>
P00-1060==>P97-1003!=<context citStr="Collins, 1997" endWordPosition="156" position="1161" startWordPosition="155">al parsing, various probabilistic evaluation models have been proposed where different models use different feature types [Black, 1992] [Briscoe, 1993] [Brown, 1991] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1991] [Magerman, 1992] [Magerman, 1995] [Eisner, 1996]. How to evaluate the different feature types effects for syntactic parsing? The paper proposes an information-theory-based feature </context>
P00-1060==>P97-1003!=<context citStr="Collins, 1997" endWordPosition="702" position="4368" startWordPosition="701">in this regard is very limited. Many probabilistic evaluation models have been published inspired by one or more of these feature types [Black, 1992] [Briscoe, 1993] [Charniak, 1997] [Collins, 1996] [Collins, 1997] [Magerman, 1995] [Eisner, 1996], but discrepancies between training sets, algorithms, and hardware environments make it difficult, if not impossible, to compare the models objectively. In the paper,</context>
P06-1013==>J91-1002!=<context citStr="Morris and Hirst, 1991" endWordPosition="1318" position="8522" startWordPosition="1314">re takes into account, and (d) the measure of sense similarity. Lexical Chains Lexical cohesion is often represented via lexical chains, i.e., sequences of related words spanning a topical text unit (Morris and Hirst, 1991). Algorithms for computing lexical chains often perform WSD before inferring which words are semantically related. Here we describe one such disambiguation algorithm, proposed by Galley and McKeown (2</context>
J99-3001==>J86-3001!=<context citStr="Grosz and Sidner (1986)" endWordPosition="505" position="3518" startWordPosition="502">ecognized the need to account for referential ambiguities in discourse and have developed various theories centered around the notion of discourse focus (Grosz 1977; Sidner 1983). In a seminal paper, Grosz and Sidner (1986) wrapped up the results of their research and formulated a model in which three levels of discourse coherence are distinguishedattention, intention, and discourse segment structure. While this paper </context>
J99-3001==>J86-3001!=<context citStr="Grosz and Sidner 1986" endWordPosition="2849" position="18971" startWordPosition="2846">king center and a list of forwardlooking centers, as well as a few rules and constraints that govern the interpretation of centers. It is assumed that discourses are composed of constituent segments (Grosz and Sidner 1986), each of which consists of a sequence of utterances. Each utterance L/i in a given discourse segment DS is assigned a list of forward-looking centers, Cf(DS, Ui), and a unique backward-looking center</context>
P04-1060==>P02-1017!=<context citStr="Klein and Manning, 2002" endWordPosition="491" position="3153" startWordPosition="487"> the approach of (Nigam et al., 2000), who apply it for EM training of a text classifier. The factors are only sensitive to the constituent/distituent (C/D) status of each span of the string in (cp. (Klein and Manning, 2002)). The C/D status is derived from an aligned parallel corpus in a way discussed in section 2. We use the Europarl corpus (Koehn, 2002), and the statistical word alignment was performed with the GIZA++</context>
P04-1060==>P02-1017!=<context citStr="Klein and Manning, 2002" endWordPosition="3682" position="21855" startWordPosition="3678">, but it may at least serve as a basis for comparison.) The evaluation criteria we apply are unlabeled bracketing precision and recall (and crossing brackets). We follow an evaluation criterion that (Klein and Manning, 2002, footnote 3) discuss for the evaluation of a not fully supervised grammar induction approach based on a binary grammar topology: bracket multiplicity (i.e., non-branching projections) is collapsed in</context>
W98-1241==>W97-1011!=<context citStr="Powers (1997" endWordPosition="1380" position="8994" startWordPosition="1379"> for most other corpora tried, with right context appearing more useful than left, coset size being more accurate than coset coverage, union size being more reliable than intersection size. Note that Powers (1997a) generalizes the approach and considers a multitude of different clustering metrics and methods, introducing a pair of goodness measures which allow a more principled approach to closing and evaluat</context>
W98-1241==>W97-1011!=<context citStr="Powers (1997" endWordPosition="1549" position="10139" startWordPosition="1548">retation of the structures or classes was offered, and no attempt was made to discover or propose cohesive constraints or semantic relationships. At the same time however, Entwisle and Groves (1994), Powers (19971,) and Entwisle and Posers (1997) have produced a constraint Powers 308 Unsupervised Clustering, Segmentation and Cohesion parser which uses precisely the kind of morphological and grammatical classe</context>
W98-1241==>W97-1011!=<context citStr="Powers (1997" endWordPosition="1619" position="10628" startWordPosition="1618">have started to address how one develop meaningful statistics for a true grammar learning system without any preconceived notions of what the correct parse/phrase structure is (if any). In particular Powers (1997b) performed experiments in the context of grammat checking application, using automatic segmentation techniques based on those of Harris (1960) and similar to those used by Brent (1997), but combined</context>
W98-1241==>W97-1011!=<context citStr="Powers, 1997" endWordPosition="1961" position="12916" startWordPosition="1960">repeated, finding the subsequent perplexity or information maxima. In addition, even the initial functional segments found may be used directly to learn or check a grammar (Entwisle and Groves, 1994; Powers, 1997b), although this already makes use of the known word segmentation and the assumption, which is for English is an excellent first approximation, that affixes are either word initial or word final, and</context>
W98-1241==>W97-1011!=<context citStr="Powers (1997" endWordPosition="2114" position="13837" startWordPosition="2113">ter of the Alice Corpus, Carroll, 1865), and where the relaxation may involve the supplying of new roles or the removal of a constraint at any one of a number of possible points. The approach used by Powers (1997b) is only intended to identify typing errors and substitution errors (e.g. 'there' for 'their') and builds and stores a differential grammar only when the word can be disambiguated from its closed-cl</context>
P06-2089==>C04-1010!=<context citStr="Nivre and Scholz, 2004" endWordPosition="30" position="268" startWordPosition="27"> Kenji Sagae and Alon Lavie Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 {sagae,alavie}@cs.cmu.edu Abstract Recently proposed deterministic classifierbased parsers (Nivre and Scholz, 2004; Sagae and Lavie, 2005; Yamada and Matsumoto, 2003) offer attractive alternatives to generative statistical parsers. Deterministic parsers are fast, efficient, and simple to implement, but generally </context>
P06-2089==>C04-1010!=<context citStr="Nivre and Scholz, 2004" endWordPosition="257" position="1722" startWordPosition="254">niak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic contextfree grammars. Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for </context>
P06-2089==>C04-1010!=<context citStr="Nivre and Scholz (2004)" endWordPosition="434" position="2805" startWordPosition="431">r accuracy was still well below that of state-of-the-art statistical parsers. We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. The parser retains much of the simplicity of deterministic classifier-based parsers, but achieve</context>
P06-2089==>C04-1010!=<context citStr="Nivre and Scholz (2004)" endWordPosition="656" position="4279" startWordPosition="653">ithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. The algorithm considers only trees with unary and binary productions. Training the parser with arbitrary branching trees is acc</context>
P06-2089==>C04-1010!=<context citStr="Nivre and Scholz (2004)" endWordPosition="2468" position="14624" startWordPosition="2465">l (or 87.5% using gold-standard POS tags) on the WSJ test section of the Penn Treebank, taking only 11 minutes to parse the test set. Sagae and Lavies parsing algorithm is similar to the one used by Nivre and Scholz (2004) for deterministic dependency parsing (using kNN). Yamada and Matsumoto (2003) have also presented a deterministic classifier-based (SVM-based) dependency parser, but using a different parsing algorit</context>
A00-1019==>P99-1041!=<context citStr="Melamed, 1997" endWordPosition="2932" position="16915" startWordPosition="2931">ier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996). It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99). Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; Furuse and Iida, 96). In this study, we filtered for potential seq</context>
W00-1415==>P98-2242!=<context citStr="Cheng, 1998" endWordPosition="1683" position="10407" startWordPosition="1682">fference between this and the second type of modifier. This type subsumes the modifiers normally generated by an aggregation module, in particular one using embedding (e.g. (Shaw and McKeown, 1997), (Cheng, 1998)). (6) a. the decoration on this cabinet the best looking food I ever saw b. This is a mighty empty country. c. the wide gilt bronze straps on the coffer fronts and sides; He lived in a fiveroom apart</context>
A00-3006==>P95-1026!=<context citStr="Yarowsky (1995)" endWordPosition="477" position="2906" startWordPosition="476">o be a fairly straightforward extension to allow for the kind of context-sensitivity in error correction which has been the focus of much recent work (cf. Golding and Roth (1999), Mays et al. (1991), Yarowsky (1995)). The approach I am pursuing is an application of Bayesian statistics. We treat the process by which the electronic text was produced as a noisy channel, and take as our goal the maximization of the </context>
W00-1327==>C00-2100!=<context citStr="Sarkar and Zeman, 2000" endWordPosition="220" position="1606" startWordPosition="217">onaries from textual corpora has become increasingly popular (e.g. Brent, 1991, 1993; Ushioda et al., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth 1998; Gahl, 1998; Lapata, 1999, Sarkar and Zeman, 2000). The different approaches vary according to the methods used and the number of scFs being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token rec</context>
C04-1089==>W04-3236!=<context citStr="Ng and Low, 2004" endWordPosition="2174" position="12100" startWordPosition="2171">ike English, Chinese text is composed of Chinese characters with no demarcation for words. So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text from a half-month period. Then we determined the new Chinese words in each half-month period p. By new C</context>
W03-1602==>P98-1056!=<context citStr="Dorna et al., 1998" endWordPosition="888" position="5849" startWordPosition="885">tactic transformation in itself can often be a motivation or goal of paraphrasing. Therefore, such an approach as semantic transfer, where morphosyntactic information is highly abstracted away as in (Dorna et al., 1998; Richardson et al., 2001), does not suit this task. Provided that the morphosyntactic stratum be an optimal level of abstraction for representing paraphrasing/transfer patterns, one must recall that </context>
W03-1602==>P98-1056!=<context citStr="Dorna et al., 1998" endWordPosition="3141" position="19787" startWordPosition="3138">an open set of dependency classes including dependency, compound, parallel, appositive and insertion. 4.3 Three-layered representation Previous work on transfer-based MT systems (Lavoie et al., 2000; Dorna et al., 1998) and alignment-based transfer knowledge acquisition (Meyers et al., 1996; Richardson et al., 2001) have proven that transfer knowledge can be best represented by declarative structure mapping (transfo</context>
H05-2015==>W04-2902!=<context citStr="[2]" endWordPosition="992" position="6491" startWordPosition="992">. 3 Experimental Results To date we have collected and analyzed a corpus of approximately 300 hours of audio lectures including 6 full MIT courses and 80 hours of seminars from the MIT World web site [2]. We are currently in the process of expanding this corpus. From manual transcriptions we have generated and verified time-aligned transcriptions for 169 hours of our corpus, and we are in the process</context>
W99-0608==>J95-4004!=<context citStr="Brill (1995)" endWordPosition="2151" position="13285" startWordPosition="2150"> training corpus 50% 60% 70% 80% 90% 95% 99% 100% # Classes 8 11 14 18 36 57 111 239 the prefix (or the suffix) of any word in the lexicon? The last group of features are inspired in those applied by Brill (1995) when addressing unknown words. The learning algorithm3 acquired, in about thirty minutes, a base of 191 trees (the other ambiguity classes had not enough examples) which required about 0,68 Mb of sto</context>
W99-0608==>J95-4004!=<context citStr="Brill, 1995" endWordPosition="3014" position="18646" startWordPosition="3013">me words. In this way, we consider information about the surrounding words at three different levels of specificity: word form, POS tag, and ambiguity class. Very similar to Brill's lexical patterns (Brill, 1995), we also have included features to capture collocational information. Such features are obtained by composition of the already described single attributes and they are sequences of contiguous words a</context>
W99-0608==>J95-4004!=<context citStr="Brill, 1995" endWordPosition="4257" position="26268" startWordPosition="4256">results reported by several state-of-the-art POS taggers, tested on the WSJ corpus with the open vocabulary assumption. In that table, TBL stands for Brill's transformation-based error-driven tagger (Brill, 1995), ME stands for a tagger based on the maximum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-base</context>
W02-0606==>J01-2001!=<context citStr="Goldsmith (2001)" endWordPosition="745" position="4739" startWordPosition="744">ld take, and we discuss some potential uses for the output of our algorithm. 2 Related work For space reason, we discuss here only three approaches that are closely related to ours. See, for example, Goldsmith (2001) for a very different (possibly complementary) approach, and for a review of other relevant work. 2.1 Jacquemin (1997) Jacquemin (1997) presents a model that automatically extracts morphologically rel</context>
W02-0606==>J01-2001!=<context citStr="Goldsmith (2001)" endWordPosition="6227" position="38899" startWordPosition="6226">t stemmers for information retrieval applications, or they could be integrated into morphological analyzers. Our procedure could also be used to replace the first step of algorithms, such as those of Goldsmith (2001) and Snover and Brent (2001), where heuristic methods are employed to generate morphological hypotheses, and then an informationtheoretically/probabilistically motivated measure is used to evaluate or</context>
P84-1047==>A83-1006!=<context citStr="[6]" endWordPosition="368" position="2514" startWordPosition="368"> interfaces based on semantic grammar, in part because Um grammar definition formalism is not well integrated with the method of defining the object and actions of the domain of discourse (though see [6]). 1 This research wns sponsored by the Air !Wee Office of Scient,fic flesearch under Contract /11-0:311-82-0219 This paper proposes an alternative approach to restricted domain language recognition c</context>
E03-1050==>W02-1037!=<context citStr="Munteanu and Marcu, 2002" endWordPosition="151" position="1062" startWordPosition="148">ion quality can be improved by using additional noisier bilingual data. Some approaches, like (Fung and MxKeown, 1997), have been developed to extract word translations from non-parallel corpora. In (Munteanu and Marcu, 2002) bilingual suffix trees are used to extract parallel sequences of words from a comparable corpus. 95% of those phrase translation pairs were judged to be correct. However, no results where reported if</context>
W06-1206==>W05-1008!=<context citStr="Baldwin (2005)" endWordPosition="4317" position="25355" startWordPosition="4316">rammars can be modelled by a mapping from word stems to atomic lexical types, we now go on designing the statistical methods that can automatically guess such mappings for unknown words. Similar to Baldwin (2005), we also treat the problem as a classification task. But there is an important difference. While Baldwin (2005) makes predictions for each unknown word, we create a new lexical entry for each occurre</context>
P98-1117==>J93-1006!=NO CONTEXT
P01-1061==>P95-1021!=<context citStr="Rambow et al., 1995" endWordPosition="1823" position="11664" startWordPosition="1820">including Categorial Grammars (CGs) (Wood, 1993), Synchronous Tree Adjoining Grammars (TAGs) (Joshi, 1985; Shieber and Schabes, 1990; Shieber, 1994), and Synchronous Description Tree Grammars (DTGs) (Rambow et al., 1995; Rambow and Satta, 1996). Most of these formalisms can be extended to define semantic associations over entire shared forests, rather than merely over individual parse trees, in a straightforward man</context>
N01-1006==>J95-2004!=<context citStr="Roche and Schabes (1995)" endWordPosition="1690" position="10077" startWordPosition="1687">tation and an implementation of &amp;quot;lazy&amp;quot; learning. The application of a transformation-based learning can be considerably sped-up if the rules are compiled in a finite-state transducer, as described in Roche and Schabes (1995). 3 The Algorithm The approach presented here builds on the same foundation as the one in (Ramshaw and Marcus, 1994): instead of regenerating the rules each time, they are stored into memory, together</context>
J05-3002==>J93-2003!=<context citStr="Brown et al. 1993" endWordPosition="10658" position="69338" startWordPosition="10655">, which have been manually compressed, are aligned with the original sentences from which they were drawn. Knight and Marcu (2000) treat reduction as a translation process using a noisychannel model (Brown et al. 1993). In this model, a short (compressed) string is treated as a source, and additions to this string are considered to be noise. The probability of a source string s is computed by combining a standard p</context>
W06-1673==>P05-1073!=<context citStr="Toutanova et al., 2005" endWordPosition="496" position="3168" startWordPosition="493">gives useful improvements (e.g., in Koomen et al. (2005)), but efficiently enumerating k-best lists often requires very substantial cognitive and engineering effort, e.g., in (Huang and Chiang, 2005; Toutanova et al., 2005). At the other extreme, one can maintain the entire space of representations (and their probabilities) at each level, and use this full distribution to calculate the full distribution at the next leve</context>
W06-1673==>P05-1073!=<context citStr="Toutanova et al. (2005)" endWordPosition="3715" position="22197" startWordPosition="3712">tems that do question answering, summarization, and any other task which directly uses a semantic interpretation. 4.2 System Description We modified the system described in Haghighi et al. (2005) and Toutanova et al. (2005) to test our method. The system uses both local models, which score subtrees of the entire parse tree independently of the labels of other nodes not in that subtree, and joint models, which score the </context>
P96-1030==>C94-1099!=<context citStr="Andry et al., 1994" endWordPosition="420" position="2533" startWordPosition="417">a suitable training corpus. For example, (Briscoe and Carroll, 1993) train an LR parser based on a general grammar to be able to distinguish between likely and unlikely sequences of parsing actions; (Andry et al., 1994) automatically infer sortal constraints, that can be used to rule out otherwise grammatical constituents; and (Grishman et al., 1984) describes methods that reduce the size of a general grammar to inc</context>
C94-1075==>P85-1018!=<context citStr="[Shi85]" endWordPosition="2213" position="13697" startWordPosition="2213">ecial techniques require more specific operations on constraints. For instance, a family of parsing strategies related to Earley's algorithm make use of the restriction operator defined by Shieber in [Shi85]. Another example: some tabular techniques take benefit from a projection operator that restricts constraints with respect to a subset of their variables. We could define the solver's interface as the</context>
P98-1113==>C96-2121!=<context citStr="[6]" endWordPosition="1822" position="11663" startWordPosition="1822">tree structure in the output SSTC Figure 6: Example-based natural language parsing based on the SSTC schema. 689 close: distance not too large, modification: edit operations (insert, delete, replace) [6]. In most of the cases, similar sentence might not occurred in the example-base, so the system utilized some close related examples to the given input sentence (i.e. similar structure to the input sen</context>
P98-1113==>C96-2121!=<context citStr="[6]" endWordPosition="3279" position="19718" startWordPosition="3279">ed based on insertion, deletion or replacement. Since words are strings of characters, sentences are strings of words, editing distances hence are not confined to words, they may be used on sentences [6]. With the similar idea, we define the edition distance as: (i) The distance is calculated at level of substitutions (i.e. only the root nodes of the substitutions will be considered, not all the word</context>
N04-1033==>J90-2002!=<context citStr="Brown et al., 1990" endWordPosition="214" position="1380" startWordPosition="211">robability: Pr(eI1|fJ1 )a (1) eI 1 Pr(eI1)  Pr(fJ1 |eI1)a (2) The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation (Brown et al., 1990). It allows an independent modeling of target language model Pr(eI1) and translation model Pr(fJ1 |eI1)1. The target language 1The notational convention will be as follows: we use the symbol Pr() to </context>
J03-3001==>P99-1068!=<context citStr="Resnik (1999)" endWordPosition="1263" position="7589" startWordPosition="1262">a and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine. Philip Resnik (1999) showed that parallel corporauntil then a promising research avenue but largely constrained to the English-French Canadian Hansardcould be found on the Web: We can grow our own parallel corpus using</context>
W04-1109==>C02-1049!=<context citStr="Chen and Ma, 2002" endWordPosition="1127" position="6717" startWordPosition="1124"> found in a dictionary. Therefore, they cannot be segmented correctly by simply referring to the dictionary. Many approaches have been reported for unknown word detection such as (Chen and Bai, 1997; Chen and Ma, 2002; Fu and Wang, 1999; Lai and Wu, 1999; Ma and Chen, 2003; Nie et al., 1995; Shen et al., 1998; Zhang et al., 2002; Zhou and Lua, 1997). There are rulebased, statistics-based or even hybrid models. In </context>
J99-4005==>J93-2003!=<context citStr="Brown et al. (1993)" endWordPosition="197" position="1413" startWordPosition="194">ified model of how certain structures (such as strings) are generated and transformed. We then instantiate the model through training on a database of sample structures and transformations. Recently, Brown et al. (1993) built a source-channel model of translation between English and French. They assumed that English strings are produced according to some stochastic process (source model) and transformed stochastical</context>
J99-4005==>J93-2003!=<context citStr="Brown et al. (1993)" endWordPosition="1047" position="6569" startWordPosition="1044">e know that plaintext is converted to ciphertext, letter by letter, according to some table. We have no such clear conception about how English gets converted to French, although many theories exist. Brown et al. (1993) recently cast some simple theories into a source-channel framework, using the bilingual Canadian parliament proceedings as training data. We may assume:  v total English words.  A bigram source mod</context>
J99-4005==>J93-2003!=<context citStr="Brown et al. (1993)" endWordPosition="1149" position="7250" startWordPosition="1146">ing substituted with French ones, though not one-for-one and not without changing their order. These are important departures from the two applications discussed earlier. In the main channel model of Brown et al. (1993), each English word token e, in a source sentence is assigned a &amp;quot;fertility&amp;quot; 0 which dictates how many French words it will produce. These assignments are made stochastically according to a table n(01</context>
J99-4005==>J93-2003!=<context citStr="Brown et al. (1993)" endWordPosition="1268" position="7936" startWordPosition="1265">ducing n, s, and d parameter estimates is easy if we are given annotations in the form of word alignments. An alignment is a set of connections between English and French words in a sentence pair. In Brown et al. (1993), alignments are asymmetric each French word is connected to exactly one English word. 609 Computational Linguistics Volume 25, Number 4 Given a collection of sentence pairs: 1. collect estimates for</context>
J99-4005==>J93-2003!=<context citStr="Brown et al. (1993)" endWordPosition="1500" position="9213" startWordPosition="1497"> s, and d counts by considering each alignment, but this is expensive. (By contrast, part-of-speech tagging involves a single alignment, leading to 0(m) training). Lacking a polynomial reformulation, Brown et al. (1993) decided to collect counts only over a subset of likely alignments. To bootstrap, they required some initial idea of what alignments are reasonable, so they began with several iterations of a simpler </context>
J05-1005==>E95-1016!=<context citStr="Framis 1995" endWordPosition="1978" position="13120" startWordPosition="1977">rs, various stochastic approaches to linguistic requirements acquisition have been proposed (Basili, Pazienza, and Velardi 1992; Hindle and Rooth 1993; Sekine et al. 1992; Grishman and Sterling 1994; Framis 1995; Dagan, Marcus, and Markovitch 1995; Resnik 1997; Dagan, Lee, and Pereira 1998; Marques, Lopes, and Coelho 2000; Ciaramita and Johnson 2000). In general, they follow comparable learning strategies, d</context>
J05-1005==>E95-1016!=<context citStr="Framis 1995" endWordPosition="2386" position="15799" startWordPosition="2385">. Finally, it can be identified as the head or the dependent role within a binary grammatical relationship such as subject, direct object, or modifier (Sekine et al. 1992; Grishman and Sterling 1994; Framis 1995). In section 4, we pay special attention to the grammatical characterization of syntactic positions. As far as cond is concerned, various types of information are used to define a linguistic condition</context>
J05-1005==>E95-1016!=<context citStr="Framis 1995" endWordPosition="2947" position="19359" startWordPosition="2946">this issue in section 8.1. In other approaches to requirement learning, linguistic conditions are defined in semantic terms by means of specific tags (Basili, Pazienza, and Velardi 1992; Resnik 1997; Framis 1995). In order to calculate the degree of association between tag doc and position (right, approve), these approaches count the frequency of pairs like ((right, approve), doc) throughout the corpus. If th</context>
J05-1005==>E95-1016!=<context citStr="Framis 1995" endWordPosition="3358" position="22118" startWordPosition="3357">ding to the quantity of linguistic knowledge they require. The most knowledgerich approaches need a handcrafted thesaurus (WordNet) to semantically annotate nouns of the training corpus (Resnik 1997; Framis 1995; Ciaramita and Johnson 2000). At the opposite end of the continuum, the most knowledge-poor methods are introduced in Dagan, Marcus, and Markovitch (1995) and Dagan, Lee, and Pereira (1998); these me</context>
H05-1005==>P02-1040!=<context citStr="Papineni et al., 2002" endWordPosition="2496" position="15501" startWordPosition="2493">04 for development purposes and present the average P, R and F for the remaining 18 sets in Table 1. There were 210 generated references in the 18 testing sets. The table also shows the popular BLEU (Papineni et al., 2002) and NIST2 MT metrics. We also provide two baselines - most frequent initial reference to the person in the input (Base1) and a randomly selected initial reference to the person (Base2). As Table 1 sh</context>
W00-1313==>J90-1003!=<context citStr="Church &amp; Hanks (1990)" endWordPosition="742" position="4928" startWordPosition="739">ach other and work together to express a query requirement. The association relationship between two words can be indicated by their mutual information, which can be further used to discover phrases [Church &amp; Hanks (1990)]. If two words are independent with each other, their mutual information would be close to zero. On the other hand, if they are strongly related, the mutual information would be much greater than zer</context>
A00-2011==>P99-1068!=<context citStr="Resnik, 1999" endWordPosition="187" position="1286" startWordPosition="186"> be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-parallel bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requir</context>
W06-2908==>W05-0407!=<context citStr="Moschitti et al. (2005)" endWordPosition="437" position="2801" startWordPosition="434">ely studied tree kernels for semantic role labeling. However, they reported that they could not successfully build an accurate argument recognizer, although the role assignment was improved. Although Moschitti et al. (2005) reported on argument recognition using tree kernels, it was a preliminary evaluation because they used oracle parse trees. Kazama and Torisawa (2005) proposed a new tree kernel for node relation labe</context>
P06-2048==>P03-1054!=<context citStr="Klein and Manning, 2003" endWordPosition="131" position="930" startWordPosition="128">duction Much of the current research into probabilistic parsing is founded on probabilistic contextfree grammars (PCFGs) (Collins, 1996; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001; Klein and Manning, 2003). For instance, consider the parse tree in Figure 1. One way to decompose this parse tree is to view it as a sequence of applications of CFG rules. For this particular tree, we could view it as the ap</context>
P06-2048==>P03-1054!=<context citStr="Klein and Manning, 2003" endWordPosition="5078" position="28364" startWordPosition="5075">e. 0CB denotes the percentage of sentences in the test set that exhibit no cross-bracketing. With a simple feature set, we manage to obtain performance comparable to the unlexicalized PCFG parser of (Klein and Manning, 2003) on the set of sentences of length 375 40 or less. On the subset of Section 23 consisting of sentences of length 100 or less, our parser slightly outperforms their results in terms of average cross-br</context>
P97-1061==>J93-1007!=<context citStr="Smadja, 1993" endWordPosition="318" position="2120" startWordPosition="317">rowing interest in corpus-based approaches which retrieve collocations from large corpora (Nagao and Mori, 1994), (Ikehara et al., 1996) (Kupiec, 1993), (Fung, 1995), (Kitamura and Matsumoto, 1996), (Smadja, 1993), (Smadja et al., 1996), (Haruno et al., 1996). Although these approaches achieved good results for the task considered, most of them aim to extract fixed collocations, mainly noun phrases, and requir</context>
P97-1061==>J93-1007!=<context citStr="Smadja, 1993" endWordPosition="2401" position="14504" startWordPosition="2400">ly the collocations retrieved to a machine translation system and evaluate how they contribute to the quality of translation. 4 Related work Algorithms for retrieving collocations has been described (Smadja, 1993) (Haruno et al., 1996). (Smadja, 1993) proposed a method to retrieve collocations by combining bigrams whose cooccurrences are greater than a given threshold3. In their approach, the bigrams are valid</context>
W94-0111==>A88-1019!=<context citStr="Church, 1988" endWordPosition="1015" position="6265" startWordPosition="1014"> of the three previous tags is A&amp;quot;. Brill's results demonstrate that this approach can outperform the Hidden Markov Model approaches that are frequently used for part-of-speech tagging (Jelinek, 1985; Church, 1988; DeRose, 1988; Cutting et al., 1992; Weischedel et al., 1993), as well as showing promise for other applications. The resulting model, encoded as a list of rules, is also typically more compact and f</context>
W04-3103==>A97-1004!=<context citStr="Reynar and Ratnaparkhi, 1997" endWordPosition="2228" position="14176" startWordPosition="2225"> in curcumin over the last decade. Each abstract set was prepared for annotation as follows: the order of the abstracts was randomized and the abstracts were broken into sentences using Mxterminator (Reynar and Ratnaparkhi, 1997). The following people performed the annotations: Padmini Srinivasan, who has analyzed crohns and turmeric documents for a separate knowledge discover research task, Xin Ying Qiu, who is completely ne</context>
H92-1041==>A88-1019!=<context citStr="[8]" endWordPosition="1442" position="9072" startWordPosition="1442">consisted only of simple noun phrases, i.e. head nouns and their immediate premodifiers. Phrases were formed using parts, a stochastic syntactic class tagger and simple noun phrase bracketing program [8]. Words 213 We estimate P(Cj = 11D,) by: P(Ci = 1) x (P(Wi = 11Cj = 1) x P(Wi = 11Dm) P(Wi = OlCi = 1) x P(Wi = 01.13,)) p(wi =1) p(vvi = 0) Explanation:  P(Ci = lipm) is the probability that catego</context>
J03-1006==>C92-2066!=<context citStr="Schabes 1992" endWordPosition="2103" position="11440" startWordPosition="2102">cting a context-free grammar c(G,w) out of a tree-adjoining grammar G and an input string w. This can straightforwardly be generalized to weighted (in particular, stochastic) tree-adjoining grammars (Schabes 1992). 3 If there is more than one goal item, then a new symbol needs to be introduced as the start symbol. 138 Nederhof Weighted Deductive Parsing It was shown by Boullier (2000) that F may furthermore be</context>
W06-1615==>N03-1027!=<context citStr="Roark and Bacchiani (2003)" endWordPosition="4406" position="26596" startWordPosition="4402">iscriminative parser to improve its adaptation properties. 8 Related Work Domain adaptation is an important and wellstudied area in natural language processing. Here we outline a few recent advances. Roark and Bacchiani (2003) use a Dirichlet prior on the multinomial parameters of a generative parsing model to combine a large amount of training data from a source corpus (WSJ), and small amount of training data from a targe</context>
N04-1002==>P98-1012!=<context citStr="Bagga and Baldwin (1998)" endWordPosition="3359" position="20932" startWordPosition="3356">scribed in Section 2. We present the results by corpus. 6.1 John Smith Corpus Results Our main goal for the John Smith corpus is to demonstrate that we have successfully approximated the algorithm of Bagga and Baldwin (1998). Figure 1 shows how recall and precision trade off against each other as the decision threshold (should a name be put into a chain) varies in the incremental vector space approach. This graph is near</context>
C00-2131==>C90-3044!=<context citStr="[7, 8, 9, 12]" endWordPosition="155" position="1142" startWordPosition="152">on patterns in a corpus-based translation system. 1 Introduction So far, a number of methodologies and systems for machine translation using large corpora exist. They include example-based approaches [7, 8, 9, 12], pattern-based approaches [10, 11, 14], and statistical approaches. For instance, example-based approaches use a large set of translation patterns each of which is a pair of parsed structures of a so</context>
A92-1012==>C90-2008!=<context citStr="Briscoe et al (1990)" endWordPosition="1703" position="10857" startWordPosition="1700">ics of the verb sense which is associated with the purpose of an entity (eating in this case). The way in which such a representation may be used in the treatment of logical metonymy was described in Briscoe et al (1990). Other features (such as PHYSICAL-STATE) are used to encode information which is useful for applications such as sense-disambiguation. This attempt to represent detailed lexical semantic information </context>
N03-1036==>W97-0313!=<context citStr="Riloff and Shepherd (1997)" endWordPosition="337" position="2192" startWordPosition="334">taxonomy can be posed in two complementary ways. Firstly, given a particular taxonomic class (such as fruit) one could seek members of this class (such as apple, banana). This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently by Widdows and Dorow (2002). Secondly, given a particular word (such as apple), one could seek suitable taxonomic classes for describing this object (such</context>
E06-1010==>P02-1018!=<context citStr="Johnson, 2002" endWordPosition="302" position="2189" startWordPosition="301">esearch devoted to different methods for correcting this approximation. Most of this work has so far focused either on post-processing to recover non-local dependencies from context-free parse trees (Johnson, 2002; Jijkoun and De Rijke, 2004; Levy and Manning, 2004; Campbell, 2004), or on incorporating nonlocal dependency information in nonterminal categories in constituency representations (Dienes and Dubey, </context>
J96-2002==>J96-3001!=<context citStr="Reiter and Mellish (1992)" endWordPosition="469" position="3323" startWordPosition="466">(1992), Daelemans (1994), Daelemans and De Smedt (1994), Ide, Le Maitre, and Veronis (1994), Lascarides et al. (1996), Mellish and Reiter (1993), Mitamura and Nyberg (1992), Penn and Thomason (1994), Reiter and Mellish (1992), Young (1992), and Young and Rounds (1993).  1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 2 show that the language is nonetheless sufficiently expressiv</context>
J92-3003==>P86-1038!=<context citStr="Kasper and Rounds (1986)" endWordPosition="6721" position="40274" startWordPosition="6718">the union of the global extensions of all lexical classes in the lexicon. 3.3 Variant SetsDiscussion Variant sets may be thought of as representing a restricted form of disjunction over complex FSs. Kasper and Rounds (1986) show general disjunctive unification to be intractable, since it involves an exponentially complex step of expansion to disjunctive normal form. Note, however, that the alternation embodied in varian</context>
W03-1801==>P98-1092!=<context citStr="Ibekwe-SanJuan, 1998" endWordPosition="1727" position="11259" startWordPosition="1726">tes a visual interface developed with the Aisee graphic visualization to enable the user explore the classes and browse through the links between terms. Earlier stages of this work were presented in (Ibekwe-SanJuan, 1998). The system comprises of two major modules: a syntactic variant identifier and a clustering module whose results are loaded onto the Aisee visualization tool. 4.1 Variants identifier module Automati</context>
W03-1801==>P98-1092!=<context citStr="Ibekwe-SanJuan, 1998" endWordPosition="2295" position="14975" startWordPosition="2294">erm variants &amp;quot;frozen dough baking, frozen dough characteristics, frozen dough products&amp;quot;. The common attribute is &amp;quot;frozen dough&amp;quot;, shared by this class of concepts &amp;quot;products, characteristics, baking&amp;quot;. (Ibekwe-SanJuan, 1998) already put forward the idea of these semantic relations and (Jacquemin, 1995) reported similar conceptual relations for his insertion and coordination variants. 4.2 Variant Clustering Module The sec</context>
P99-1032==>J93-1003!=<context citStr="Dunning, 1993" endWordPosition="1644" position="10201" startWordPosition="1643">n the data conform to the constraints is called the fit of the model. In this work, model fit is reported in terms of the likelihood ratio statistic, G2, and its significance (Read and Cressie, 1988; Dunning, 1993). The higher the G2 value, the poorer the fit. We will consider model fit to be acceptable if its reference significance level is greater than 0.01 (i.e., if there is greater than a 0.01 probability t</context>
P06-1132==>P02-1004!=<context citStr="Gamon et al. (2002)" endWordPosition="2093" position="12813" startWordPosition="2090">orm ours, as we aim to generate surface forms of case markers rather than recover deeper case relations for which surface case marker are often used as a proxy. In the context of sentence generation, Gamon et al. (2002) used a decision tree to classify nouns into one of the four cases in German, as part of their sentence realization from a semantic representation, achieving high accuracy (87% to 93.5%). Again, this </context>
P01-1005==>P98-1029!=<context citStr="Brill and Wu (1998)" endWordPosition="1362" position="8716" startWordPosition="1359">ce and therefore for voting to be effective. But does voting still offer performance gains when classifiers are trained on much larger corpora? The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy. As training size increases signif</context>
J01-1003==>J95-4004!=<context citStr="Brill 1995" endWordPosition="984" position="6610" startWordPosition="983">hops, recent special issues of Machine Learning Journal (Vol. 34 Issue 1/3, Feb. 1999) and Al Magazine (Vol. 18, No. 4, 1997). 2 Not in the sense in which it is used in transformation-based learning (Brill 1995). 60 Oflazer, Nirenburg, and McShane Bootstrapping Morphological Analyzers be reasonable for a small root word list, the potential for &amp;quot;noisy&amp;quot; or incorrect alignments is quite high when the corpus of </context>
J01-1003==>J95-4004!=<context citStr="Brill 1995" endWordPosition="2056" position="13549" startWordPosition="2055">and morphographemic rules. The morphographemic rules describing changes in spelling as a result of affixation operations are induced from the examples provided by using transformation-based learning (Brill 1995; Satta and Henderson 1997). The result is an ordered set of contextual replace or rewrite rules, much like those used in phonology. 4 We use the term citation form to refer to the word form that is u</context>
J01-1003==>J95-4004!=<context citStr="Brill 1995" endWordPosition="5179" position="33335" startWordPosition="5178">nd surface forms. The segmented forms contain the citation forms and affixes; the affix boundaries are marked by the + symbol. This list is then processed by a transformation-based learning paradigm (Brill 1995; Satta and Henderson 1997), as illustrated in Figure 4. The basic idea is that we consider the list of segmented words as our input and find transformation rules (expressed as contextual rewrite rule</context>
P01-1049==>J98-1003!=<context citStr="Chen &amp; Chang, 1998" endWordPosition="352" position="2308" startWordPosition="349">nguistic techniques has been developed to alleviate such problems, ranging from the use of stemming, lexical chain and thesaurus (Jing &amp; Tzoukermann, 1999; Green, 1999), to word-sense disambiguation (Chen &amp; Chang, 1998; Leacock et al, 1998; Ide &amp; Veronis, 1998) and context (Cohen &amp; Singer, 1999; Jing &amp; Tzoukermann, 1999). The connectionist approach has been widely used to extract knowledge in a wide range of inform</context>
N04-1032==>C02-1126!=<context citStr="Chiang and Bikel 2002" endWordPosition="3283" position="19227" startWordPosition="3280">atistical parser that has high performance on English (Collins, 1999) and Czech(Collins et al. 1999). There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003), but there has been no report on applying the Collins parser on Chinese. The Collins parser is a lexicalized statistical parser based on a head-driven extended PCFG model; thu</context>
N04-1032==>C02-1126!=<context citStr="Chiang &amp; Bikel 2002" endWordPosition="3991" position="23380" startWordPosition="3988">rds, vs. 22.1 in TEST2. TEST1 has 32% long sentences (&gt;40 words) while TEST2 has only 13%. Table 8 Comparison with other parsers: TEST2  40 words LP(%) LR(%) F1(%) Bikel &amp; Chiang 2000 77.2 76.2 76.7 Chiang &amp; Bikel 2002 81.1 78.8 79.9 Levy &amp; Manning 2003 78.4 79.2 78.8 Collins parser 86.4 85.5 85.9 4.2 Semantic parsing using Collins parses In the test set of 113 sentences, there are 3 sentences in which target verbs</context>
A00-2004==>A94-1013!=<context citStr="Palmer and Hearst, 1994" endWordPosition="533" position="3528" startWordPosition="530">mprove accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format. 3.1 Similarity measure Punctuation and uninformative words</context>
P95-1028==>P88-1005!=<context citStr="Moran, 1988" endWordPosition="293" position="2029" startWordPosition="292">tic judgments (Webber, 1979; Kamp, 1981; Heim, 1983; Poesio, 1991; Reyle, 1993). And there are computational approaches that screen unavailable and/or redundant semantic forms (Hobbs &amp; Shieber, 1987; Moran, 1988; Vestre, 1991). This paper will show that these approaches allow unavailable readings, and thereby miss an important generalization concerning the readings that actually are available. This paper exa</context>
P95-1028==>P88-1005!=<context citStr="Moran, 1988" endWordPosition="942" position="6317" startWordPosition="941"> traditional linguistic judgments on grammatical readings. While there are undoubted differences in degree of availability among readings dependent upon semantics or discourse preference (Bunt, 1985; Moran, 1988), we will focus on all-or-none structural possibilities afforded by competence grammar.3 2In this simplistic notation, we gloss over tense analysis, among others. 'Moran's preference-based algorithm t</context>
H90-1022==>H89-2019!=<context citStr="[2]" endWordPosition="234" position="1561" startWordPosition="234">uation, chaired by Dave Pallett of NIST. The charge of this committee was to develop a methodology for data collection, training data dissemination, and testing for SLS systems under development (see [2] and [31). The emphasis of the committee's work has been on automatic evaluation of queries to an air travel information system. The first community-wide evaluation using the first version of methodol</context>
H90-1022==>H89-2019!=<context citStr="[2]" endWordPosition="1623" position="9832" startWordPosition="1623">lled AA 123 connecting different cities with different departure times and other characteristics). What should be produced for an answer is determined both by domain-independent linguistic principles [2] and domainspecific stipulation (Appendix B). The language used to express the answers is defined in Appendix C. 4.5 Developing Comparators A final necessary component is, of course, a program to comp</context>
P03-1009==>J01-3003!=<context citStr="Merlo and Stevenson, 2001" endWordPosition="187" position="1364" startWordPosition="184">semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g. (Jackendoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)). While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of red</context>
W99-0629==>W96-0102!=<context citStr="Daelemans et al., 1996" endWordPosition="260" position="1621" startWordPosition="256">scuss some Memory-Based (MB) shallow parsing techniques to find labeled chunks and grammatical relations in a sentence. Several MB modules have been developed in previous work, such as: a POS tagger (Daelemans et al., 1996), a chunker (Veenstra, 1998; Tjong Kim Sang and Veenstra, 1999) and a grammatical relation (GR) assigner (Buchholz, 1998). The questions we will answer in this paper are: Can we reuse these modules in</context>
W99-0629==>W96-0102!=<context citStr="Daelemans et al., 1996" endWordPosition="974" position="6008" startWordPosition="971">992) and (Sekine, 1998). 3 Methods and Results In this section we describe the stages of the cascade. The very first stage consists of a MemoryBased Part-of-Speech Tagger (MBT) for which we refer to (Daelemans et al., 1996). The next three stages involve determining boundaries and labels of chunks. Chunks are nonrecursive, non-overlapping constituent parts of sentences (see (Abney, 1991)). First, we simultaneously chunk</context>
W03-1026==>E99-1023!=<context citStr="Sang and Veenstra (1999)" endWordPosition="4348" position="26888" startWordPosition="4345">ombination. Briefly, the IOB general encoding scheme associates a label with each word, corresponding to whether it begins a specific entity, continues the entity, or is outside any entity. Tjong Kim Sang and Veenstra (1999) describes in detail the IOB schemes. The final experiment also has access to the output of systems trained in the IOB2 encoding. The addition of each feature type resulted in better performance, with</context>
W05-0905==>W04-2319!=<context citStr="Shriberg et al., 2004" endWordPosition="1464" position="9259" startWordPosition="1461">ng a different GUI. This GUI showed both their textual summary and the orthographic transcription, without topic segmentation but with one line per dialogue act based on the pre-existing MRDA coding (Shriberg et al., 2004) (The dialogue act categories themselves were not displayed, just the segmentation). Annotators were told to extract dialogue acts that together would convey the information in the textual summary, an</context>
W02-0602==>P00-1027!=<context citStr="Yarowsky and Wicentowski (2000)" endWordPosition="5124" position="30577" startWordPosition="5121">ithm. Furthermore, the output of the present system is potentially useful in discovering spelling change rules, which could then be bootstrapped to aid in discovering further morphological structure. Yarowsky and Wicentowski (2000) have developed a system that learns such rules given a preliminary morphological hypothesis and part of speech tags. While the experiments reported here are based on an input lexicon of orthographic </context>
P90-1023==>P85-1017!=<context citStr="Pereira (1985)" endWordPosition="401" position="2646" startWordPosition="400">area of research in unification-based NLP, where the focus has been on reducing the amount of DAG copying, and several approaches have arisen. Different versions of structure sharing were employed by Pereira (1985) as well as Karttunen and Kay (1985). In Karttunen (1986) structure sharing was abandoned for a technique allowing reversible unification. Wroblewski (1987) presents what he calls a non-destructive un</context>
W03-0411==>J02-3001!=<context citStr="Gildea and Jurafsky (2002)" endWordPosition="3159" position="20415" startWordPosition="3156">2) presents manuallyderived rules for disambiguating prepositions, in particular for of. Srihari et al. (2001) present manually-derived rules for disambiguating prepositions used in named entities. Gildea and Jurafsky (2002) classify semantic role assignments using all the annotations in FRAMENET, for example, covering all types of verbal arguments. They use several features derived from the output of a parser, such as t</context>
W03-0411==>J02-3001!=<context citStr="Gildea and Jurafsky (2002)" endWordPosition="3646" position="23460" startWordPosition="3643">arned over FrameNet can be carried over into Treebank, given a mapping of the fine-grained FrameNet roles into the coarsegrained Treebank ones. Such a mapping would be similar to the one developed by Gildea and Jurafsky (2002). Acknowledgements The first author is supported by a generous GAANN fellowship from the Department of Education. Some of the work used computing resources at NMSU made possible through MII Grants EIA</context>
W99-0502==>P96-1006!=<context citStr="Marcus et al, 1993" endWordPosition="338" position="2147" startWordPosition="335">re relatively well defined and agreed-upon criteria of what it means to have the &amp;quot;correct&amp;quot; part of speech or syntactic structure assigned to a word or sentence For instance, the Penn Treebank corpus (Marcus et al, 1993) pro'ides a large repository of texts annotated with partof-speech and syntactic structure information Two independent human annotators can achieve a high rate of agreement on assigning part-of-speech</context>
P04-3014==>H01-1035!=<context citStr="Yarowsky et al., 2001" endWordPosition="189" position="1340" startWordPosition="186">tructural technology for multilingual processing. It is crucial for the development of translation models and translation lexica (Tufi s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002). It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000). Syntax-light alignment models such as the fi</context>
P99-1040==>P98-1122!=<context citStr="Levow, 1998" endWordPosition="2970" position="18619" startWordPosition="2969">oustic feature predicts misrecognitions because users modify their pronunciation in response to system rejection messages in such a way as to lead to further misunderstandings (Shriberg et al., 1992; Levow, 1998). However, despite our expectations, the REJECTION% accuracy rate is not better than the BASELINE at our desired level of statistical significance. Using the EFFICIENCY features does improve the perfo</context>
P99-1040==>P98-1122!=<context citStr="Levow, 1998" endWordPosition="4502" position="28034" startWordPosition="4501">gher-level information. This work differs from previous work in focusing on behavior at the (sub)dialogue level, rather than on identifying single misrecognitions at the utterance level (Smith, 1998; Levow, 1998; van Zanten, 1998). The rationale is that a single misrecognition may not warrant a global change in dialogue strategy, whereas a user's repeated problems communicating with the system might warrant </context>
J00-2001==>P88-1022!=<context citStr="Hovy 1988" endWordPosition="804" position="5542" startWordPosition="803">e names given to the components vary; they have been called &amp;quot;strategic&amp;quot; and &amp;quot;tactical&amp;quot; components (e.g., McKeown 1985; Thompson 1977; Danlos 1987)1, &amp;quot;planning&amp;quot; and &amp;quot;realization&amp;quot; (e.g., McDonald 1983; Hovy 1988a), or simply &amp;quot;what to say&amp;quot; versus &amp;quot;how to say it&amp;quot; (e.g., Danlos 1987; Reithinger 1990). The precise division of work between the components can also vary, as can the extent to which the text planner </context>
J00-2001==>P88-1022!=<context citStr="Hovy 1988" endWordPosition="2764" position="17517" startWordPosition="2763">n failure (Appelt 1985; Nogier 1989), allowing the linguistic component to interrogate the planner (Mann 1983; Sondheimer and Nebel 1986), and Hovy's notion of restrictive (i.e., bottom-up) planning (Hovy 1988a, 1988c). All of these approaches, though, require that potential interactions be determined either by the tactical component or by the system designer in advance. The text planning component still h</context>
J00-2001==>P88-1022!=<context citStr="Hovy 1988" endWordPosition="4668" position="29781" startWordPosition="4667">cture Theory (Mann and Thompson 1987). In IGEN, the plans can involve any goals or actions that could be achieved via communication. Hovy has described another text planner that builds similar plans (Hovy 1988b). This system, however, starts with a list of information to be expressed and merely arranges it into a coherent pattern; it is thus not a planner in the sense used here (as Hovy makes clear). 10 Si</context>
J00-2001==>P88-1022!=<context citStr="Hovy 1988" endWordPosition="11566" position="74145" startWordPosition="11565">eing expressed. The planner could supply whatever information is needed to drive the network.' Something like this approach is in fact used in some systems (e.g., Elhadad and Robin 1992; PenMan 1989; Hovy 1988a). The problem with the discrimination network approach is that it can't handle the range of examples shown here without violating the modularity of the generator. The examples shown here have involv</context>
H91-1014==>H91-1010!=<context citStr="[6]" endWordPosition="580" position="3614" startWordPosition="580"> of these areas are described here in more detail. Speech Recognition Component The speech recognition configuration is similar to the one used in the VOYAGER system and is based on the SUMMIT system [6]. For the ATIS task, we used 76 context-independent phone models trained on speaker-independent data collected at TI and MIT [3]. There were 1284 TI sentences (read and spontaneous versions of 642 sen</context>
H91-1014==>H91-1010!=<context citStr="[6]" endWordPosition="803" position="4985" startWordPosition="803">ntences that parsed failed to pass the word-pair grammar. The interface to the natural language component was implemented with the N-best mechanism we have described previously for the VOYAGER system [6]. In our original implementation, the first N-best output which parsed was used by the back-end to generate a response. Since our natural language component (TINA) is able to produce a parse probabili</context>
W93-0112==>M91-1033!=<context citStr="[9, 5]" endWordPosition="322" position="2244" startWordPosition="321">th considerably less time and development effort (notably demonstrated by [11, 8]), these systems achieve performance comparable to more standard systems that rely heavily on full syntactic analysis ([9, 5]). However, because these pattern-based systems are still viewed as linguistically ungrounded and somewhat ad hoc, formal work in the application and acquisition of lexical patterns has lagged system </context>
I05-2033==>W99-0633!=<context citStr="Megyesi, 1999" endWordPosition="861" position="5325" startWordPosition="860">generalization method called RGLearn. Row 4 shows the test results of that tagger in Table 1. Transformation Based Learning is a rule-based method that we will discuss in depth in Section 2. Megyesi (Megyesi, 1999) and Kuba et al. (Kuba et al., 2004) produced results with TBL taggers that are given in Table 1, in rows 5 and 6, respectively. Kuba et al. (Kuba et al., 2004) performed experiments with combinations</context>
I05-2033==>W99-0633!=<context citStr="Megyesi, 1999" endWordPosition="1170" position="7088" startWordPosition="1169">he Hungarian language, Megyesi applied this technique initially with moderate success. (Megyesi, 1998) The weak part of her first implementation was the lexical module of the tagger, as described in (Megyesi, 1999). With the use of extended lexical templates, TBL produced a much better performance but still lagged behind the statistical taggers. We chose a different approach that is similar to (Kuba et al., 200</context>
W03-1501==>J98-4003!=<context citStr="Knight and Graehl, 1998" endWordPosition="310" position="2213" startWordPosition="307">on name and English name. Lin and Chen (2000) further classified the works into two directions  say, forward transliteration (Wan and Verspoor, 1998) and backward transliteration (Chen et al., 1998; Knight and Graehl, 1998), and proposed a phoneme-based model. Lin and Chen (2002) employed a machine learning approach to determine phonetic similarity scores for machine transliteration. AI-Onaizan and Knight (2002) investi</context>
P04-1067==>P95-1050!=<context citStr="Rapp, 1995" endWordPosition="475" position="3077" startWordPosition="474"> of the different methods proposed. Open issues are then discussed in section 7. 2 Standard approach Bilingual lexicon extraction from comparable corpora has been studied by a number of researchers, (Rapp, 1995; Peters and Picchi, 1995; Tanaka and Iwasaki, 1996; Shahzad et al., 1999; Fung, 2000, among others). Their work relies on the assumption that if two words are mutual translations, then their more fre</context>
J88-2004==>E87-1042!=<context citStr="Almeida 1987" endWordPosition="9712" position="60273" startWordPosition="9711"> the presence of such a verb form does not necessarily signal a shift: the reference time of the sentence may remain the same as, rather than precede, the current TF. (Section 7 illustrates; see also Almeida 1987 and Webber 1987b, this volume.) 6.3 SPATIAL DISCONTINUITIES The only purely spatial discontinuities I have looked at are discontinuities of scale. Just as h-types have time scales associated with the</context>
P06-1064==>P03-1013!=<context citStr="Dubey and Keller (2003)" endWordPosition="508" position="3277" startWordPosition="505">tion, which uses flat phrase structure trees instead of the crossing dependency structures in the original corpus. This version has been used by Cahill et al. (2005) to extract a German LFG. However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approx</context>
A00-1005==>J99-3003!=<context citStr="Chu-Carroll and Carpenter (1999)" endWordPosition="700" position="4468" startWordPosition="697">either the option number or a keyword from a list of options/descriptions. However, the only known work which automates part of a customer service center using natural language dialogue is the one by Chu-Carroll and Carpenter (1999). The system described here is used as the front-end of a bank's customer service center. It routes calls by extracting key phrases from a user utterance and then by statistically comparing these phra</context>
C02-1118==>W00-1325!=<context citStr="Briscoe and Carroll (1997)" endWordPosition="1049" position="6773" startWordPosition="1046">for the most common 40 with a hand-made dictionary he achieved a precision of 90% and recall of 43%. All three authors suppose the set of possible categories is relatively small and known in advance. Briscoe and Carroll (1997) also have a predefined set but it contains 160 different frames. Gahl (1998) presents an RE-based extraction tool that creates subcorpora of the BNC containing different subcategorization frames for </context>
H05-1085==>J03-1002!=<context citStr="Och and Ney, 2003" endWordPosition="2882" position="18009" startWordPosition="2879"> all of our experiments, we used the same language model, trained with the CMU Statistical Language Modelling Toolkit (Clarkson and Rosenfeld, 1997). Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modi'Although we did not use it for the experiments in this paper, the PCEDT corpus does contain lemma information for the English data. There is a slight discrepancy between the English an</context>
H90-1027==>H89-2037!=<context citStr="[3]" endWordPosition="1066" position="6492" startWordPosition="1066">ed results are displayed to the user. Results Our current system has a 484 word vocabulary and a word pair grammar with perplexity 85. We use the Vocabulary-independent phone models generated by Hon. [3] We have not yet added the non-verbal and out-ofvocabulary models to the system. The only technique currently used to cope with spontaneous speech is a word pair grammar and a flexible parser. Structu</context>
W02-1001==>P02-1034!=<context citStr="Collins and Duffy 2002" endWordPosition="498" position="3283" startWordPosition="495">pplicable to a wide variety of models where Viterbi-style algorithms can be used for decoding: examples are Probabilistic Context-Free Grammars, or ME models for parsing. See (Collins and Duffy 2001; Collins and Duffy 2002; Collins 2002) for other applications of the voted perceptron to NLP problems.l 2 Parameter Estimation 2.1 HMM Taggers In this section, as a motivating example, we describe a special case of the algo</context>
J93-4001==>C86-1018!=<context citStr="Hasida 1986" endWordPosition="3410" position="22064" startWordPosition="3409"> This is what makes a context-free chart parser polynomial instead of exponential. There are also several disjunctive unification algorithms that exploit independence, such as constraint unification (Hasida 1986; Nakano 1991), contexted unification (Maxwell and Kaplan 1989), and unification based on disjunctive feature logic (Dorm and Eisele 1990). We say that a system of constraints is in free-choice form i</context>
J93-4001==>C86-1018!=<context citStr="Hasida 1986" endWordPosition="7644" position="48323" startWordPosition="7643"> equality. In recent years there has been a considerable amount of research devoted to the development of unification algorithms that perform well when confronted with disjunctive constraint systems (Hasida 1986; Maxwell and Kaplan 1989; Done and Eisele 1990; Nakano 1991). Some of these unifiers take advantage of the same properties of constraint systems that we have discussed in this paper. For example, Kas</context>
P06-2028==>J94-2001!=<context citStr="Merialdo, 1994" endWordPosition="524" position="3258" startWordPosition="523">engine used to perform the tagging in these experiments is a direct descendent of the maximum entropy (ME) tagger of (Ratnaparkhi, 1996) which in turn is related to the taggers of (Kupiec, 1992) and (Merialdo, 1994). The ME approach is well-suited to this kind of labeling because it allows the use of a wide variety of features without the necessity to explicitly model the interactions between them. The literatur</context>
P06-2028==>J94-2001!=<context citStr="Merialdo, 1994" endWordPosition="1972" position="11853" startWordPosition="1971">ces 5.1 Lexical Dependencies Features derived from n-grams of words and tags in the immediate vicinity of the word being tagged have underpinned the world of POS tagging for many years (Kupiec, 1992; Merialdo, 1994; Ratnaparkhi, 1996), and have proven to be useful features in WSD (Yarowsky, 1993). Lower-order n-grams which are closer to word being tagged offer the greatest predictive power (Black et al., 1998).</context>
P04-1042==>A97-1014!=<context citStr="Skut et al., 1997" endWordPosition="939" position="6294" startWordPosition="936">n. 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English (WSJ) and the context-free version of the NEGRA (version 2) corpus of German (Skut et al., 1997b). Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training, 24 for development, and trees whose yield is under 100 words from section 23 for testing. Experime</context>
P04-1042==>A97-1014!=<context citStr="Skut et al. (1997" endWordPosition="1455" position="9523" startWordPosition="1452">d in WSJ annotation guidelines). We corrected these errors manually before model testing and training. 3For a detailed description of the algorithm for creating the context-free version of NEGRA, see Skut et al. (1997a). S NP-3 NNP Farmers VP S quick*ICH*-2 S-2 NP *-3 TO VP VP NP NN yesterday NP NP DT NN the problems to VB point PRT RP out 0 S NP PRP it VBZ sees VP NP *T*-1 SBAR WHNP-1 VBD ADJP was JJ . . The RMV</context>
A97-1041==>A94-1002!=<context citStr="Kukich et al., 1994" endWordPosition="2811" position="17798" startWordPosition="2808">nflicts with graphics durations arise (see (Pan and McKeown, 1996) for details). 4 Related Work There is considerable interest in producing fluent and concise sentences. EPICURE (Dale, 1992), PLANDoc(Kukich et al., 1994; Shaw, 1995), and systems developed by Dalianis and Hovy (Dalianis and Hovy, 1993) all use various forms of conjunction and ellipsis to generate more concise sentences. In (Horacek, 1992) aggregation</context>
N06-4009==>W04-3102!=<context citStr="Koike and Takagi (2004)" endWordPosition="744" position="4531" startWordPosition="741">h field. This is the model for several existing tools, including the VisDic viewer for WordNet (Horak and Smrz, 2004), the INOH ontology viewer (INOH, 2004), and the Gene Ontology viewer presented by Koike and Takagi (2004), among others. SconeEdit improves on this browsing paradigm by giving a user who is unfamiliar with the knowledge base an easy way to start exploring. Rather than generating a series of guesses at wh</context>
P04-1038==>J02-3001!=<context citStr="Gildea and Jurafsky, 2002" endWordPosition="396" position="2748" startWordPosition="393">ly labeled lexicon and determining lexical choice in machine translation (Rooth et al., 1998), automatic acquisition of verb semantic classes (Schulte im Walde, 2000) and automatic semantic labeling (Gildea and Jurafsky, 2002). In our task, we equipped the EM clustering model with rich linguistic features that capture the predicate-argument structure information of verbs and restricted the feature set for each verb using k</context>
P98-1090==>J93-4004!=<context citStr="Moore and Paris, 1993" endWordPosition="2050" position="12127" startWordPosition="2047">f sentence 3. (We have employed the set of rhetorical relations currently used to analyse the ILEX data.) The relation between G&amp;S's and RST'S notion of structure has been analysed by, among others, (Moore and Paris, 1993; Moser and Moore, 1996). According to Moser and Moore, the relation can be characterised as follows: an RST nucleus expresses an intention In; a satellite expresses an intention Is; and In dominates </context>
P89-1021==>P81-1028!=<context citStr="Moore 1981" endWordPosition="2369" position="14773" startWordPosition="2368">y 1988) for descriptions of similar systems). At each level queries are represented by logic-based formulas (see (Olawsky 1989) for examples) with generalized quantifiers ((Barwise and Cooper 1981), (Moore 1981) and (Pereira 1983)) using predicates defined for that level. The initial level is based on often ambiguous Englishoriented predicates. At the other end is a description of the query in unambiguous da</context>
P98-2199==>W96-0403!=<context citStr="Huang and Fiedler, 1996" endWordPosition="1042" position="6750" startWordPosition="1039">xical aggregation, such as transforming propositions into modifiers (adjectives, prepositional phrases, or relative clauses), in a sentence planner (Scott and de Souza, 1990; Dalianis and Hovy, 1993; Huang and Fiedler, 1996; Callaway and Lester, 1997; Shaw, 1998). Though other systems have implemented coordination, their aggregation rules only handle simple conjunction inside a syntactic structure, such as subject, obje</context>
H05-1098==>P04-1077!=<context citStr="Lin and Och, 2004" endWordPosition="2076" position="12364" startWordPosition="2073">er the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed</context>
C00-2133==>E99-1006!=<context citStr="Eckert and Strube (1999)" endWordPosition="845" position="5451" startWordPosition="842">trative pronoun. Pronoun resolution algorithms tend not to cover demonstratives. Notable exceptions are Webbers model for discourse deixis (Webber, 1991) and the model developed for spoken dialog by Eckert and Strube (1999). This algorithm encompasses both personal and demonstrative pronouns and exploits their contrastive usage patterns, relying on syntactic clues and verb subcategorizations as input. Neither study inve</context>
W96-0109==>J94-4002!=<context citStr="Lappin and Leass, 1994" endWordPosition="235" position="1549" startWordPosition="232"> its subject or theme. In the past, the problem has been addressed mostly by computational linguists in relation to issues like coreference (Hobbs, 1978), anaphora resolution (Grosz and Sidner, 1986; Lappin and Leass, 1994), or discourse center (Joshi and Weinstein, 1981; Walker et al., 1994). In information retrieval, predicting important terms in document is crucial for an effective retrieval of relevant documents(Sal</context>
W02-1605==>C96-1030!=<context citStr="[3, 6, 11]" endWordPosition="202" position="1471" startWordPosition="200"> give accuracy only 48%. Introduction Machine translation has been developed for many decades. Many approaches have been proposed such as rule-based, statistic-based [5], and example-based approaches [3, 6, 11]. However, there is no machine learning technique that meets humans requirement. Each technique has its own advantages and disadvantages. Statistic-based, example-based and corpus-based approaches we</context>
W99-0313==>J86-3001!=<context citStr="Grosz and Sidner, 1986" endWordPosition="718" position="4707" startWordPosition="715">ed at a meso level of granularity, and used non-hierarchical (and possibly discontinuous) utterance sets as its structuring principle. The second scheme concerned intentional/informational structure (Grosz and Sidner, 1986; Nakatani et al., 1995) as content, operated at a macro level of granularity, and was structured as hierarchical trees (with annotations for capturing discontinuities). In addition, these two schemes</context>
W99-0313==>J86-3001!=<context citStr="Grosz and Sidner, 1986" endWordPosition="2335" position="14676" startWordPosition="2331"> the lowest level of macro-structure as I-UNITS (Ills), where &amp;quot;I&amp;quot; stands for either informational or intentional. III trees are created by identifying certain kinds of discourse relations. Following (Grosz and Sidner, 1986), macro-level analysis captures two fundamental intentional relations between I-units, those of domination (or parent-child) and satisfactionprecedence (or sibling) relations. The corresponding inform</context>
W06-1617==>W05-0620!=<context citStr="Carreras and Marquez, 2005" endWordPosition="170" position="1095" startWordPosition="167"> Introduction Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. A successful PropBank-based SRL system would correctly determine that Ben Bernanke is the subje</context>
J94-4006==>P92-1005!=<context citStr="Alshawi and Crouch 1992" endWordPosition="3017" position="17824" startWordPosition="3014">ing our logical form routines with Tomita's parser. The conclusions we draw can also be applied to more efficient parsers (Earley 1970; Schabes 1991) that produce other logical representations (e.g., Alshawi and Crouch 1992; Hirst 1987; Weischedel 1989) in more compact forests (Nederhof 1993). 3. Combining Logical Form with Forests: A Case Study Previously, our logical form routines were interfaced with a one-parse-tree</context>
H94-1040==>H91-1013!=<context citStr="[9]" endWordPosition="161" position="1078" startWordPosition="161">ontaining components for both speech recognition and language processing. An immediate problem is the nature of the interface between the two. A popular solution has been the N-best listfor example, [9]; for some N, the speech recognizer hands the language processor the N utterance hypotheses it considers most plausible. The recognizer chooses the hypotheses on the basis of the acoustic information </context>
W97-0613==>H94-1016!=<context citStr="Schwartz et al., 1994" endWordPosition="785" position="4971" startWordPosition="782">iments contained approximately 40 million words, and researchers using this database have indicated that their speech systems work better when they were able to double the size of their training set (Schwartz et al., 1994). While the recognition achieved on the WSJ with this technique is impressive, the information embodied in the statistical model is so specific there is not much &amp;quot;transfer&amp;quot; to recognizing text that va</context>
W06-0206==>P05-1051!=<context citStr="Ji and Grishman, 2005" endWordPosition="1329" position="8342" startWordPosition="1326">development test set) and reject those which do not help; and 3) apply the latest updated best model to each subsequent 1 We have also used this metric in the context of rescoring of name hypotheses (Ji and Grishman, 2005); Scheffer et al. (2001) used a similar metric for active learning of name tags. 49 segment. The procedure can be formalized as follows. 1. Select a related set RelatedC from a large corpus of unlabel</context>
W05-1001==>N04-1032!=<context citStr="Sun and Jurafsky (2004)" endWordPosition="1199" position="7536" startWordPosition="1196">on and 50% recall. With chunking only, performance further degraded to below 30%. Problems mostly arise from arguments which correspond to more than one chunk, and the misplacement of core arguments. Sun and Jurafsky (2004) also reported a drop in F-score with automatic syntactic parses compared to perfect parses for role labelling in Chinese, despite the comparatively good results of their parser (i.e. the Collins pars</context>
W03-1902==>P01-1040!=<context citStr="Ide and Romary, 2001" endWordPosition="352" position="2314" startWordPosition="349"> that we dealt with. In Section 3 we give an overview of the work in COMMOn-REFs. Section 4 relates our current model with the standards recently proposed (Ide and Romary, 2002; Ide and Romary, 2003; Ide and Romary, 2001). Section 5 describes our tool for coreference resolution. A discussion on the problems we face with our annotation model is presented in Section 6. 2 Previous work Our first annotation schemes were P</context>
W03-1902==>P01-1040!=<context citStr="Ide and Romary, 2001" endWordPosition="954" position="6446" startWordPosition="951">as detailed in Section 5. As we are interested in having our resources made available, we relate our annotation schemes to standard proposals presented in (Ide and Romary, 2002; Ide and Romary, 2003; Ide and Romary, 2001). 3The MMAX tool is available for download in http://www.eml.org/english/Research/NLP/Downloads. &lt;struct type=&amp;quot;CRAnnot&amp;quot;&gt; ... &lt;struct id=&amp;quot;m1&amp;quot; type=&amp;quot;C-level&amp;quot;&gt; &lt;feat type=&amp;quot;coref&amp;quot;&gt;no&lt;/feat&gt; &lt;feat type=&amp;quot;cl</context>
W03-1902==>P01-1040!=<context citStr="Ide and Romary, 2001" endWordPosition="1078" position="7768" startWordPosition="1075">&amp;quot;word_11&amp;quot;&gt;.&lt;/word&gt; &lt;/words&gt; Figure 4: Words file. 4 Data model 4.1 Encoding standards Directions for standard corpus encoding in XML have been proposed in (Ide and Romary, 2002; Ide and Romary, 2003; Ide and Romary, 2001). Such efforts consist on defining abstract formats for corpus annotation that could be instantiated according to specific project requirements. An abstract XML file can be generated for each annotati</context>
C94-1025==>A88-1019!=<context citStr="Church, 1988" endWordPosition="244" position="1683" startWordPosition="243">gs (cf. sec. 2). Starting point for the iinplementation of the feature structure tagger was a second-order-II MM tagger (trigrams) based on a modified version of the Viterbi algorithm (Viterbi, 1907; Church, 1988) which we had earlier implemented in C (Kempe ,1994). There we modified the calculus of the contextual probabilities of the tags in the above-described way (cf sec. 4). A test of both taggers under th</context>
C94-1025==>A88-1019!=<context citStr="Church, 1988" endWordPosition="404" position="2601" startWordPosition="403">THEMATICAL BACKGROUND In order to assign tags to a word sequence, a IIMM can be used where the tagger selects among all possible tag sequences the most probable one (Garside, Leech and Sampson, 1987; Church, 1988; Brown et al., 1989; Rabiner, 1990). The joint probability of a tag sequence I = /0...1N-1 given a word sequence w = w11WN--1 in the case of a second order IIMM: P(Irth) 11 ti  P(W011))  P(1111111</context>
C94-1025==>A88-1019!=<context citStr="Church, 1988" endWordPosition="2461" position="14471" startWordPosition="2460"> 4 TAGGING ALGORITHM Starting point for the implementation of a feature structure tagger was a second-order-IIMM tagger (trigrams) based on a modified version of the Viterbi algorithm (Viterbi, 1967; Church, 1988) which we had earlier implemented in C (Kemp 4994). There we replaced the function which estimated the contextual probability of a tag (state transition probability) by dividing a trigram frequency b</context>
C04-1060==>P02-1050!=<context citStr="Hwa et al. (2002)" endWordPosition="2045" position="12209" startWordPosition="2042"> 25 words in either language, for a total of 788 English words and 580 Chinese words. A separate development set of 49 sentence pairs was used to control overfitting. These sets were the data used by Hwa et al. (2002). The hand aligned test data consisted of 745 individual aligned word pairs. Words could be aligned oneto-many in either direction. This limits the performance achievable by our models; the IBM models</context>
P00-1042==>A97-1029!=<context citStr="Bikel et al., 1997" endWordPosition="4087" position="23577" startWordPosition="4084">8), and a transformation-based error-driven learning model (Aberdeen et al., 1995) have been proposed so far. In the MUG competition, the highest accuracy has been achieved by a system called Nymble (Bikel et al., 1997) which is based on an HMM. This system extracts NEs by applying the following procedure. A finite-state transition network is prepared. Each state of the network represents an NE defined in the MUGNE </context>
P06-2068==>N06-1049!=<context citStr="Lin and Demner-Fushman, 2006" endWordPosition="2480" position="15992" startWordPosition="2477">the goodness of a particular sentence in terms of nugget content.1 Due to known issues with the vital/okay distinction (Hildebrandt et al., 2004), it was ignored for this computation; however, see (Lin and Demner-Fushman, 2006b) for recent attempts to address this issue. When presented with a test question, the system ranked all sentences from the top ten retrieved documents using the regression model. Answers were generat</context>
P06-2068==>N06-1049!=<context citStr="Lin and Demner-Fushman, 2006" endWordPosition="4516" position="29021" startWordPosition="4513">ms, but undoubtedly performance can be gained by better query formulation strategies. These are difficult challenges, but recent work on applying semantic models to QA (Narayanan and Harabagiu, 2004; Lin and Demner-Fushman, 2006a) provide a promising direction. While our formulation of answering relationship questions as sentence retrieval is productive, it clearly has limitations. The assumption that information nuggets do </context>
P98-2251==>P96-1043!=<context citStr="Mikheev, 1996" endWordPosition="378" position="2401" startWordPosition="377">od creates a probability distribution for an unknown word based on certain features: word endings, hyphenation, and capitalization. The features to be used are chosen by hand for the system. Mikheev (Mikheev, 1996; Mikheev, 1997) uses a general purpose lexicon to learn affix and word ending information to be used in tagging unknown words. His work returns a set of possible tags for unknown words, with no proba</context>
N04-4015==>P03-1051!=<context citStr="Lee et al. 2003" endWordPosition="422" position="3013" startWordPosition="419">bic to accomplish a syntactic as well as morphological symmetry between the two languages. 2. Word Segmentation We pre-suppose segmentation of a word into prefix(es)-stem-suffix(es), as described in (Lee et al. 2003) The category prefix and suffix encompasses function words such as conjunction markers, prepositions, pronouns, determiners and all inflectional morphemes of the language. If a word token contains mor</context>
J05-4004==>J93-2003!=<context citStr="Brown et al. 1993" endWordPosition="1458" position="9749" startWordPosition="1455">e generation, simple statistical models are used for aligning documents and headlines (Banko, Mittal, and Witbrock 2000; Berger and Mittal 2000; Schwartz, Zajic, and Dorr 2002), based on IBM Model 1 (Brown et al. 1993). These models treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines. Such mappings can be</context>
J05-4004==>J93-2003!=<context citStr="Brown et al. 1993" endWordPosition="8764" position="54265" startWordPosition="8761">and Paste model only on the extracts.9 5.1.1 Machine Translation Models. We compare against several competing systems, the first of which is based on the original IBM Model 4 for machine translation (Brown et al. 1993) and the HMM machine translation alignment model (Vogel, Ney, and Tillmann 1996) as implemented in the GIZA++ package (Och and Ney 2003). We modified the code slightly to allow for longer inputs and h</context>
J05-4004==>J93-2003!=<context citStr="Brown et al. 1993" endWordPosition="10896" position="67368" startWordPosition="10893">nstrated that our model is able to learn the complex structure of (document, abstract) pairs. Our system outperforms competing approaches, including the standard machine translation alignment models (Brown et al. 1993; Vogel, Ney, and Tillmann 1996) and the state-of-the-art Cut and Paste summary alignment technique (Jing 2002). We have analyzed two sources of error in our model, including issues of nullgenerated s</context>
J05-4004==>J93-2003!=<context citStr="Brown et al. 1993" endWordPosition="11144" position="68890" startWordPosition="11141">believe that there is room for improvement computationally, as well. One obvious first approach would be to run a simpler model for the first iteration (for example, Model 1 from machine translation (Brown et al. 1993), which tends to be very recall oriented) and use this to see subsequent iterations of the more complex model. By doing so, one could recreate the extracts at each iteration using the previous iterati</context>
C96-2106==>C94-2144!=<context citStr="Krieger and Schafer, 1994" endWordPosition="858" position="5612" startWordPosition="855">ance, 2. achieving incremental and interactive behaviour, 3. minimizing the overhead in communication between the processors. We used a mid-size HPSG-kind German grammar written in the I'M formalism (Krieger and Schafer, 1994). The grammar cospecifies syntax and semantics in the attributes SYN and SEM. A simplified example is shown in the lexical entry for the verb come. in Fig. 1. In the following section, we start with a</context>
W04-0915==>P03-2017!=<context citStr="Dymetman et al., 2003" endWordPosition="2503" position="15898" startWordPosition="2500">fortunately unlikely to yield good results, even if linguistically-oriented techniques can improve accuracy (Arampatzis et al., 2000). We have advocated an interactive approach to text understanding (Dymetman et al., 2003) where the input text is used as a source of information to assist the user in re-authoring its content. Following fuzzy inverted generation, an interactive negotiation can take place between the syst</context>
C04-1030==>J97-3002!=<context citStr="Wu, 1997" endWordPosition="1629" position="9895" startWordPosition="1628">ion of two consecutive blocks. on setting k = 0 results in a search algorithm that is monotone at the phrase level. trg 3.2 ITG Constraints In this section, we describe the ITG constraints (Wu, 1995; Wu, 1997). Here, we interpret the input sentence as a sequence of blocks. In the beginning, each alignment template is a block of its own. Then, the reordering process can be interpreted as follows: we select </context>
W05-1008==>W00-1427!=<context citStr="Minnen et al., 2000" endWordPosition="3106" position="18911" startWordPosition="3103"> part-of-speech (POS) tagging. For our purposes, we use a Penn treebank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001), and further lemmatise the output of the tagger using morph (Minnen et al., 2000). 8Note that we will have less than 50 feature instances for some feature types, e.g. the POS tag of the target word, given that the combined size of the Penn POS tagset is 36 elements (not including </context>
J95-2002==>P92-1017!=<context citStr="Pereira and Schabes (1992)" endWordPosition="10943" position="63740" startWordPosition="10940">Unfortunately, it seems that in the case of unconstrained SCFG estimation local maxima present a very real problem, and make success dependent on chance and initial conditions (Lan i and Young 1990). Pereira and Schabes (1992) showed that partially bracketed input samples can alleviate the problem in certain cases. The bracketing information constrains the parse of the inputs, and therefore the parameter estimates, steerin</context>
W97-0106==>W96-0114!=<context citStr="[Per92]" endWordPosition="4395" position="27084" startWordPosition="4395">xt-free grammar. In this research the acquired grammar is evaluated based on its entropy or perplexity where the accuracy of parsing is not taken into account. As another research, Pereira and Schabes[Per92][Sch93] proposed a. modified method to infer a. stochastic grammar from a partially parsed corpus and evaluated the results with a bracketed corpus. This approach gained up to around 90 % bracketing r</context>
N06-1027==>H05-1033!=<context citStr="Sporleder and Lapata, 2005" endWordPosition="1105" position="7288" startWordPosition="1102">pplications in sentence compression and summarization. Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003) or the intermediate step (Sporleder and Lapata, 2005). Analyzing and utilizing discourse information at a higher level, e.g., at the paragraph level, still remains a challenge to the natural language community. In our work, we utilize the discourse info</context>
W02-0905==>J93-2002!=<context citStr="Brent, 1993" endWordPosition="169" position="1262" startWordPosition="168">syntactic ambiguity. Over the last years various methods for acquiring subcategorisation information from corpora has been proposed. Some of them induce syntactic subcategorisation from tagged texts (Brent, 1993; Briscoe and Carrol, 1997; Marques, 2000). Unfortunately, syntactic information is not enough to solve structural ambiguity. Consider the following verbal phrases: (1) [peel [ the potato] [ with a kn</context>
P91-1053==>P83-1017!=<context citStr="[8, 11]" endWordPosition="99" position="706" startWordPosition="98">ment has followed this wisdom, and much effort has been focused on formulating structural and lexical strategies for resolving noun-phrase and verb-phrase (NP-PP vs. VP-PP) attachment ambiguity (e.g. [8, 11]). In one study, statistical analysis of the distribution of lexical items in a very large text yielded 78% correct parses while two humans achieved just 85%[5]. The close performance of machine and h</context>
P91-1053==>P83-1017!=<context citStr="[11]" endWordPosition="262" position="1739" startWordPosition="262">s discourse-level strategies by arguing that a certain PP-attachment ambiguity, sentential vs. verb-phrase (S-PP vs. VP-PP), reflects a third kind of relation that is pragmatic in nature. As noted in [11], context-dependent preferences cannot be computed a priori, so pragmatic PP-attachment ambiguities are among those that defy structural and lexical rules for disambiguation. Another criticism aimed a</context>
E03-2007==>H93-1052!=<context citStr="Yarowsky, 1993" endWordPosition="909" position="5450" startWordPosition="908">. This inventory may be drawn from the user's knowledge, from a perusal of the word sketch, or from a pre-existing dictionary entry. As Table 2 shows, and in keeping with &amp;quot;one sense per collocation&amp;quot; (Yarowsky, 1993) in most cases, high-salience patterns or clues indicate just one of the word's senses. The user then has the task of associating, by selecting from a pop-up menu, the required sense for unambiguous c</context>
J97-1006==>P88-1015!=<context citStr="Whittaker and Stenton (1988)" endWordPosition="2675" position="17457" startWordPosition="2672">o fix the level of initiative for the duration of a session. We next review the work of others who have examined issues in mixed-initiative interaction. 2.3 Prior Theoretical Work on Mixed-Initiative Whittaker and Stenton (1988) propose a definition for dialogue control based on the utterance type of the speaker (question, assertion, command, or prompt) as follows:  Question: The speaker has control unless the question dire</context>
J97-1006==>P88-1015!=<context citStr="Whittaker and Stenton (1988)" endWordPosition="10852" position="68464" startWordPosition="10849">of utterances between control shifts was greater by 21.2 utterances in directive mode over declarative mode. These results show that there is a relationship between our notion of task control and the Whittaker and Stenton (1988) notion of linguistic control evaluated by Walker and Whittaker (1990)namely, that as users exploit their task expertise, linguistic control shifts occur much more frequently. This result may prove u</context>
P06-2074==>C02-1151!=<context citStr="Roth and Yih (2002)" endWordPosition="1410" position="9198" startWordPosition="1407">implies that human experts have to spend long hours to annotate a sufficiently large amount of training corpus. Several recent researches focused on the extraction of relationships using classifiers. Roth and Yih (2002) learned the entities and relations together. The joint learning improves the performance of NE recognition in cases such as X killed Y. It also prevents the propagation of mistakes in NE extraction</context>
W01-0701==>W99-0621!=<context citStr="Sang and Buchholz, 2000" endWordPosition="2392" position="14710" startWordPosition="2389">irst one in its chunk, or somewhere in the middle (&amp;quot;B&amp;quot; and &amp;quot;I&amp;quot;). Table 2 shows an example sentence with POS and text chunk tags. Text chunking was featured as the shared task at CoNLL 2000 (Tjong Kim Sang and Buchholz, 2000); the training set consisted of the sections 15-18 of the Penn Treebank, and as test the section 20 of the same corpus was selected. The task attracted several participants; the best individual system</context>
W03-1017==>J95-4004!=<context citStr="Brill, 1995" endWordPosition="2476" position="15799" startWordPosition="2475"> ADJ within a sentence, where Freqall POS ADJ represents the collocation frequency of all wordsall of part of speech POS with ADJ and is a smoothing constant ( in our case). We used Brills tagger (Brill, 1995) to obtain part-of-speech information. 5.2 Sentence Polarity Tagging As our measure of semantic orientation across an entire sentence we used the average per word loglikelihood scores defined in the p</context>
C02-1169==>P98-1035!=<context citStr="Chelba and Jelinek 1998" endWordPosition="1230" position="7809" startWordPosition="1226"> by concentrating on the local dependencies between words. However, our experiments have shown that N-grams are insufficient for the recognition of spoken question words. Possible alternatives (e.g. (Chelba and Jelinek 1998), (Kuhn and de Mori 1992), (Lafferty et al. 1992) (Lau et al. 1992), (Jardino 1996) and (Bahl et al. 1989)) emphasize syntactic dependencies whereas experiments in opendomain textual Q&amp;A have shown th</context>
W06-1406==>P99-1004!=<context citStr="Lee, 1999" endWordPosition="868" position="5254" startWordPosition="867">es, and the meaning of grammatical relations among them, is related to the restriction of combinations of these entities relative to other entities. Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al., 2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. Similarity means that words with similar meaning tend to appear in simil</context>
W01-0701==>J96-3004!=NO CONTEXT
H90-1008==>J88-2002!=<context citStr="[6]" endWordPosition="939" position="5830" startWordPosition="939">worlds models with multiple time indices). The description of 0, however, is often very informal or limited to a very restricted set of constructions. Furthermore, the formalized accounts (e.g., [4], [6], [14]) tend to take context for granted  the focus is on truth conditions, and so the model is simply assumed to supply the needed reference times. AI-motivated work on tense and aspect has emphasiz</context>
C96-2109==>J92-3001!=<context citStr="Andry et al., 1992" endWordPosition="218" position="1491" startWordPosition="215">and derivational morphology (Gazdar, 1992; Kilbury, 1992; Corbett and Fraser, 1993), lexical semantics (Kilgariff, 1993), morphonology (Cahill, 1993), prosody (Gibbon and Bleiching, 1991) and speech (Andry et al., 1992). In more recent work, DATR has been used to provide a concise, inheritance-based encoding of Lexicalized Tree Adjoining Grammar (Evans et al., 1995). There are around a dozen different implementation</context>
W03-1024==>P86-1031!=<context citStr="Kameyama, 1986" endWordPosition="420" position="2677" startWordPosition="419">n, which is shortened for simplicity to zero. Most studies on Japanese zero pronoun resolution have not tried to resolve zeros in full-text newspaper articles. They have discussed simple sentenses (Kameyama, 1986; Walker et al., 1994; YamuraTakei et al., 2002), dialogues (Yamamoto et al., 1997), stereotypical lead sentences of newspaper articles (Nakaiwa and Ikehara, 1993), intrasentential resolution (Nakaiwa</context>
W03-1024==>P86-1031!=<context citStr="Kameyama (1986)" endWordPosition="508" position="3286" startWordPosition="507">rning ap1http://trec.nist.gov/data/qa.html proach. The Centering Theory (Grosz et al., 1995) is important in the heuristic approach. Walker et al. (1994) proposed forward center ranking for Japanese. Kameyama (1986) emphasized the importance of a property-sharing constraint. Okumura and Tamura (1996) experimented on the roles of conjunctive postpositions in complex sentences. However, these improvements are not </context>
W03-1024==>P86-1031!=<context citStr="Kameyama, 1986" endWordPosition="1810" position="11145" startWordPosition="1809">tics have been reported in past literature. Here, we use the following heuristics. 1. Forward center ranking (Walker et al., 1994): (topic empathy subject object2 object others). 2. Property-sharing (Kameyama, 1986): If a zero is the subject of a verb, its antecedent is perhaps a subject in the antecedents sentence. If a zero is an object, its antecedent is perhaps an object. 3. Semantic constraints (Yamura-Tak</context>
W04-3208==>P95-1050!=<context citStr="Rapp 1995" endWordPosition="684" position="4602" startWordPosition="683">d for word alignment in statistical MT systems. This EM method differs from some previous work, which used a seed-word lexicon to extract new word translations or word senses from comparable corpora (Rapp 1995, Fung &amp; McKeown 1997, Grefenstette 1998, Fung and Lo 1998, Kikui 1999, Kaji 2003). 2. Bilingual Sentence Alignment There have been conflicting definitions of the term comparable corpora in the rese</context>
W04-3208==>P95-1050!=<context citStr="Rapp (1995)" endWordPosition="931" position="6308" startWordPosition="930">ted bilingual documents that are topic-aligned. For example, newspaper articles from two sources in different languages, within the same window of published dates, can constitute a comparable corpus. Rapp (1995), Grefenstette (1998), Fung and Lo (1998), and Kaji (2003) derived bilingual lexicons or word senses from such corpora. Munteanu et al., (2004) constructed a comparable corpus of Arabic and English ne</context>
W04-3208==>N04-1034!=<context citStr="Munteanu et al., 2004" endWordPosition="327" position="2263" startWordPosition="324">documents. Figure1. Parallel sentence and lexicon extraction via Bootstrapping and EM The most challenging task is to extract bilingual sentences and lexicon from very-non-parallel data. Recent work (Munteanu et al., 2004, Zhao and Vogel, 2002) on extracting parallel sentences from comparable data, and others on extracting paraphrasing sentences from monolingual corpora (Barzilay and Elhadad 2003) are based on the fi</context>
W04-3208==>N04-1034!=<context citStr="Munteanu et al., (2004)" endWordPosition="458" position="3168" startWordPosition="455">with roughly similar sentence order of content. This corpus can be more accurately described as noisy parallel corpus. Barzilay and Elhadad (2003) mined paraphrasing sentences from weather reports. Munteanu et al., (2004) used news articles published within the same 5-day window. All these corpora have documents in the same, matching topics. They can be described as on-topic documents. In fact, both Zhao and Vogel (20</context>
W04-3208==>N04-1034!=<context citStr="Munteanu et al., (2004)" endWordPosition="953" position="6451" startWordPosition="950"> same window of published dates, can constitute a comparable corpus. Rapp (1995), Grefenstette (1998), Fung and Lo (1998), and Kaji (2003) derived bilingual lexicons or word senses from such corpora. Munteanu et al., (2004) constructed a comparable corpus of Arabic and English news stories by matching the publishing dates of the articles. Finally, a very-non-parallel corpus is one that contains far more disparate, very-</context>
W04-3208==>N04-1034!=<context citStr="Munteanu et al., (2004)" endWordPosition="1874" position="12335" startWordPosition="1871">ontain more parallel sentences  find-one-get-more 5. Extracting Bilingual Sentences from Very-Non-Parallel Corpora Existing algorithms such as Zhao and Vogel, (2002), Barzilay and Elhadad, (2003), Munteanu et al., (2004) for extracting parallel or paraphrasing sentences from comparable documents, are based on the find-topic-extract-sentence principle which looks for document pairs with high similarities, and then l</context>
W06-3126==>W03-1002!=<context citStr="Schafer and Yarowsky (2003)" endWordPosition="406" position="2602" startWordPosition="403">etween the use of linguistic data views and data sparsity. Fortunately, we hava data enough so that statistical parameter estimation remains reliable. The approach which is closest to ours is that by Schafer and Yarowsky (2003) who suggested a combination of models based on shallow syntactic analysis (part-of-speech tagging and phrase chunking). They followed a backoff strategy in the application of their models. Decoding w</context>
P06-3004==>A00-2018!=<context citStr="Charniak, 2000" endWordPosition="220" position="1447" startWordPosition="219">w parsing models. For the English WSJ, high accuracy parsing models have been created, some of them using extensions to classical PCFG parsing such as lexicalization and markovization (Collins, 1999; Charniak, 2000; Klein and Manning, 2003). However, since most research has been limited to a single language (English) and to a single treebank (WSJ), the question of how portable the parsers and their extensions a</context>
P99-1041==>J94-4003!=<context citStr="Dagan and Itai, 1994" endWordPosition="185" position="1372" startWordPosition="181"> expressions need to be treated differently than other phrases in many statistical or corpus-based NLP methods. For example, an underlying assumption in some word sense disambiguation systems, e.g., (Dagan and Itai, 1994; Li et al., 1995; Lin, 1997), is that if two words occurred in the same context, they are probably similar. Suppose we want to determine the intended meaning of &amp;quot;product&amp;quot; in &amp;quot;hot product&amp;quot;. We can fin</context>
P01-1052==>P98-2181!=<context citStr="Rigau et al., 1998" endWordPosition="698" position="4485" startWordPosition="695">-matching (Chodorow et al., 1985), genus disambiguation (Bruce and Guthrie, 1992), especially constructed definition parsers (Wilks et al., 1996) or broad coverage parsers (Richardson et al., 1998), (Rigau et al., 1998), (ISI, 1998). All those efforts were limited to extracting genus terms, unlabeled or labeled relations, or build taxonomies. 2 LFT Definitions Predicates A predicate is generated for every noun, verb</context>
W05-0634==>W00-0730!=<context citStr="Kudo and Matsumoto, 2000" endWordPosition="696" position="4472" startWordPosition="693">closed task. 2 System Description We again formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifiers. TinySVM1 along with YamCha2 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system. Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained, where n is number of semantic classes inc</context>
P06-2089==>W05-1514!=<context citStr="Tsuruoka and Tsujii, 2005" endWordPosition="302" position="2003" startWordPosition="299">stic parsers are capable of high levels of accuracy, despite great simplicity. This work has led to the development of deterministic parsers for constituent structures as well (Sagae and Lavie, 2005; Tsuruoka and Tsujii, 2005). However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al., 1993) show that the accuracy of these parsers still lags behind the state-of-theart. A reasonable and commonly</context>
P06-2089==>W05-1514!=<context citStr="Tsuruoka and Tsujii (2005)" endWordPosition="383" position="2475" startWordPosition="379">ministic classifier-based parsers can be improved if determinism is abandoned in favor of a search over a larger space of possible parses. While this assumption was shown to be true for the parser of Tsuruoka and Tsujii (2005), only a moderate improvement resulted from the addition of a non-greedy search strategy, and overall parser accuracy was still well below that of state-of-the-art statistical parsers. We present a st</context>
P06-2089==>W05-1514!=<context citStr="Tsuruoka and Tsujii, 2005" endWordPosition="2334" position="13812" startWordPosition="2331"> given parser state. One such approach is maximum entropy classification (Berger et al., 1996), which we use in the form of a library implemented by Tsuruoka1 and used in his classifier-based parser (Tsuruoka and Tsujii, 2005). We used the same classes and the same features as Sagae and Lavie, and an additional feature that represents the previous parser action applied the current parser state (figure 1). 3 Related Work As</context>
P06-2089==>W05-1514!=<context citStr="Tsuruoka and Tsujii (2005)" endWordPosition="2506" position="14893" startWordPosition="2503">g kNN). Yamada and Matsumoto (2003) have also presented a deterministic classifier-based (SVM-based) dependency parser, but using a different parsing algorithm, and using only unlabeled dependencies. Tsuruoka and Tsujii (2005) developed a classifier-based parser that uses the chunk-parsing algorithm and achieves extremely high parsing speed, but somewhat low recall. The algorithm 1The SS MaxEnt library is publicly availabl</context>
P06-2089==>W05-1514!=<context citStr="Tsuruoka and Tsujii, 2005" endWordPosition="3811" position="22552" startWordPosition="3808">terestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). The parser of Tsuruoka and Tsujii (Tsuruoka and Tsujii, 2005) has comparable speed, but we obtain more accurate results. In addition to being fast, our deterministic parser is also lean, requiring only about 25 megabytes of RAM. A summary of these results is sh</context>
W03-1023==>C92-2082!=<context citStr="Hearst, 1992" endWordPosition="639" position="4025" startWordPosition="638">sity, nor does it allow to infer that age is a risk factor. There have been efforts to extract missing lexical relations from corpora in order to build new knowledge sources and enrich existing ones (Hearst, 1992; Berland and Charniak, 1999; Poesio et al., 2002).3 However, the size of the used corpora still leads to data sparseness (Berland and Charniak, 1999) and the extraction procedure can therefore requir</context>
W03-1023==>C92-2082!=<context citStr="Hearst, 1992" endWordPosition="698" position="4441" startWordPosition="697">xtensive smoothing. Moreover, some relations should probably not be encoded in fixed contextindependent ontologies at all. Should, e.g., underspecified and point-of-view dependent hyponymy relations (Hearst, 1992) be included? Should age, for example, be classified as a hyponym of risk factor independent of context? Building on our previous work in (Markert et al., 2003), we instead claim that the Web can be u</context>
W03-1023==>C92-2082!=<context citStr="Hearst, 1992" endWordPosition="2702" position="17180" startWordPosition="2701">rally explicitly express the same lexical relations. E.g., the listcontext NP1 and other NP2 (as Ex. (4)) usually expresses hyponymy/similarity relations between the hyponym NP1 and its hypernym NP2 (Hearst, 1992). 3. If the implicit lexical relationship between anaphor and antecedent is strong, it is likely that anaphor and antecedent also frequently cooccur in the selected explicit patterns. We instantiate t</context>
W03-1023==>C92-2082!=<context citStr="Hearst, 1992" endWordPosition="4410" position="27746" startWordPosition="4409">, (Keller et al., 2002) have shown that using the Web handles data sparseness better than smoothing. Second, we do not process the returned Web pages in any way (tagging, parsing, e.g.), unlike e.g. (Hearst, 1992; Poesio et al., 2002). Third, the linguistically motivated patterns we use reduce long-distance dependencies MIant =log Pr(Xant)Pr(Yant) Pr(Imaxant) between anaphor and antecedent to local dependenci</context>
W06-1670==>W04-0842!=<context citStr="Molina et al., 2004" endWordPosition="2052" position="13216" startWordPosition="2049">. (de Loupy et al., 1998) 596 also investigated the potential advantages of using HMMs for disambiguation. More recently, variants of the generative HMM have been applied to WSD (Molina et al., 2002; Molina et al., 2004) and evaluated also on Senseval data, showing performance comparable to the first sense baseline. Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has fo</context>
W06-1670==>W04-0842!=<context citStr="Molina et al., 2004" endWordPosition="4480" position="27859" startWordPosition="4477"> the baseline, with the two best systems (Mihalcea and Faruque, 2004; Decadt et al., 2004) achieving an F-score of 65.2% (2.8% improvement, 7.45% error reduction). The system based on the HMM tagger (Molina et al., 2004), 6Scoring was performed with a re-implementation of the conlleval script. achieved an F-score of 60.9%. The supersense tagger improves mostly on precision, while also improving on recall. Overall t</context>
E06-1038==>W02-1001!=<context citStr="Collins, 2002" endWordPosition="4146" position="24178" startWordPosition="4145">-5 training epochs performance on the development data was maximized. The final weight vector is the average of all weight vectors throughout training. Averaging has been shown to reduce overfitting (Collins, 2002) as well as reliance on the order of the examples during training. We found it to be particularly important for this data set. 4 Experiments We use the same experimental methodology as Knight and Marc</context>
W06-3801==>H05-1056!=<context citStr="Minkov et al., 2005" endWordPosition="2362" position="13667" startWordPosition="2359">nd Singer, 1999). 5 Evaluation We experiment with three separate corpora. The Cspace corpus contains email messages collected from a management course conducted at Carnegie Mellon University in 1997 (Minkov et al., 2005). In this course, MBA students, organized in teams of four to six members, ran simulated companies in different market scenarios. The corpus we used here includes the emails of all teams over a period</context>
W06-3801==>H05-1056!=<context citStr="Minkov et al., 2005" endWordPosition="2814" position="16410" startWordPosition="2811">-trivial for a human other than the intended recipient. We evaluated this task using three labeled datasets, as detailed in Table 2. The Cspace corpus has been manually annotated with personal names (Minkov et al., 2005). Additionally, with the corpus, there is a great deal of information available about the composition of the individual teams, the way the teams interact, and the full names of the team members. Using</context>
P05-1048==>C04-1030!=NO CONTEXT
A00-2002==>P99-1047!=<context citStr="Marcu (1999)" endWordPosition="3103" position="18824" startWordPosition="3102">uilding the modules of a Discourse-Based Machine Translation system that works along the following lines. 1. A discourse parser, such as those described by Sumita et al. (1992), Kurohashi (1994), and Marcu (1999), initially derives the discourse structure of the text given as input. 2. A discourse-structure transfer module rewrites the discourse structure of the input text so as to reflect a discourse renderi</context>
A00-2002==>P99-1047!=<context citStr="Marcu (1999)" endWordPosition="3354" position="20302" startWordPosition="3353">l properties similar to those of the trees T. In order to solve the problem in definition 3.1, we extend the shift-reduce parsing paradigm applied by Magerman (1995), Hermjakob and Mooney (1997), and Marcu (1999). In this extended paradigm, the transfer process starts with an empty Stack and an Input List that contains a sequence of elementary discourse trees edts, one edt for each edu in the tree T, given as</context>
A00-2002==>P99-1047!=<context citStr="Marcu, 1999" endWordPosition="3914" position="23641" startWordPosition="3913">nments NUCLEUS-SATELLITE (NS), SATELLITE-NUCLEUS (SN), AND NUCLEUSNUCLEUS (NN), there are two possible ways to reduce two adjacent trees (one results in a binary tree, the other in a non-binary tree (Marcu, 1999)), and 85 relation names.)  three types of BREAK operations; (In our corpus, a Japanese unit is broken into two, three, or at most four units.)  one type of CREATE-NEXT operation;  one type of FUSE</context>
C04-1055==>W04-0214!=<context citStr="Tetreault et al., 2004" endWordPosition="1090" position="6492" startWordPosition="1087">nification grammar of approximately 1300 rules with a rich model of semantic features (Dzikovska, 2004). The parser 2Parseable utterances exclude utterances that are incomplete or ungrammatical (see (Tetreault et al., 2004).) is an agenda-driven best-first chart parser that supports experimentation with different parsing strategies, although in practice we almost always use a straightforward bi-directional bottom-up alg</context>
C02-1068==>A00-2018!=<context citStr="Charniak, 2000" endWordPosition="1951" position="11624" startWordPosition="1950">e method described above. The next two results are for two other nonlexicalised treebank methods (Charniak, 1996; Krotov et al., 2000), and the last two are for the best existing lexicalised results (Charniak, 2000; Collins, 2000). The results highlighted in boldface font are the best individual runs of Partition Search (Ps) for Sections 1 and 20 of WSJC respectively (both obtained with grammar Ply). Results in</context>
C02-1068==>A00-2018!=<context citStr="Charniak (2000)" endWordPosition="2150" position="12773" startWordPosition="2149">ing Grammar BARE (S 1/20) 74.09/73.49 Nonlex.: Krotov et al. (2000) (79.12) 76.09 Charniak (1996) (79.59) - - Best S 1 PS result (PN) (80.54) 78.01 Best S 20 PS result (PN) (79.99) 77.46 Lexicalised: Charniak (2000) - - 90.10 Collins (2000) - - 90.25 The different results are not completely comparable, because the earlier results were obtained training on WSJC Sections 2-21 and testing on Section 23, but given t</context>
P06-1114==>N03-1003!=<context citStr="Barzilay and Lee, 2003" endWordPosition="3509" position="21904" startWordPosition="3506">um Entropy 10K pairs 0.881 0.851 0.866 Maximum Entropy 450K pairs 0.902 0.944 0.922 Table 4: Performance of Alignment Classifier 3.2 Paraphrase Acquisition Much recent work on automatic paraphrasing (Barzilay and Lee, 2003) has used relatively simple statistical techniques to identify text passages that contain the same information from parallel corpora. Since sentence-level paraphrases are generally assumed to contain </context>
P06-1114==>N03-1003!=<context citStr="Barzilay and Lee, 2003" endWordPosition="3686" position="23033" startWordPosition="3683"> likelihood that 909 only true paraphrases were considered as phraselevel alternations for an example, extracted sentences were clustered using complete-link clustering using a technique proposed in (Barzilay and Lee, 2003). 3.3 Creating New Sources of Training Data In order to obtain more training data for our TE system, we extracted more than 200,000 examples of textual entailment from large newswire corpora. Positive</context>
P04-1073==>P01-1052!=<context citStr="Moldovan &amp; Rus, 2001" endWordPosition="646" position="4234" startWordPosition="643">very different from todays systems, however, primarily in that there is no text corpus to process. Inferencing is used in at least two of the more visible systems of the present day. The LCC system (Moldovan &amp; Rus, 2001) uses a Logic Prover to establish the connection between a candidate answer passage and the question. Text terms are converted to logical forms, and the question is treated as a goal which is proven</context>
W06-0101==>N03-1036!=<context citStr="Widdows, 2003" endWordPosition="1887" position="11105" startWordPosition="1886">s and unrelated words. 4.2 Nonlinear interpolated precision The Nap evaluation is used to measure the performance of restoring words to taxonomy, a similar task of restoring words in WordNet (Dominic Widdows, 2003). The way we adopted Nap evaluation is to reconstruct a partial Chinese synonym set, and measure the structure resemblance between original synonyms and the reconstructed one. By doing so, one has to </context>
W97-0405==>P94-1004!=<context citStr="Miller et al., 1994" endWordPosition="2302" position="15302" startWordPosition="2299">f. 4.2 The Noisy Channel Model The &amp;quot;noisy-channel&amp;quot; model from information theory has proven highly effective in speech recognition and, more recently, in language understanding (Epstein et al., 1996; Miller et al., 1994). We adopt this model for translation by analogy in the following manner. Given an input expression, the analogical matching algorithm must determine the example expression that is closest in meaning </context>
P06-1062==>P01-1067!=<context citStr="Yamada and Knight, 2001" endWordPosition="1360" position="8755" startWordPosition="1357">st work discussing DOM tree alignments, there is substantial research focusing on syntactic tree alignment model for machine translation. For example, (Wu 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001) have studied synchronous context free grammar. This formalism requires isomorphic syntax trees for the source sentence and its translation. (Shieber and Schabes 1990) presents a synchronous tree adjo</context>
W01-1309==>P00-1010!=<context citStr="Mani and Wilson (2000)" endWordPosition="143" position="1025" startWordPosition="140"> in the present paper includes dates (e.g. 08.04.2001), prepositional phrases (PPs) containing some time expression (e.g. on Friday), and verbs referring to a situation (e.g. opened). Related work by Mani and Wilson (2000) focuses only on the core temporal expressions neglecting the temporal information conveyed by prepositions (e.g. Friday vs. by Friday). The main part of the system is a temporal expression tagger tha</context>
W01-1309==>P00-1010!=<context citStr="Mani and Wilson (2000)" endWordPosition="3736" position="23085" startWordPosition="3732">ses. Based on the extracted temporal expression chunks the temporal information was derived and evaluated. 5.1 Tagging results First, the class of simple temporal expressions was tagged and analysed. Mani and Wilson (2000) call this class TIMEX expression (of type TIME or DATE). We computed the precision and recall values for our data regarding this type of expressions in order to obtain a better comparability with the</context>
W01-1309==>P00-1010!=<context citStr="Mani and Wilson, 2000" endWordPosition="4139" position="25566" startWordPosition="4136">t of the temporal expressions was defined so that temporal inferences were drawn regarding dates and events described. A more complex set of temporal expressions as extracted by recent systems (e.g. (Mani and Wilson, 2000)) was tagged. Our definition of temporal expressions also includes PPs capturing temporal relations. The system achieved an overall precision rate of 84.49 which is likely to go up as soon as the sema</context>
H05-1108==>J02-3001!=<context citStr="Gildea and Jurafsky, 2002" endWordPosition="269" position="1934" startWordPosition="266">(Burnard, 1995). The availability of rich annotations for the surface realisation of semantic roles has triggered interest in semantic parsing and enabled the development of data-driven models (e.g., Gildea and Jurafsky, 2002). Mirella Lapata School of Informatics University of Edinburgh Edinburgh, UK mlap@inf.ed.ac.uk Frame: DEPARTING Frame Elements THEME The officer left the house. The plane leaves at seven. His departur</context>
P01-1035==>E95-1021!=<context citStr="Chanod and Tapanainen, 1995" endWordPosition="598" position="3807" startWordPosition="595"> (Karlsson et al., 1995). From languages we are acquainted with, the method has been applied on a larger scale only to English (Karlsson et al., 1995), (Samuelsson and Voutilainen, 1997), and French (Chanod and Tapanainen, 1995). Also (Bick, 1996) and (Bick, 2000) use manually written rules for Brazilian Portuguese, and there are several publications by Oflazer for Turkish. Authors of such systems claim that handwritten syst</context>
W97-1105==>J94-3004!=<context citStr="Lowe &amp; Mazaudon, 1994" endWordPosition="561" position="3635" startWordPosition="558">sets and user-defined keystrokes for entering them, and a utility to dump a database into an RTF file in a user-defined lexicon format for use in desktop publishing. 2For example, see (Ellison, 1992; Lowe &amp; Mazaudon, 1994; Coleman, Dirksen, Hussain &amp; Waals, 1996). 33 modules are described in detail. The last two sections describe planned future work and present the conclusions. EXAMPLE This section shows how the syste</context>
P01-1017==>P98-1035!=<context citStr="[4,15]" endWordPosition="2435" position="14085" startWordPosition="2435">ts 4.1 The Immediate-Bihead Language Model The parser as described in the previous section was trained and tested on the data used in the previously described grammar-based language modeling research [4,15]. This data is from the Penn Wall Street Journal tree-bank [13], but modified to make the text more speech-like. In particular: 1. all punctuation is removed, 2. no capitalization is used, 3. all sy</context>
W98-1110==>A92-1021!=<context citStr="Brill, 1992" endWordPosition="563" position="3748" startWordPosition="562">ly consists of one or more stem morphemes and functional morphemes. 85 re-studied to overcome the limitations of statistical approaches by learning symbolic tagging rules automatically from a corpus (Brill, 1992; Brill. 1994). Some systems even perform the POS tagging as part of a syntactic analysis process (Voutilainen, 1995). However, rule-based approaches alone, in general, are not very robust, and not po</context>
W98-1110==>A92-1021!=<context citStr="Brill, 1992" endWordPosition="1885" position="12351" startWordPosition="1884">hing the optimal tag sequence for POS disambiguation. For remeding the defects of a statistical tagger, we introduce a post errorcorrection mechanism. The error-corrector is a rule-based transformer (Brill, 1992), and it corrects the mis-tagged morphemes by considering the lexical patterns and the necessary contextual information. 4.1 Statistical POS tagger Statistical tagging model has the morpheme graph as </context>
W98-1110==>A92-1021!=<context citStr="Brill, 1992" endWordPosition="2765" position="17914" startWordPosition="2764">ion and modeling errors of the statistical morpheme tagging. However, designing the error-correction rules with knowledge engineering is tedious and error-prone. Instead, we adopted Brill's approach (Brill, 1992) to automatically learn the error-correcting rules from small amount of tagged corpus. Fortunately, Brill showed that we don't need a large amount of tagged corpus to extract the symbolic tagging rule</context>
W98-1110==>A92-1021!=<context citStr="Brill, 1992" endWordPosition="3253" position="20963" startWordPosition="3252"> greatly enhance the tagging performance in Korean. The rules are automatically learned by comparing the correctly tagged corpus with the outputs of the statistical tagger. The training is leveraged (Brill, 1992) so the error-correcting rules are gradually learned as the statistical tagged texts are corrected by the rules learned so far. 6 Experiment results For morphological analysis and POS tagging experime</context>
P98-1095==>P88-1015!=<context citStr="Whittaker and Stenton, 1988" endWordPosition="2877" position="18140" startWordPosition="2874">ely its applicability to other domains seems to be promising, which should obviously need further work. 4.3 Patterns of initiative taking The concept 'initiative' is defined by Whittaker and Stenton (Whittaker and Stenton, 1988) using a classification of utterance types assertions, commands, questions and prompts. The initiative was used to analyse behaviour of anaphoric expressions in (Walker and Whittaker, 1990). 3 act seq</context>
P06-1132==>P00-1056!=<context citStr="Och and Ney, 2000" endWordPosition="4376" position="27179" startWordPosition="4373">Japanese sentences from a technical (computer) domain. We used 15,000 sentence pairs for training, 5,000 for development, and 4,241 for testing. The parallel sentences were word-aligned using GIZA++ (Och and Ney, 2000), and submitted to a tree-to-string-based MT system (Quirk et al., 2005) which utilizes the dependency structure of the source language and projects dependency structure to the target language. Figure</context>
P97-1056==>J93-2006!=<context citStr="Weischedel et al., 1993" endWordPosition="3577" position="21316" startWordPosition="3574">alization, numbers, special characters etc.). There is a large number of potentially informative features that could play a role in correctly predicting the tag of an unknown word (Ratnaparkhi, 1996; Weischedel et al., 1993; Daelemans et al., 1996). A priori, it is not clear what the relative importance is of these features. We compared Naive Back-off estimation and MBL with two sets of features:  pDAss: the first lett</context>
J03-3001==>P01-1005!=<context citStr="Banko and Brill (2001)" endWordPosition="2045" position="12444" startWordPosition="2042">antities of data, even if those data are noisy, are better than ones based on estimates (using sophisticated smoothing techniques) from smaller, cleaner data sets. Another argument is made vividly by Banko and Brill (2001). They explore the performance of a number of machine learning algorithms (on a representative disambiguation task) as the size of the training corpus grows from a million to a billion words. All the </context>
P94-1018==>P91-1011!=<context citStr="Hepple 1991" endWordPosition="2700" position="16397" startWordPosition="2698">ticality of a string of categories be viewed as a proof, there have been quite a few proposals put forth for computing only normal forms of derivations or proofs (Konig 1989; Hepple and Morrill 1989; Hepple 1991; inter alia). The basic idea with all of these works is to define 'normal forms'  distinguished members of each equivalence class of derivations, and to require the parser to search this smaller spa</context>
P06-1067==>J93-2003!=<context citStr="Brown et al., 1993" endWordPosition="270" position="1665" startWordPosition="267"> selecting among several candidate word realization of a given acoustic signal. N-gram language models have also been used in Statistical Machine Translation (SMT) as proposed by (Brown et al., 1990; Brown et al., 1993). The run-time search procedure used to find the most likely translation (or transcription in the case of Speech Recognition) is typically referred to as decoding. There is a fundamental difference be</context>
P06-1067==>J93-2003!=<context citStr="Brown et al., 1993" endWordPosition="1408" position="8807" startWordPosition="1405"> This observation is also reported by (Xia and McCord, 2004).We argue that the distortion model we propose leads to a better translation as measured by BLEU. Distortion models were first proposed by (Brown et al., 1993) in the so-called IBM Models. IBM Models 2 and 3 define the distortion parameters in terms of the word positions in the sentence pair, not the actual words at those positions. Distortion probability i</context>
C96-2165==>J89-1003!=<context citStr="McCord 1989" endWordPosition="338" position="2318" startWordPosition="337">ambiguate properly. We can distinguish two classes of approaches: &amp;quot;surfaceoriented&amp;quot; approaches and &amp;quot;inference-based&amp;quot; approaches. Surface-oriented approaches rely on selectional restrictions (cf. e.g. McCord 1989) (sometimes supplied by an external type hierarchy/ontology (e.g. Nirenburg 1989) or are statistical (e.g. Kameyama, Peters, and Schiitze 1993). Although quite useful for some purposes, the performanc</context>
N01-1016==>A00-2018!=<context citStr="[3,5]" endWordPosition="653" position="4021" startWordPosition="653">he training corpus contains the same kind of ungrammatical examples as the testing corpus, one would not expect ungrammaticality itself to be a show stopper. Furthermore, the best statistical parsers [3,5] do not use grammatical rules, but rather define probability distributions over all possible rules. Similarly, parentheticals and filled pauses exist in the newspaper text these parsers currently hand</context>
N01-1016==>A00-2018!=<context citStr="[3]" endWordPosition="802" position="4900" startWordPosition="802">g to detect edited nodes (Sections 2.1  2.2) and an evaluation of the model as a stand-alone edit detector (Section 2.3). Section 3 describes the parser. Since the parser is that already reported in [3], this section simply describes the parsing metrics used (Section 3.1), the details of the experimental setup (Section 3.2), and the results (Section 3.3). 2 Identifying EDITED words The Switchboard c</context>
N01-1016==>A00-2018!=<context citStr="[3,5, 6]" endWordPosition="2774" position="16456" startWordPosition="2773"> only words other than punctuation and filled pauses. Our logic here is much the same as that in the statistical parsing community which ignores the location of punctuation for purposes of evaluation [3,5, 6] on the grounds that its placement is entirely conventional. The same can be said for filled pauses in the switchboard corpus. Our results are given in Table 2. They show that our classifier makes onl</context>
N01-1016==>A00-2018!=<context citStr="[3,5]" endWordPosition="3070" position="18116" startWordPosition="3070"> logical extension of that used to grade previous statistical parsing work. We have taken as our starting point what we call the relaxed labeled precision/recall metric from previous research (e.g. [3,5]). This metric is characterized as follows. For a particular test corpus let N be the total number of nonterminal (and non-preterminal) constituents in the gold standard parses. Let M be the number of</context>
N01-1016==>A00-2018!=<context citStr="[3]" endWordPosition="3772" position="22199" startWordPosition="3772">e parsers output, a state of affairs that might allow complicated schemes to improve the parsers performance as measured by the metric. See Figure 1. 3.2 Parsing experiments The parser described in [3] was trained on the Switchboard training corpus as specified in section 2.1. The input to the training algorithm was the gold standard parses minus all EDITED nodes and their children. We tested on th</context>
A00-2009==>P96-1006!=<context citStr="Ng and Lee, 1996" endWordPosition="3690" position="22069" startWordPosition="3687">locations are recognized as potent sources of disambiguation information. While many other contextual features are often employed, it isn't clear that they offer substantial advantages. For example, (Ng and Lee, 1996) report that local collocations alone achieve 80% accuracy disambiguating interest, while their full set of features result in 87%. Preliminary experiments for this paper used feature sets that includ</context>
W05-1513==>J93-2004!=<context citStr="Marcus et al., 1993" endWordPosition="3179" position="18727" startWordPosition="3176">ort vector machine implementation by Taku Kudo)2, and the memory-based learner TiMBL (Daelemans et al., 2004). We trained and tested the parser on the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1993) using the standard split: sections 2-21 were used for training, section 22 was used for development and tuning of parameters and features, and section 23 was used for testing. Every experiment report</context>
W03-1812==>W03-1810!=<context citStr="McCarthy et al. (2003)" endWordPosition="1534" position="10004" startWordPosition="1531"> classifying verb-particles as being compositional or not. They successfully combined statistical and distributional techniques (including LSA) with a substitution test in analysing compositionality. McCarthy et al. (2003) also targeted verb-particles for a study on compositionality, and judged compositionality according to the degree of overlap in the N most similar words to the verbparticle and head verb, e.g., to de</context>
H05-1038==>N03-2037!=<context citStr="Voorhees, 2003" endWordPosition="2067" position="12241" startWordPosition="2066">for a set of list questions is the mean of the individual questions F scores. 2.3 Other questions The Other questions were evaluated using the same methodology as the TREC 2003 definition questions (Voorhees, 2003). A systems response for an Other question is an unordered set of [doc-id, answer-string] pairs as for list questions. Each string is presumed to be a facet in the definition of the series target th</context>
N06-2051==>P03-1051!=<context citStr="Lee et al., 2003" endWordPosition="323" position="2176" startWordPosition="320">dge the inflectional gap that addresses the issues described above through a series of preprocessing steps based on the Buckwalter Arabic Morphological Analyzer (BAMA) tool (Buckwalter, 2004). While (Lee et al., 2003) develop accurate segmentation models of Arabic surface word forms using manually segmented data, we rely instead on the translated context in the target language, leveraging the manually constructed </context>
W04-3256==>C02-1073!=<context citStr="Saggion et al., 2002" endWordPosition="2622" position="17342" startWordPosition="2619">e the input to the system. Summary length was restricted to 665 bytes. Brute force truncation was applied on longer summaries. The ROUGE-L metric is based on Longest Common Subsequence (LCS) overlap (Saggion et al., 2002). Figure 2 shows that our system (86) performs at an equivalent level with the best systems 9 and 10, that is, they both lie within our systems 95% upper confidence interval. The 2- class classificat</context>
W04-3256==>C02-1073!=<context citStr="Saggion et al., 2002" endWordPosition="3010" position="19500" startWordPosition="3007">to ROUGE-L shown here. While cosine similarity and unigram and bigram overlap demonstrate a sufficient measure on content coverage, they are not sensitive on how information is sequenced in the text (Saggion et al., 2002). In evaluating and analyzing MDS results, metrics, such as ROUGE-L, that consider linguistic sequence are essential. Radev and McKeown (1998) point out when summarizing interesting news events from m</context>
C94-1038==>J91-4003!=<context citStr="Pustejovsky (1991" endWordPosition="820" position="5220" startWordPosition="819">a new generative component that links universal abstract lexical structures with the surface forms of words for each language. This generative machinery is based on work by Hale and Keyser (1993) and Pustejovsky (1991a). The basic architecture is shown in Figure 1. Fig. 1. Generative Syntax of Word Formation Crucially, only a restricted number of argument structures can be generated. The basic idea is that lexical</context>
C94-1038==>J91-4003!=<context citStr="Pustejovsky (1991" endWordPosition="2363" position="14955" startWordPosition="2362"> empty preposition and an empty verb that incorporates a noun. In particular, we show how to replace thematic roles with the lexical syntax proposed in Hale and Keyser (1993) and augmented by work in Pustejovsky (1991a).7 This technique yields several potential benefits: (i) robustness of the lexicon, (ii) greater flexibility in selecting more natural renditions of target language structures in translation, as in </context>
W98-0614==>J91-4003!=<context citStr="Pustejovsky 1991" endWordPosition="636" position="4395" startWordPosition="634">nal approaches are mostly concerned with inferring implicitly expressed metonymic relations in English texts  (Fass 1991), Hobbs (Hobbs et al. 1993) (Lakoff and Johnson 1980), (Nunberg 1993, 1995), (Pustejovsky 1991), and (Wilks 1975) are prominent representatives. Some analyses also consider French (Kayser 1988), (Pustejovsky and, Bouillon 1995) and German (Horacek 1996). In his program met*, (Fass 1991) makes u</context>
W98-0614==>J91-4003!=<context citStr="Pustejovsky 1991" endWordPosition="872" position="5960" startWordPosition="870">nvolved, the real and the literal referent, always appear in singular form. There are only two approaches which in some aspects deviate from this characterization:  Pustejovsky's Generative Lexicon (Pustejovsky 1991) addresses the first aspect. He proposes a Theory of Qualia within a Generative Lexicon, which enables the explanation of systematic polysemy. Applying type coercion enables one to arrive at cases of </context>
C04-1027==>H90-1021!=<context citStr="Doddington and Godfrey, 1990" endWordPosition="357" position="2132" startWordPosition="354">tion decisions. 2 Some background (Pulman, 2000) showed that it was possible to learn a simple domain theory from a disambiguated corpus: a subset of the ATIS (air travel information service) corpus (Doddington and Godfrey, 1990). Ambiguous sentences were annotated as shown to indicate the preferred reading: [i,would,like, [the,cheapest,flight, from,washington,to,atlanta] ] [i,would,like, [a,flight,from,boston, to,san_francis</context>
C00-1057==>C00-2119!=<context citStr="[Suzuki00]" endWordPosition="529" position="3626" startWordPosition="529"> of records is present in its output. It is the function of the parsing component to select the best analysis from this lattice. With this model, our system achieves roughly 97% recall/precision (see [Suzuki00] for more details). 1 System Overview Figure 1 shows a simple block diagram of our Natural Language Understanding system for Japanese, the goal of which is to robustly produce syntactic and logical fo</context>
C00-1057==>C00-2119!=<context citStr="[Suzuki00]" endWordPosition="773" position="5292" startWordPosition="773">formalism. The grammar rules are language-specific while the core engine is shared among 7 languages (Chinese, Japanese, Korean, English, French, German, Spanish). The Japanese parser is described in [Suzuki00]. 2 Recall vs. Precision In this architecture, data is fed forward from one component to the next; hence, it is crucial that the base components (like the segmenter) generate a minimal number of omiss</context>
C00-1057==>C00-2119!=<context citStr="[Suzuki00]" endWordPosition="2759" position="18009" startWordPosition="2759"> coverage (i.e., input strings for which a complete and acceptable sentential parse is obtained), and 97% accuracy for POS labeled breaking accuracy. A full description of these results is given in [Suzuki00]. 7.2 Segmenter Evaluation Three criteria are relevant to segmenter performance: recall, precision and speed. 7.2.1 Recall Analysis of a randomly chosen set of tagged sentences gives a recall of 99.91</context>
C00-1057==>C00-2119!=<context citStr="[Suzuki00]" endWordPosition="2856" position="18610" startWordPosition="2856"> missing verbs/adjs = 15%, orthographic idiosyncrasies = 15%, archaic inflections = 8%. It is worth noting that for derived forms (those that  Tested on a 15,000 sentence blind, balanced corpus. See [Suzuki00] for details. 40% 20% 50% 30% 10% 0% 15 25 35 45 55 65 75 85 95 105 115 Japanese Chinese Figure 3: Characters/second (y-axis) vs. sentence length (x-axis) for segmenter alone (upper curve) and our NL </context>
C02-1041==>P91-1024!=<context citStr="Sumita and Iida, 1991" endWordPosition="1227" position="7722" startWordPosition="1224">prehensive database of all the possible commands a user could request a system to do. It has the same function as the database of parallel translations in an Example-based machine translation system (Sumita and Iida, 1991). 3. The toolkit then creates methods for attaching the SLUI to the back end applications. 4. When the SLUI enabled system is released, a user may enter an NL sentence, which is translated into a sema</context>
N06-2036==>P05-1006!=<context citStr="Dang &amp; Palmer (2005)" endWordPosition="754" position="4819" startWordPosition="751">rithm (Berger et al. 1996) to develop word sense recognition signatures for each lemma which predicts the most likely sense for the lemma according to the context in which the lemma occurs. Following Dang &amp; Palmer (2005) and Kohomban &amp; Lee (2005), Sanfilippo et al. (2006) use contextual, syntactic and semantic information to inform our verb class disambiguation system.  Contextual information includes the verb under</context>
P00-1065==>P99-1001!=<context citStr="Hearst, 1999" endWordPosition="344" position="2263" startWordPosition="343">edge. Semantic roles could also act as an important intermediate representation in statistical machine translation or automatic text summarization and in the emerging field of Text Data Mining (TDM) (Hearst, 1999). Finally, incorporating semantic roles into probabilistic models of language should yield more accurate parsers and better language models for speech recognition. This paper proposes an algorithm for</context>
J05-3004==>M98-1028!=<context citStr="Chinchor 1997" endWordPosition="408" position="2795" startWordPosition="407">k, release 2; BNC stands for British National Corpus (Burnard 1995), and MUC-6 for the combined training/test set for the coreference task of the Sixth Message Understanding Conference (Hirschman and Chinchor 1997). Submission received: 15 December 2003; revised submission received: 21 November 2004; accepted for publication: 19 March 2005.  2005 Association for Computational Linguistics Computational Linguist</context>
J05-3004==>M98-1028!=<context citStr="Chinchor 1997" endWordPosition="1632" position="10868" startWordPosition="1631">g on work that yields insights into the use of lexical and semantic knowledge. 2.1.1 Coreference. The prevailing current approaches to coreference resolution are evaluated on MUC-style (Hirschman and Chinchor 1997) annotated text and treat pronominal and full NP anaphora, named-entity coreference, and non-anaphoric coreferential links that can be stipulated by appositions and copula. The performance of these ap</context>
J05-3004==>M98-1028!=<context citStr="Chinchor 1997" endWordPosition="4652" position="30804" startWordPosition="4651">r the corpusbased methods, and compounds are often not recorded in WordNet. For the same reasons we automatically resolved named entities (NEs). They were classified into the ENAMEX MUC-7 categories (Chinchor 1997) PERSON, ORGANIZATION and LOCATION, using the software ANNIE (GATE2; http://gate.ac. uk). We then automatically obtained more-fine-grained distinctions for the NE categories LOCATION and ORGANIZATION,</context>
P05-3021==>N03-2019!=<context citStr="Mani et al., 2003" endWordPosition="1088" position="6858" startWordPosition="1085">e component in a larger machine-learning based framework for ordering events. Another component which will be developed will leverage document-level inference, as in the machine learning approach of (Mani et al., 2003), which required annotation of a reference time (Reichenbach, 1947; Kamp and Reyle, 1993) for the event in each finite clause. 1TimeBank is a 200-document news corpus manually annotated with TimeML ta</context>
A00-1028==>P94-1026!=<context citStr="Samuelsson, 1994" endWordPosition="2527" position="15522" startWordPosition="2526"> Work The work presented in the current article is related to previous work on corpus-based grammar specialization as presented in (Rayner, 1988; Samuelsson and Rayner, 1991; Rayner and Carter, 1996; Samuelsson, 1994; Srinivas and Joshi, 1995; Neumann, 1997). Parser with specialized grammar Parser with original grammar 207 Parses/sentence Anyparse Coverage Avg. time Max. time Speedup (secs.) (secs.) French origin</context>
A00-1028==>P94-1026!=<context citStr="Samuelsson, 1994" endWordPosition="2728" position="16744" startWordPosition="2727"> pruning on 'VPv' and 'NPadj' 0.397 1.40 Figure 5: Results for the simulated two-stage architecture. The line of work described in (Rayner, 1988; Samuelsson and Rayner, 1991; Rayner and Carter, 1996; Samuelsson, 1994) deals with unificationbased grammars that already have a purelyconcatenative context-free backbone, and is more concerned with a different form of specialization, consisting in the application of exp</context>
A00-1028==>P94-1026!=<context citStr="Samuelsson, 1994" endWordPosition="2946" position="18080" startWordPosition="2945">Finding suitable tree-cutting criteria requires a considerable amount of work, and must be repeated for each new grammar and for each new domain to which the grammar is to be specialized. Samuelsson (Samuelsson, 1994) proposes a technique to automatically selects what subtrees to retain. The selection of appropriate subtrees is done by choosing a subset of nodes at which to cut trees. Cutnodes are determined by co</context>
A94-1019==>P94-1027!=<context citStr="Jacquemin 1994" endWordPosition="2361" position="14830" startWordPosition="2360">ich may make the parsing process totally inefficient. Due to the very large size of our grammar, we have opted for the dynamic approach. The computational performances of the application reported in (Jacquemin 1994a) indicate that the parser only spends 10% of its time in generating metarules and fully justify the run-time approach. 115 Computational Lexicalization The keystone of the computational tractability</context>
A94-1019==>P94-1027!=<context citStr="Jacquemin (1994" endWordPosition="2553" position="16005" startWordPosition="2552">iple lexical items such as the rules representing multi-word terms, the anchor can be any of the lexical items. For example, the term aortic disease can be anchored either to aortic or to disease. In Jacquemin (1994b), an algorithm for optimizing the determination of computational anchors is described. It yields a uniform distribution of the rules on to the lexical items with respect to a given weighting functio</context>
W94-0307==>J89-4002!=<context citStr="[8, 1]" endWordPosition="873" position="5420" startWordPosition="872">ose defined in Mann and Thompson's RST [7]. This makes RST an attractive tool for studying this genre. From these two remarks, the planning of instructional texts is often seen as a two-stage process [8, 1]: a task planning stage, where the plan of the procedure is developed, followed by a text planning stage, where the content of the text is selected from the task representation3, and the rhetorical st</context>
W94-0307==>J89-4002!=<context citStr="[8]" endWordPosition="3678" position="22684" startWordPosition="3678">ormation is found in the task representation in the hierarchical relation between operations. Previous work on deciding whether or not to include hierarchical relation prescribed the inclusion of all [8], or no relation [1]. According to our analysis, a guidance is generally introduced when:  The execution of a basiclevel operation depends on the execution of its parent operation (eg. for a stoppin</context>
W06-1629==>P04-1021!=<context citStr="Haizhou et al., 2004" endWordPosition="625" position="4188" startWordPosition="622">. 2 Related Work In a broad sense, the term transliteration has been used to refer to two tasks. The first task is transliteration in the strict sense, which creates new words in a target language (Haizhou et al., 2004; Wan and Verspoor, 1998). The second task is back-transliteration (Fujii and Ishikawa, 2001; Jeong et al., 1999; Knight and Graehl, 1998; Qu et al., 2003), which identifies the source word correspond</context>
W06-1629==>P04-1021!=<context citStr="Haizhou et al., 2004" endWordPosition="757" position="5049" startWordPosition="754">ktransliteration is outside the scope of this paper. In the following, we use the term transliteration to refer to transliteration in the strict sense. Existing transliteration methods for Chinese (Haizhou et al., 2004; Wan and Verspoor, 1998) aim to spell out foreign names of people and places, and do not model impression. However, as exemplified by Coca-Cola in Section 1, the impression of words needs to be mod</context>
H92-1034==>H90-1064!=<context citStr="[1, 9, 2, 10, 11]" endWordPosition="1105" position="7141" startWordPosition="1101">lary system is generally very large. With limited training data, there is no hope to obtain well-trained models. Therefore, different technologies have been studied to reduce the number of parameters [1, 9, 2, 10, 11]. In generalized triphones, every state of a triphone is merged with the corresponding state of another triphone in the same cluster. It may be true that some states are merged not because they are si</context>
C04-1100==>J97-1003!=<context citStr="Hearst, 1997" endWordPosition="1843" position="11958" startWordPosition="1842">resentations reported in (Harabagiu, 2004) considers the notion of topic theme that associates clusters of topic relation with text segments. The segmentation is produced by the TEXTTILING algorithm (Hearst, 1997). The nominalization of the verb corresponding to the most relevant topic relation in a segment is considered to be linked to the nominalization from the following topic-relevant segment. Such segment</context>
J82-2006==>C80-1008!=<context citStr="Sondheimer and Weischedel 1980" endWordPosition="2261" position="14887" startWordPosition="2258">d to formatted databases but entails translation into a set of well-formed formulas in a many-sorted first-order logic (Haas and Hendrix 1980). Recent work on treating departures from grammaticality (Sondheimer and Weischedel 1980, Hayes and Mouradian 1980, McKeown 1980, Kwasny and Sondheimer 1981, Miller et al. 1981) can be use in handling specialized language that deviates syntactically from the standard language. Devices fo</context>
E06-1034==>J93-2004!=<context citStr="Marcus et al., 1993" endWordPosition="773" position="4881" startWordPosition="770">n another occurrence of the same ngram in the corpus. The string exhibiting the variation is referred to as the variation nucleus. For example, in the WSJ corpus, part of the Penn Treebank 3 release (Marcus et al., 1993), the string in (1) is a variation 12-gram since off is a variation nucleus that in one corpus occurrence is tagged as a preposition (IN), while in another it is tagged as a particle (RP). (1) to ward</context>
W95-0113==>P90-1034!=<context citStr="[11]" endWordPosition="3514" position="20413" startWordPosition="3514">ment of a treebank. In addition, the simple but effective chunker can also be applied to many natural language applications such as extracting the predicate-argument structures [9,10], grouping words [11] and gathering collocation [12]. The evaluation criterion adopted in this paper is not very strict. Under a strict criterion, the method proposed in this paper may not be suitable for short-fat trees.</context>
J03-1005==>J99-4005!=<context citStr="Knight (1999)" endWordPosition="14948" position="89264" startWordPosition="14947">or the multiword-based translation model proposed in Och, Tillmann, and Ney [1999]). On the other hand, since the decoding problem for the IBM-4 translation model is provably NP-complete, as shown in Knight (1999) and Germann et al. (2001), word reordering restrictions as introduced in this article are essential for obtaining an efficient search algorithm that guarantees that a solution close to the optimal on</context>
C04-1030==>J03-1005!=<context citStr="Tillmann and Ney, 2003" endWordPosition="1136" position="7157" startWordPosition="1133">. The next target word has to be the translation of one of the first k uncovered, i.e. not translated, source positions. The IBM constraints are illustrated in Figure 1. For further details see e.g. (Tillmann and Ney, 2003). For the phrase-based translation approach, we use the same idea. The target sentence is produced phrase by phrase. Now, we allow skipping of up to k phrases. If we set k = 0, we obtain a search that</context>
C04-1030==>J03-1005!=<context citStr="Tillmann and Ney, 2003" endWordPosition="3765" position="22082" startWordPosition="3762">e bracketing of the source sentence into sub-sentential chunks. Investigations on the IBM constraints (Berger et al., 1996) for single-word based statistical machine translation can be found e.g. in (Tillmann and Ney, 2003). A comparison of the ITG constraints and the IBM constraints for single-word based models can be found in (Zens and Ney, 2003). In this work, we investigated these reordering constraints for phraseba</context>
J05-1004==>A00-2034!=<context citStr="McCarthy 2000" endWordPosition="1862" position="12249" startWordPosition="1861">ddress the semantic roles associated with the syntactic arguments. More recent work has attempted to group verbs into classes based on alternations, usually taking Levins classes as a gold standard (McCarthy 2000; Merlo and Stevenson 2001; Schulte im Walde 2000; Schulte im Walde and Brew 2002). But without an annotated corpus of semantic roles, this line of research has not been able to measure the frequency </context>
W93-0102==>H93-1054!=<context citStr="[8]" endWordPosition="3350" position="20896" startWordPosition="3350"> target. An obvious shortcoming of this approach is the amount of work involved. Recently there has been much interest in automatic and semi-automatic acquisition of local context (Hearst [2], Resnik [8], Yarowsky [13]). These systems are all plagued with the same problem, excellent precision but low recall. That is, if the local information that the methods learn is also present in a novel context, </context>
W93-0102==>H93-1054!=<context citStr="[8]" endWordPosition="4331" position="26751" startWordPosition="4331">biguities, Resnik 6Yarowslcy uses the term collocation to denote constructs similar to what we have called templates. 18 investigated four different methods for combining three sources of information [8]. The &amp;quot;backing off&amp;quot; strategy, in which the three sources of information were tried in order from most reliable to least reliable until some match was found (no resolution was done if no method matched</context>
W03-1812==>P99-1041!=<context citStr="Lin, 1999" endWordPosition="270" position="1896" startWordPosition="269">ag et al., 2002). Analysis of the semantic correlation between the constituent parts and whole of an MWE is perhaps more commonly discussed under the banner of compositionality (Nunberg et al., 1994; Lin, 1999). Our claim here is that the semantics of the MWE are deconstructed and the parts coerced into often idiosyncratic interpretations to attain semantic alignment, rather than the other way around. One i</context>
W03-1812==>P99-1041!=<context citStr="Lin (1999)" endWordPosition="1143" position="7472" startWordPosition="1142"> has been little work on detecting non-compositional (i.e. non-decomposable and idiosyncratically decomposable) items of variable syntactic type in monolingual corpora. One interesting exception is Lin (1999), whose approach is explained as follows: The intuitive idea behind the method is that the metaphorical usage of a noncompositional expression causes it to have a different distributional characterist</context>
W04-0839==>W04-2404!=<context citStr="Mohammad and Pedersen, 2004" endWordPosition="1487" position="8898" startWordPosition="1484">s further away from the target word are not expected to be as strong indicators of intended sense as the immediate neighbors. However, inclusion of such features has been shown to improve accuracies (Mohammad and Pedersen, 2004). The nodes in the decision trees are features of the form: P = Tag , P = Tag , P = Tag, P = Tag or P = Tag . The system achieves a fine grained and coarse grained accuracy of 61.8% and 68.4%, respect</context>
W04-0839==>W04-2404!=<context citStr="Mohammad and Pedersen, 2004" endWordPosition="1927" position="11443" startWordPosition="1924">re slightly lower. We believe this is due to the low training data per task ratio, which usually means that the weak indicators (P and P ) are likely to be overwhelmed by idiosyncrasies of the data. (Mohammad and Pedersen, 2004) show results to the same conclusions for SENSEVAL-1 and SENSEVAL-2 data that have similar low training data per task, while, the line, hard, serve and interest data which have much larger training da</context>
W04-0839==>W04-2404!=<context citStr="Mohammad and Pedersen, 2004" endWordPosition="2101" position="12496" startWordPosition="2098">e mutually complementary. Significant complementarity implies that a marked increase in accuracies may be achieved by suitably combining the bigram and Part of Speech features. We have shown earlier (Mohammad and Pedersen, 2004) that there is indeed large complementarity between lexical and syntactic features by experiments on line, hard, serve, interest, SENSEVAL-1 and SENSEVAL-2 data. We use the measures Optimal Ensemble a</context>
W06-0906==>J96-2004!=<context citStr="Carletta, 1996" endWordPosition="1985" position="12001" startWordPosition="1984">tions enables us to visualize quickly the level of agreement among different annotators for each event, a quantitative measurement of the agreement is needed. The kappa statistic (Krippendorff, 1980; Carletta, 1996) has become the de facto standard to assess inter-annotator agreement. It is computed as: P(A)  P(E) 1( ) P E P(A) is the observed agreement among the annotators, and P(E) is the expected agreement,</context>
W06-0906==>J96-2004!=<context citStr="Carletta, 1996" endWordPosition="2245" position="13565" startWordPosition="2243">ic contexts. In the literature on the kappa statistic, most authors address only category data; some can handle more general data, such as data in interval scales or ratio scales (Krippendorff, 1980; Carletta, 1996). However, none of the techniques directly apply to our data, which are ranges of durations from a lower bound to an upper bound. In fact, what coders were instructed to annotate for a given event is </context>
W01-0810==>C00-1069!=<context citStr="Hakkani et al., 1996" endWordPosition="613" position="4035" startWordPosition="610">in, while the former is catered for in most tactical generation systems, only selected aspects of the latter have been dealt with and only for selected languages (e.g., (Hoffman, 1994; Hoffman, 1995; Hakkani et al., 1996)). For example, (Hoffman, 1994) proposes a treatment of WO in Turkish using a categorial grammar framework (CCG, (Steedman, 2000)) and relating this to Steedmans (earlier) account of information stru</context>
H90-1046==>H89-2021!=<context citStr="[1, 5, 6]" endWordPosition="1205" position="7785" startWordPosition="1203">view, the recognizer introduces a delay at least fifteen percent of the time (in addition to other delays, due to signal acquisition and processing). Since delays have a clear impact on what users do [1, 5, 6], we believe that a &amp;quot;percent real-time&amp;quot; measure is more relevant in characterizing spoken language system performance than a simple mean response time. Our current recognizer is 85 percent realtime fo</context>
H90-1046==>H89-2021!=<context citStr="[5]" endWordPosition="2237" position="14145" startWordPosition="2237">ing a stream of keyboard and mouse events for each utterance that correspond to the equivalent input for those modalities or it can access functions within the application directly. A previous system [5] was implemented using the former strategy. In the present case, we chose to have the Mapper access application functionality directly. The availability of spoken language disposes the user to express</context>
A00-2032==>W96-0205!=<context citStr="Nagata, 1996" endWordPosition="158" position="1131" startWordPosition="157">n technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) errors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Ring, 1998). Typically, Japanese word segmentation is performed by morphological analysis based on lexical and grammatical knowledge. This analysis is aided by t</context>
A00-2032==>W96-0205!=<context citStr="Nagata, 1996" endWordPosition="3145" position="19825" startWordPosition="3144">ts, which took only 42 minutes. In contrast, previously proposed supervised approaches have used segmented training sets ranging from 1000-5000 sentences (Kashioka et al., 1998) to 190,000 sentences (Nagata, 1996a). To test how much annotated training data is actually necessary, we experimented with using miniscule parameter-training sets: five sets of only five strings each (from which any sequences repeated</context>
A00-2032==>W96-0205!=<context citStr="Nagata, 1996" endWordPosition="3457" position="21674" startWordPosition="3456">existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupe</context>
W01-0909==>P93-1041!=<context citStr="Kozima, 1993" endWordPosition="756" position="4595" startWordPosition="755">tion network. A topic is detected by computing a cohesion value for each word resulting from the relations found in the network between these words and their neighbors in a text. As in Kozimas work (Kozima, 1993), this computation operates on words belonging to a focus window that is moved all over the text.The cohesion values lead to build a graph and by successive transformations applied to it, texts are au</context>
P98-1082==>P97-1066!=<context citStr="Assadi, 1997" endWordPosition="2846" position="18126" startWordPosition="2845">educed to compare our results with those obtained by different methods even though they are not perfect either. We are planning to compare the clusters found by our method with the clustering one of (Assadi, 1997) to study how the results overlap and are complementary. 4 Related works The variant detection in specialized corpora must be taken into account for information retrieval. This complex operation invol</context>
P98-1082==>P97-1066!=<context citStr="Assadi, 1997" endWordPosition="3038" position="19424" startWordPosition="3037">rpora (Hindle, 1990; Agirre and Rigau, 1996). Therefore, they cannot be applied for technical documents which usually are medium size corpora. However, dealing with already linguistic filtered data, (Assadi, 1997) aims at statistically build rough clusters supposing that similar candidate terms have similar expansions. Then he relies on human expertise for the semantic interpretation. It differs from our work </context>
W05-1514==>W95-0107!=<context citStr="Ramshaw and Marcus, 1995" endWordPosition="748" position="4673" startWordPosition="745">s the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach The performance of chunk parsing heavily d</context>
W06-2910==>W03-0410!=<context citStr="Stevenson and Joanis, 2003" endWordPosition="270" position="1837" startWordPosition="266">tions, automatic methods such as classification and clustering are applied to induce verb classes from corpus data, e.g. (Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Ferrer, 2004). Depending on the types of verb classes to be induced, the automatic approaches vary their choice of verbs and classification/clustering algorithm. However, ano</context>
W06-2910==>W03-0410!=<context citStr="Stevenson and Joanis, 2003" endWordPosition="416" position="2768" startWordPosition="413"> features are chosen such that they model the syntactic frame alternation proportions and also heuristics for semantic role assignment. In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient. The verb features need to relate to a behavi</context>
W06-2910==>W03-0410!=<context citStr="Stevenson and Joanis, 2003" endWordPosition="3252" position="20704" startWordPosition="3249">, and evaluated against the FrameNet classes. For the evaluation of the clustering results, we calculated the accuracy of the clusters, a cluster similarity measure that has been applied before, cf. (Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: 4Note that we can use accuracy for the evaluation because we have a fixed cut in the hierarchy as based on the gold standard, as opposed</context>
C96-2114==>J88-1003!=<context citStr="DeRose 1988" endWordPosition="919" position="5620" startWordPosition="918">ing description of how XPOST is used on the Swedish material and also sketches the major differences between this algorithm and some others used for tagging, such as PARTS (Church 1988) and VOLSUNGA (DeRose 1988). A characteristic feature of the SUC is its high number of different tags. The number of part-ofspeech tags used in the SUC is 21. With the addition of a category for foreign words the number of majo</context>
W97-1108==>P96-1022!=<context citStr="Kiraz, 1996" endWordPosition="590" position="3866" startWordPosition="589">. In fact, for many languages, the phonological and orthographic rules tend to be more numerous than the morphological rules. This is the case in Semitic. For example, the Syriac grammar reported in (Kiraz, 1996) contains 48 rules. Only six rules (a mere 12.5%)1 are motivated by templatic morphology. The rest are phonological and orthographic. Consider the above derivation of /katab/, but for Syriac rather th</context>
J01-4002==>C88-1021!=<context citStr="Carbonell and Brown 1988" endWordPosition="307" position="2173" startWordPosition="304">is known to be a difficult problem. In principle, a variety of constraints and preference heuristics, including factors that rely on semantic, pragmatic, and world knowledge, contribute to this task (Carbonell and Brown 1988). Robust, operational approaches to anaphor resolution on unrestricted discourse, however, are confined to strategies exploiting globally available evidence like morphosyntactic, syntactic, and surfac</context>
P06-1018==>J92-4004!=<context citStr="Vijay-Shanker 1992" endWordPosition="2814" position="16288" startWordPosition="2813">ts end by representing the phrase with an edge from the beginning to the end. S a Peter N V beans Adj S VP NP NP eats red S NP VP A B C D E 140 monotonic and cannot be simulated with PUG. As shown by Vijay-Shanker 1992, to obtain a monotonic formalism, TAG must be viewed as a grammar combining quasi-trees. Intuitively, a quasi-tree is a tree whose nodes has been split in two parts and have each one an upper part an</context>
P06-1018==>J92-4004!=<context citStr="Vijay-Shanker 1992" endWordPosition="3156" position="18183" startWordPosition="3155"> which will force two seminodes to unify by neutralizing the dominance link between them. This last rule again shows the advantage of PUG: the reunification of nodes, which is procedurally ensured in Vijay-Shanker 1992 is given here as a declarative rule. 3.4 HPSG (Head-driven Phrase Structure Grammar) There are two ways to translate feature structures (FSs) into PUG. Clearly atomic values must be labels and (embed</context>
C94-1032==>A88-1019!=<context citStr="[2, 3, 4,11]" endWordPosition="137" position="955" startWordPosition="137">ATR Corpus. 1 Introduction In recent years, we have seen a fair number of papers reporting accuracies of more than 95% for English part of speech tagging with statistical language modeling techniques [2, 3, 4,11]. On the other hand, there are few works on stochastic Japanese morphological analysis [9,12,14], and they don't seem to have convinced the Japanese NLP community that the statistically-based techniqu</context>
C94-1032==>A88-1019!=<context citStr="[2, 3, 4, 11]" endWordPosition="3921" position="24130" startWordPosition="3920">on for the top candidate, and 97.8% recall and 73.2% precision for the 5 best candidates. This performance is very encouraging, and is comparable to the state-of-the-art stochastic tagger for English [2, 3, 4, 11]. Since the segmentation accuracy of the proposed system is relatively high (97.7% recall and 97.2% precision for the top candidate) compared to the morphological analysis accuracy, it is likely that </context>
P92-1001==>C92-2108!=<context citStr="Oberlander and Lascarides (1992)" endWordPosition="1266" position="8094" startWordPosition="1263">ems involved in calculating implicature. Let (r, a, be the update function, which means &amp;quot;the representaiLascarides and Asher (1991a) introduces the general framework and applies it to interpretation; Oberlander and Lascarides (1992) and Lascarides and Oberlander (1992b) use the framework for generation. 2 tion T of the text so far (of which a is already a part) is to be updated with the representation # of the current clause via</context>
H05-1099==>P97-1003!=<context citStr="Collins (1997)" endWordPosition="303" position="2049" startWordPosition="302">ficiency benefit of shallow parsing, Li and Roth (2001) have further claimed both an accuracy and a robustness benefit versus context-free parsing. The output of a contextfree parser, such as that of Collins (1997) or Charniak (2000), can be transformed into a sequence of shallow constituents for comparison with the output of a shallow parser. Li and Roth demonstrated that their shallow parser, trained to label</context>
H05-1099==>P97-1003!=<context citStr="Collins (1997)" endWordPosition="940" position="6113" startWordPosition="939">ree, shown in Figure 1(a), had an empty subject NP in the embedded clause which has been removed for the modified tree. To compare the output of their shallow parser with the output of the well-known Collins (1997) parser, Li and Roth applied the chunklink conversion script to extract the shallow constituents from the output of the Collins parser on WSJ section 00. Un1These include: ADJP, ADVP, CONJP, INTJ, LST</context>
H05-1099==>P97-1003!=<context citStr="Collins (1997)" endWordPosition="1296" position="8107" startWordPosition="1295">P-SBJ -NONE(b) S *-1  NP VP They 788 System Phrase Evaluation Scenario Type (a) (b) (c) Modified All 98.37 99.72 99.72 Truth VP 92.14 98.70 98.70 Li and Roth All 94.64 - - (2001) VP 95.28 - - Collins (1997) All 92.16 93.42 94.28 VP 88.15 94.31 94.42 Charniak All 93.88 95.15 95.32 (2000) VP 88.92 95.11 95.19 Table 1: F-measure shallow bracketing accuracy under three different evaluation scenarios: (a) ba</context>
W05-0637==>J02-3001!=<context citStr="Gildea and Jurafsky, 2002" endWordPosition="573" position="3579" startWordPosition="570">ntify. The maximum recall score attainable with our phrases is 84.64% for the development data set. We have experimentally evaluated 30 features based on the previous work in semantic role labelling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004):  Lexical features (5): predicate (verb), first phrase word, last phrase word and words immediately before and after the phrase.  Syntactic features (14</context>
P97-1029==>A92-1021!=<context citStr="Brill, 1992" endWordPosition="237" position="1635" startWordPosition="236"> et al., 1995; Voutilainen, 1995b; Voutilainen, Heikkila, and Anttila, 1992; Voutilainen and Tapanainen, 1993; Oflazer and Kurth:5z, 1994; Oflazer and Tiir, 1996) and transformation-based techniques (Brill, 1992; Brill, 1994; Brill, 1995). This paper presents a novel approach to constraint based morphological disambiguation which relieves the rule developer from worrying about conflicting rule ordering requi</context>
P97-1029==>A92-1021!=<context citStr="Brill, 1992" endWordPosition="3389" position="20589" startWordPosition="3388"> this process, we essentially are considering just the top level inflectional information of the parses. This is very similar to Brill's use of contexts to induce transformation rules for his tagger (Brill, 1992; Brill, 1995), but instead of generating transformation rules from a training text, we gather statistics and apply them to parses in the text being disambiguated. 5 Efficient Implementation Technique</context>
N01-1010==>J96-2004!=<context citStr="Carletta, 1996" endWordPosition="845" position="5530" startWordPosition="843">t al., 1998) and DSO (Ng and Lee, 1996). The results are quite promising: our extraction method discovered 89% of the WordNet cousins, and the sense partitions in our lexicon yielded better K values (Carletta, 1996) than arbitrary sense groupings on the agreement data. 2 The Tree-cut Technique The tree-cut technique is an unsupervised learning technique which partitions data items organized in a tree structure i</context>
N01-1010==>J96-2004!=<context citStr="Carletta, 1996" endWordPosition="4203" position="25285" startWordPosition="4202">0 Total 13,936 14,836 28,772 .264 (%) (48.4) (51.6) (100.0) can see, the agreement is not very high: only around 48%.11 This low agreement ratio is also reflected in a measure called the K statistic (Carletta, 1996; Bruce and Wiebe, 1998; Ng et al., 1999). K measure takes into account chance agreement, thus better representing the state of disagreement. A K value is calculated for each word, on a confusion matr</context>
N01-1010==>J96-2004!=<context citStr="Carletta, 1996" endWordPosition="4704" position="27657" startWordPosition="4703">ive value when there is a systematic disagreement between the two judges (e.g., some values in the diagonal cells are 0, that is, Pii = 0 for some i). Normally, K &gt; .8 is considered a good agreement (Carletta, 1996). By using the formula above, the average K for the 191 words was .264, as shown in Table 5.12 This means the agreement between Semcor and DSO is quite low. We selected the same 191 words from our lex</context>
W06-1607==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="330" position="2202" startWordPosition="327">c( s,  t) of source/target phrase pairs observed in a word-aligned parallel corpus. Traditionally, maximum-likelihood estimation from relative frequencies is used to obtain conditional probabilities (Koehn et al., 2003), eg, p( s| t) = c( s,  t)/ Es  c( s,  t) (since the estimation problems for p( s| t) and p( t| s) are symmetrical, we will usually refer only to p( s| t) for brevity). The most obvious example of the</context>
W06-1607==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="957" position="5949" startWordPosition="954">ms BLEU score (Papineni et al., 2001) on a development corpus. The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and trigram language model probabilities log p(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002). Phrase translation model pr</context>
W06-1607==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="1089" position="6766" startWordPosition="1086">top-ranked translations t  according to p( t| s) are retained. To derive the joint counts c( s,  t) from which p( s| t) and p( t| s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993). 3 Smoothing Techniques Smoothing involves some recipe for modifying conditional distributions away from pure relati</context>
W06-1607==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="1705" position="10751" startWordPosition="1702">re suitable approach. For combining relative-frequency estimates with glass-box smoothing distributions, we employed loglinear interpolation. This is the traditional approach for glass-box smoothing (Koehn et al., 2003; Zens and Ney, 2004). To illustrate the difference between linear and loglinear interpolation, consider combining two Bernoulli distributions p1(x) and p2(x) using each method: plinear(x) = p1(x) + </context>
E91-1006==>P90-1036!=<context citStr="Schabes and Joshi, 1988" endWordPosition="341" position="2215" startWordPosition="338">s for TAGs have been proposed in the literature: the worst-case time complexity varies from 0(n4 log n) (Harbusch, 1990) to 0(n6) (Vijay-Shanker and Joshi, 1985, Lang, 1990, Schabes, 1990) and 0(n9) (Schabes and Joshi, 1988). `Part of this work was done while Giorgio Satta was completing his Doctoral Dissertation at the University of Padova (Italy). We would like to thank Yves Schabes for his valuable comments. We would </context>
J98-3005==>A97-1033!=<context citStr="Radev and McKeown 1997" endWordPosition="722" position="4615" startWordPosition="719">nformation that the writer has considered important, whereas briefings are based on information that the user is looking for. We present a system, called SUMMONS' (McKeown and Radev 1995; Radev 1996; Radev and McKeown 1997), shown in Figure 1, which introduces novel techniques in the following areas:  It briefs the user on information of interest using tools related to information extraction, conceptual combination, an</context>
J98-3005==>A97-1033!=<context citStr="Radev and McKeown 1997" endWordPosition="9920" position="62699" startWordPosition="9917">tem can find the needed information in other on-line sources, then it can produce an improved summary by merging information extracted from the input articles with information from the other sources (Radev and McKeown 1997). In the news domain, a summary needs to refer to people, places, and organizations and provide descriptions that clearly identify the entity for the reader. Such descriptions may not be present in th</context>
J95-2004==>A88-1019!=<context citStr="Church 1988" endWordPosition="359" position="2540" startWordPosition="358">speech tagging (Tapanainen and Voutilainen 1993; Silberztein 1993), none of these approaches has the same flexibility as stochastic techniques. Unlike stochastic approaches to part-of-speech tagging (Church 1988; Kupiec 1992; Cutting et al. 1992; Merialdo 1990; DeRose 1988; Weischedel et al. 1993), up to now the knowledge found in finite-state taggers has been handcrafted and was not automatically acquired. </context>
J95-2004==>A88-1019!=<context citStr="Church (1988)" endWordPosition="3869" position="23810" startWordPosition="3868">mall number of rules (between 200 and 300). We empirically compared our tagger with Eric Brill's implementation of his tagger, and with our implementation of a trigram tagger adapted from the work of Church (1988) that we previously implemented for another purpose. We ran the three programs on large files and piped their output into a file. In the times reported, we included the time spent reading the input an</context>
W01-0504==>P95-1026!=<context citStr="Yarowsky (1995)" endWordPosition="2427" position="14885" startWordPosition="2426">s still finding the overall best translation for a word, not the advanced task of finding the right translation in a given context. 6 Using Parallel and Monolingual Corpora and Lexicon 6.1 Background Yarowsky (1995) proposes a bootstrapping scheme that uses a initial decision list trained on supervised data as a starting point. By labeling new word occurrences in a monolingual corpus, he was able to collect more</context>
P06-2066==>P98-1106!=<context citStr="Kahane et al. (1998)" endWordPosition="2398" position="13742" startWordPosition="2394">ncy frameworks that in principle allow unrestricted graphs, but provide mechanisms to control the actually permitted forms of non-projectivity in the grammar. The non-projective dependency grammar of Kahane et al. (1998) is based on an operation on dependency trees called lifting: a lift of a tree T is the new tree that is obtained when one replaces one 2We use the term edge degree instead of the original simple te</context>
N06-1024==>P03-1055!=<context citStr="Dienes and Dubey, 2003" endWordPosition="333" position="2209" startWordPosition="330">ork in the last few years on enriching the output of state-of-the-art parsers that output Penn Treebank-style trees with function tags (e.g. (Blaheta, 2003)) or empty categories (e.g. (Johnson, 2002; Dienes and Dubey, 2003a; Dienes and Dubey, 2003b), only one system currently available, the dependency graph parser of (Jijkoun and de Rijke, 2004), recovers some representation of both these aspects of the Treebank repres</context>
N06-1024==>P03-1055!=<context citStr="Dienes and Dubey, 2003" endWordPosition="2461" position="15158" startWordPosition="2458">ategories: Approach Most learningbased, phrasestructurebased (PSLB) work9 on recovering empty categories has fallen into two classes: those which integrate empty category recovery into the parser (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b) and those which recover empty categories from parser output in a postprocessing step (Johnson, 2002; Levy and Manning, 2004). Levy and Manning note that thus far no PSLB p</context>
P87-1027==>E87-1011!=<context citStr="Boguraev et al. 1987" endWordPosition="802" position="5124" startWordPosition="798"> detailed information about the grammatical behaviour of individual words. We have mounted the dictionary on-line and, following its conversion into a flexible lexical knowledge base (as described in Boguraev et al. 1987), a range of experiments have since been carried out with the aim of establishing LDOCE's appropriateness to the task of deriving a word list with associated grammatical definitions indexed to the ana</context>
H05-1095==>W99-0604!=<context citStr="Och et al., 1999" endWordPosition="148" position="1075" startWordPosition="145"> generalize from the training data. 1 Introduction Possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (Och et al., 1999; Marcu and Wong, 2002; Yamada and Knight, 2002; Tillmann and Xia, 2003). While in traditional word-based statistical models (Brown et al., 1993) the atomic unit that translation operates on is the wo</context>
W05-1518==>N03-1004!=<context citStr="Chu-Carroll et al., 2003" endWordPosition="308" position="2034" startWordPosition="305">ch recognition (Fiscus, 1997), named entity recognition (Borthwick et al., 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al., 2003). Brill and Hladka (Haji et al., 1998) have first explored committee-based dependency parsing. However, they generated multiple parsers from a single one using bagging (Breiman, 1994). There have not </context>
C02-1131==>A00-1033!=<context citStr="Neumann et al. (2000)" endWordPosition="1502" position="9183" startWordPosition="1499">t of several POS taggers for German, which vary in training data. For the system at hand, we use three instances of the TnT triture sentences for an information retrieval system (cf. Braun (1999) and Neumann et al. (2000)). gram tagger (Brants, 2000) trained separately on manually annotated news texts, on novels, and on all texts available.6 Following the strategies outlined in van Halteren et al. (1998), the best POS</context>
C00-2100==>W99-0503!=<context citStr="Stevenson et al., 1999" endWordPosition="4251" position="24424" startWordPosition="4248">ng SF information from data can be used along with other research which aims to discover relationships between different SFs of a verb (Stevenson and Merlo, 1999; Lapata and Brew, 1999; Lapata, 1999; Stevenson et al., 1999). The statistical models in this paper were based on the assumption that given a verb, different SFs occur independently. This assumption is used to justify the use of the binomial. Future work perhap</context>
J98-4002==>P96-1042!=<context citStr="Engelson and Dagan (1996)" endWordPosition="1058" position="7151" startWordPosition="1055">e gives us an optimally informative database of a given size irrespective of the stage at which processing is terminated. Several researchers have proposed this type of approach for NLP applications. Engelson and Dagan (1996) proposed a committee-based sampling method, which is currently applied to HMM training for part-of-speech tagging. This method sets several models (the committee) taken from a given supervised data s</context>
J98-4002==>P96-1042!=<context citStr="Engelson and Dagan 1996" endWordPosition="8493" position="52251" startWordPosition="8490">en corpus is randomly selected for training,  uncertainty sampling (US), in which examples with minimum interpretation certainty are selected (Lewis and Gale 1994),  committee-based sampling (CBS) (Engelson and Dagan 1996),  our method based on the notion of training utility (TU). 590 Fujii, Inui, Tokunaga, and Tanaka Selective Sampling We elaborate on uncertainty sampling and committee-based sampling in Section 4.2. </context>
J98-4002==>P96-1042!=<context citStr="Engelson and Dagan 1996" endWordPosition="9224" position="56765" startWordPosition="9220"> in the comparative experiments in Section 4, in which our method outperformed uncertainty sampling to the highest degree in early stages. 4.2.2 Committee-based Sampling. In committee-based sampling (Engelson and Dagan 1996), which follows the &amp;quot;query by committee&amp;quot; principle (Seung, Opper, and Computational Linguistics Volume 24, Number 4 85 80 70 TU US ----x---- CBS random 200 400 600 800 1000 1200 no. of training data s</context>
J98-4002==>P96-1042!=<context citStr="Engelson and Dagan (1996)" endWordPosition="9674" position="59431" startWordPosition="9671">that of the overall text. Through this argument, one can assume that committeebased sampling is better suited to statistics-based systems, while our method is more suitable for example-based systems. Engelson and Dagan (1996) criticized uncertainty sampling (Lewis and Gale 1994), which they call a &amp;quot;single model&amp;quot; approach, as distinct from their &amp;quot;multiple model&amp;quot; approach: sufficient statistics may yield an accurate 0.51 pr</context>
J98-4002==>P96-1042!=<context citStr="Engelson and Dagan 1996" endWordPosition="10235" position="63156" startWordPosition="10232">eported on the performance of our sampling method by way of experiments in which we compared our method with random sampling, uncertainty sampling (Lewis and Gale 1994), and committee-based sampling (Engelson and Dagan 1996). The result of the experiments showed that our method reduced both the overhead for supervision and the overhead for searching the database to a larger degree than any of the above three methods, wit</context>
P05-1021==>N04-1037!=<context citStr="Kehler et al. (2004)" endWordPosition="368" position="2561" startWordPosition="365"> the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun reso</context>
P05-1021==>N04-1037!=<context citStr="Kehler et al. (2004)" endWordPosition="4071" position="25679" startWordPosition="4068">feating all the other possible combinations. Especially, it considerably outperforms (up to 11.5% success) the system with the Corpus+SC combination, which is commonly adopted in previous work (e.g., Kehler et al. (2004)). Personal pronoun resolution vs. Neutral pronoun resolution Interestingly, the statistics-based semantic feature has no effect on the resolution of personal pronouns, as shown in the table 2. We fou</context>
P05-1068==>J93-2003!=<context citStr="Brown et al., 1993" endWordPosition="279" position="1809" startWordPosition="276">on (Koehn et al., 2003; Marcu et al., 2002; Och et al., 1999; Och and Ney, 2000; Zens et al., 2004). Most of the phrase-based translation models have adopted the noisy-channel based IBM style models (Brown et al., 1993): ei = argmaxe1Pr(fJ Iei)Pr(ef) (1) In these model, we have two types of knowledge: translation model, Pr(f1 Ie1) and language model, Pr(eI). The translation model links the source language sentence t</context>
P05-1068==>J93-2003!=<context citStr="Brown et al., 1993" endWordPosition="1915" position="11827" startWordPosition="1912">based Translation We learn chunk alignments from a corpus that has been word-aligned by a training toolkit for wordbased translation models: the Giza++ (Och and Ney, 2000) toolkit for the IBM models (Brown et al., 1993). For aligning chunk pairs, we consider word(bunsetsu/eojeol) sequences to be chunks if they are in an immediate dependency relationship in a dependency tree. To identify chunks, we use a word-aligned</context>
N04-1031==>N03-2003!=<context citStr="Bulyko et al., 2003" endWordPosition="1162" position="7208" startWordPosition="1159">and try to find expressions unique to either of the styles (Kilgarriff, 2001). However, those studies did not deal with paraphrases. Bulyko et al. also collected spoken language corpora from the Web (Bulyko et al., 2003). The method of Bulyko et al. used N-grams in a training corpus and is different from ours (the detail of our method is described in Section 4). In respect of automatically collecting corpora which ha</context>
P04-1018==>M95-1005!=<context citStr="Vilain et al., 1995" endWordPosition="3215" position="18599" startWordPosition="3212">s to a mention recall and precision . The ECM-F measures the percentage of mentions that are in the right entities. For tests on the MUC data, we report both F-measure using the official MUC score (Vilain et al., 1995) and ECM-F. The MUC score counts the common links between the reference and the system output. 5 Experiments 5.1 Performance Metrics The official performance metric for the ACE task is ACE-value. ACE-</context>
W04-0211==>J86-3001!=<context citStr="Grosz and Sidner (1986)" endWordPosition="585" position="3897" startWordPosition="582">their 2 Complex default logic based reasoning as in Structured Discourse Representation Theory (Asher 1993; Asher and Lascarides 2003), speculations about the intentions or beliefs of speakers (as in Grosz and Sidner (1986)), or the intricate node labeling exercises familiar from Rhetorical Structure Theory (Mann and Thompson 1988; Marcu 1999, 2000) are not necessary. referents in the DPT representation of the structure</context>
N01-1003==>W00-0306!=<context citStr="Oh and Rudnicky (2000)" endWordPosition="4897" position="29951" startWordPosition="4894">lity output by concatenating the template-based output for individual communicative goals, and templates are difficult to develop and maintain for a mixed-initiative dialog system. For these reasons, Oh and Rudnicky (2000) use-gram models and Ratnaparkhi (2000), maximum entropy to choose templates, using hand-written rules to score different candidates. But syntactically simplistic approaches may have quality problems,</context>
P06-1073==>P02-1002!=<context citStr="Goodman, 2002" endWordPosition="2364" position="14517" startWordPosition="2362"> Z(x)j=1 aZj ,=i j where Z(X) is a normalization factor. To estimate the optimal j values, we train our MaxEnt model using the sequential conditional generalized iterative scaling (SCGIS) technique (Goodman, 2002). While the MaxEnt method can nicely integrate multiple feature types seamlessly, in certain cases it is known to overestimate its confidence in especially low-frequency features. To overcome this pro</context>
P06-1116==>P93-1024!=<context citStr="Pereira et al. (1993)" endWordPosition="2831" position="17557" startWordPosition="2828"> to window-based contexts, constrain the context of a word to only those words that are grammatically related to it. We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. We use the cosine similarity measure for windowbased contexts and the following commonly used similarity measures for the syntactic vector space: Hindles (1990) measure</context>
P06-1116==>P93-1024!=<context citStr="Pereira et al. (1993)" endWordPosition="4004" position="24824" startWordPosition="4001">by Melcuk to describe collocations, can be used as an interlingual device in the machine translation of such structures. (9410009, S-126) Correctly rejected: Goal-type: Perhaps the method proposed by Pereira et al. (1993) is the most relevant in our context. (9605014, S-76) Continuation-type: Neither Kamp nor Kehler extend their copying/ substitution mechanism to anything besides pronouns, as we have done. (9502014, S</context>
N03-1022==>C02-1167!=<context citStr="Moldovan and Noviscki 2002" endWordPosition="616" position="4028" startWordPosition="613">nal modules such as question processing, document retrieval, answer extraction, built in ontologies, as well as many tools such as syntactic parser, name entity recognizer, word sense disambiguation (Moldovan and Noviscki 2002), logic representation of text (Moldovan and Rus 2001) and others. The Logic Prover is integrated in this rich NLP environment and augments the QA system operation. As shown in Figure 1, the inputs to</context>
N03-1022==>C02-1167!=<context citStr="Moldovan and Noviscki 2002" endWordPosition="1460" position="9249" startWordPosition="1457">ing QLF ALF XWN axioms NLP axioms Lexical chains Figure 1: COGEX Architecture Lexical Chains A much improved source of world knowledge is obtained when the gloss words are semantically disambiguated (Moldovan and Noviscki 2002). By doing this, the connectivity between synsets is dramatically increased. Lexical chains can be established between synsets in different hierarchies. These are sequences of semantically related wor</context>
N03-1022==>C02-1167!=<context citStr="Moldovan and Noviscki 2002" endWordPosition="1558" position="9898" startWordPosition="1555">ld knowledge axioms that link question keywords with answers concepts. We developed software that automatically provides connecting paths between any two WordNet synsets and up to a certain distance (Moldovan and Noviscki 2002). The meaning of these paths is that the concepts along a path are topically related. The path may contain any of the WordNet relations augmented with a GLOSS relation which indicates that a certain c</context>
P98-2144==>C96-2160!=<context citStr="Torisawa and Tsujii, 1996" endWordPosition="2641" position="15806" startWordPosition="2638">make parsing more time-efficient. Even though they are sometimes considered to be slow in practical application because of their heavy feature structures, actually we found them to improve speed. In (Torisawa and Tsujii, 1996), an efficient HPSG parser is proposed, and our preliminary experiments show that the parsing time of the efficient parser is about three times shorter than that of the naive one. Thus, the average pa</context>
W03-1006==>W02-1031!=<context citStr="Wang and Harper, 2002" endWordPosition="1396" position="8697" startWordPosition="1393">rious kinds from the PTB, whether it be TAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000), combinatory categorial grammar (Hockenmaier and Steedman, 2002), or constraint dependency grammar (Wang and Harper, 2002). We will discuss TAGs and an important principle guiding their formation, the extraction procedure from the PTB that is described in (Chen, 2001) including extensions to extract a TAG from the PropBa</context>
P06-1009==>P05-1057!=<context citStr="Liu et al., 2005" endWordPosition="4685" position="27968" startWordPosition="4682">native word alignment models have been proposed, however these early models are typically very complicated with many proposing intractable problems which require heuristics for approximate inference (Liu et al., 2005; Moore, 2005). An exception is Taskar et al. (2005) who presented a word matching model for discriminative alignment which they they were able to solve optimally. However, their model is limited to o</context>
P06-1009==>P05-1057!=<context citStr="Liu et al. (2005)" endWordPosition="4825" position="28824" startWordPosition="4822">P estimation is the powerful smoothing offered by the prior, which allows us to avoid heuristics such as early stopping and hand weighted loss-functions that were needed for the maximum-margin model. Liu et al. (2005) used a conditional log-linear model with similar features to those we have employed. They formulated a global model, without making a Markovian assumption, leading to the need for a sub-optimal heuri</context>
P99-1079==>P98-2204!=<context citStr="Strube (1998)" endWordPosition="251" position="1593" startWordPosition="250">holinguistic plausibility. A second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods. Many small comparisons of methods have been made, such as by Strube (1998) and Walker (1989), but those usually consist of statistics based on a small handtested corpus. The problem with evaluating 1Henceforth BFP algorithms by hand is that it is time consuming and difficul</context>
P99-1079==>P98-2204!=<context citStr="Strube 1998" endWordPosition="346" position="2181" startWordPosition="345"> amounts of data and generate more reliable results. In this project, the new algorithm is tested against three leading syntax-based pronoun resolution methods: Hobbs' naive algorithm (1977), S-list (Strube 1998), and BFP. Section 2 presents the motivation and algorithm for Left-Right Centering. In Section 3, the results of the algorithms are presented and then discussed in Section 4. 2 Left-Right Centering A</context>
W05-0610==>W03-0425!=<context citStr="Florian et al., 2003" endWordPosition="2592" position="16173" startWordPosition="2589">g algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al., 2003), the SVM-based system (Mayfield et al., 2003) and the Perceptron-based system (Carreras et al., 2003). Firstly, our uneven margins SVM system performed significantly better than the other SVMbased sy</context>
W04-0403==>H92-1045!=<context citStr="Gale et al., 1992" endWordPosition="1713" position="10195" startWordPosition="1710">that the meanings of their constituent words change resulting a specific idiomatic meaning of the whole contstruction. In this case we cannot accept the general assumption of one sense per discourse (Gale et al., 1992), because words such as line, large in English or kljuch in Russian can function in the same discourse in a totally different sense. However, the assumption of one sense per collocation can hold, beca</context>
P06-2081==>W02-1011!=<context citStr="Pang et al., 2002" endWordPosition="139" position="949" startWordPosition="136">troduction There is now considerable interest in affective language processing. Work focusses on analysing subjective features of text or speech, such as sentiment, opinion, emotion or point of view (Pang et al., 2002; Turney, 2002; Dave et al., 2003; Liu et al., 2003; Pang and Lee, 2005; Shanahan et al., 2005). Discussing affective computing in general, Picard (1997) notes that phenomena vary in duration, ranging</context>
P06-2081==>W02-1011!=<context citStr="Pang et al., 2002" endWordPosition="4620" position="27738" startWordPosition="4617">son of different machine learning methodologies is required. A richer set of features besides n-grams should be checked, and we should not ignore the potential effectiveness of unigrams in this task (Pang et al., 2002). A completely new test set can be gathered, so as to further guard against overfitting, and to explore systematically the effects of the amount of training data available for each author. And as just</context>
W06-1503==>P98-1033!=<context citStr="Candito, 1998" endWordPosition="229" position="1596" startWordPosition="228">engineering point of view, and also from a linguistic point of view: cross-linguistic generalizations are expressed directly in the MG. In this paper, we extend some earlier work on multilingual MGs (Candito, 1998; Kinyon and Rambow, 2003) by proposing crosslinguistic and framework-neutral syntactic invariants, which we apply to TAG. We focus on the verb-second phenomenon as a prototypical example of cross-lan</context>
P05-1049==>J04-1001!=<context citStr="Li and Li, 2004" endWordPosition="416" position="2737" startWordPosition="413">enses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al., 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al., 2003), (3) bootstrapping sensetagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al.</context>
P05-1049==>J04-1001!=<context citStr="Li and Li, 2004" endWordPosition="1712" position="9850" startWordPosition="1709">ping and LP For WSD, SVM is one of the state of the art supervised learning algorithms (Mihalcea et al., 2004), while bootstrapping is one of the state of the art semi-supervised learning algorithms (Li and Li, 2004; Yarowsky, 1995). For comparing LP with SVM and bootstrapping, let us consider a dataset with two-moon pattern shown in Figure 1(a). The upper moon consists of 9 points, while the lower moon consists</context>
P05-1049==>J04-1001!=<context citStr="Li and Li, 2004" endWordPosition="3129" position="17998" startWordPosition="3126">et, then only the first tag is considered as correct answer. Furthermore, if the answer of the instance in test set is U, then this instance will be removed from test set. Table 2: Accuracies from (Li and Li, 2004) and average accuracies of LP with c x b labeled examples on interest and line corpora. Major is a baseline method in which they always choose the most frequent sense. MB-D denotes monolingual boo</context>
P05-1049==>J04-1001!=<context citStr="Li and Li (2004)" endWordPosition="3207" position="18477" startWordPosition="3204">ts monolingual bootstrapping with ensemble of Naive Bayes as base classifier, and BB is bilingual bootstrapping with ensemble of Naive Bayes as base classifier. 4.3 Experiment 2: LP vs. Bootstrapping Li and Li (2004) used interest and line corpora as test data. For the word interest, they used its four major senses. For comparison with their results, we took reduced interest corpus (constructed by retaini</context>
P05-1049==>J04-1001!=<context citStr="Li and Li, 2004" endWordPosition="3486" position="20128" startWordPosition="3483">est set). For data Ambiguous words interest 54.6% 54.7% 69.3% 75.5% line 53.5% 55.6% 54.1% 62.7% Ambiguous words interest 4x15=60 80.22.0% 79.82.0% line 6x15=90 60.34.5% 59.43.9% Accuracies from (Li and Li, 2004) Major MB-D MB-B BB Our results #labeled examples LPcosine LPiS 399 (a) Initial Seting (b) Groundtruth 0.5 0.5 0 0 0.5 0.5 0.4 0.2 0 0.2 0.4 0.6 0.4 0.2 0 0.2 0.4 0.6 (c) SVM (d) Bootstrapping </context>
C00-1018==>C96-2120!=<context citStr="Lehmann and Oepen, 1996" endWordPosition="2793" position="17513" startWordPosition="2790">eable no equivalents no similar cases though the question is not explicitly addressed in (Balkan, 1994), all the testsuites reviewed there also seem to follow the same methodology. The TSNLP project (Lehmann and Oepen, 1996) and its successor DiET (Netter et al., 1998), which built large multilingual testsuites, likewise fall into this category. The use of corpora (with various levels of annotation) has been studied, but</context>
W05-0203==>W03-0203!=<context citStr="Mitkov and Ha, 2003" endWordPosition="2166" position="12912" startWordPosition="2163">electing suitable distractors, which is left to future work, would be a more important process in generating a question. A semantic distance between an alternative and the right answer are suggested (Mitkov and Ha, 2003), to be a good measure to evaluate an alternative. We are investigating on a method of measuring those distances and a mechanism to retrieve best alternatives automatically. 7 Conclusion We have prese</context>
W04-3204==>W00-1326!=<context citStr="Martinez and Agirre, 2000" endWordPosition="4130" position="24500" startWordPosition="4127"> corpora (Semcor and Senseval). There have been previous works on portability of hand-tagged corpora that show how some constraints, like the genre or topic of the corpus, affect heavily the results (Martinez and Agirre, 2000). If we train on the web-corpus the results improve, and the best results are obtained with the combination of both corpora, reaching 51.6%. We need to note, however, that this is still lower than the</context>
P03-1021==>W02-1019!=<context citStr="Kumar and Byrne, 2002" endWordPosition="3522" position="21496" startWordPosition="3519"> models used remain unchanged in the minimum Bayes risk approach. In the field of natural language processing this approach has been applied for example in parsing (Goodman, 1996) and word alignment (Kumar and Byrne, 2002). 9 Conclusions We presented alternative training criteria for loglinear statistical machine translation models which are directly related to translation quality: an unsmoothed error count and a smoot</context>
P98-1032==>P84-1055!=<context citStr="Cohen (1984)" endWordPosition="898" position="5926" startWordPosition="897">an essay. Literature in the field of discourse analysis supports our approach. It points out that rhetorical cue words and structures can be identified and used for computer-based discourse analysis (Cohen (1984), (Mann and Thompson (1988), Hovy, et al (1992), Hirschberg and Litman (1993), Vander Linden and Martin (1995), and Knott (1996)). E-rater follows this approach by using rhetorical cue words and struc</context>
C04-1073==>P02-1040!=<context citStr="Papineni et al., 2002" endWordPosition="713" position="4514" startWordPosition="710">The rewrite patterns are learned from the same bilingual text. We found in an experiment for English-French that after such preprocessing, the translation performance improved by 10% in Bleu measure (Papineni et al., 2002). 2 System Overview In this section, we give a brief overview of the baseline SMT system and then outline a new approach. 2Alignment templates in (Och et al., 1999) can describe the reorderings of wor</context>
W05-0407==>W04-3201!=<context citStr="Taskar et al., 2004" endWordPosition="310" position="2099" startWordPosition="307">re space. The selection of the relevant structural features was left to the voted perceptron learning algorithm. Another interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al., 2004). The good results show that tree kernels are very promising for automatic feature engineering, especially when the available knowledge about the phenomenon is limited. Along the same line, automatic </context>
W05-0407==>W04-3201!=<context citStr="Taskar et al., 2004" endWordPosition="2802" position="16147" startWordPosition="2799">pace. In other words, we can use Eq. 1 to estimate the similarity between two PASTs avoiding to define explicit features. The same idea has been successfully applied to the parse-tree reranking task (Taskar et al., 2004; Collins and Duffy, 2002) and predicate argument classification (Moschitti, 2004). For the second problem, i.e. the high computational complexity, we can cut the search space by using a traditional b</context>
W05-0407==>W04-3201!=<context citStr="Taskar et al., 2004" endWordPosition="4223" position="24410" startWordPosition="4220">ollins and Duffy, 2002) for syntactic parsing reranking. It was experimented with the Voted Perceptron and was shown to improve the syntactic parsing. A refinement of such technique was presented in (Taskar et al., 2004). The substructures produced by the proposed tree kernel were bound to local properties of the target parse tree and more lexical information was added to the overall kernel function. In (Zelenko et a</context>
E93-1030==>J86-3001!=NO CONTEXT
J99-1003==>P93-1001!=<context citStr="Church 1993" endWordPosition="1885" position="11705" startWordPosition="1884">t algorithms described above work nearly perfectly given clean bitexts that have easily detectable sentence boundaries. However bitext mapping at the sentence level is not an option for many bitexts (Church 1993). Sentences are often difficult to detect, especially when punctuation is missing due to OCR errors. More importantly, bitexts often contain lists, tables, titles, footnotes, citations and/or markup c</context>
J99-1003==>P93-1001!=<context citStr="Church (1993)" endWordPosition="2198" position="13750" startWordPosition="2197">ted points of correspondence in the bitext map, much the same way as matching character n-grams. These points of correspondence could then be further refined using the methods previously developed by Church (1993) and Dagan, Church, and Gale (1993). Later, Fung and McKeown (1994) improved on K-Vec by employing relative position offsets, instead of a fixed model of co-occurrence. This strategy made the algorith</context>
J99-1003==>P93-1001!=<context citStr="Church 1993" endWordPosition="2299" position="14404" startWordPosition="2298">e cor2 See Melamed (1998a) for a general discussion of models of co-occurrence. 111 Computational Linguistics Volume 25, Number 1 relation between the lengths of mutual translations. Like char_align (Church 1993), SIMR infers bitext maps from likely points of correspondence between the two texts, points that are plotted in a two-dimensional space of possibilities. Unlike previous methods, SIMR greedily search</context>
J99-1003==>P93-1001!=<context citStr="Church 1993" endWordPosition="4132" position="25626" startWordPosition="4131">it much easier for SIMR to stay on track. Other bitext mapping algorithms mitigate this source of noise either by assigning lower weights to correspondence points associated with frequent word types (Church 1993) or by deleting frequent word types from the bitext altogether (Dagan, Church, and Gale 1993). However, a word type that is relatively frequent overall can be rare in some parts of the text. In those </context>
J99-1003==>P93-1001!=<context citStr="Church 1993" endWordPosition="8282" position="50640" startWordPosition="8281">n of this difference is that segment correspondence can represent order inversions, but segment alignment cannot. Inversions occur surprisingly often in real bitexts, even for sentence-size segments (Church 1993). Figure 10 provides another illustration. If, instead of the point in cell (H,e), there was a point in cell (G,f), the correct alignment for that region would still be ((G,H), (e,f)). If there were p</context>
N06-1002==>P03-1021!=<context citStr="Och 2003" endWordPosition="3566" position="22824" startWordPosition="3565"> translation pairs, train the order model, and train MTU models. The target language models were trained using only the target side of the corpus. Finally we trained model weights by maximizing BLEU (Och 2003) and set decoder optimization parameters (n-best list size, timeouts fInverseM1 =   S T A p s t ( , , ) ( |) =  ( , , ) S T A ( , )    treelets(A) = ( , , ) S T A  c (*, )  ( , )    treelet</context>
N06-1002==>P03-1021!=<context citStr="Och (2003)" endWordPosition="3865" position="24366" startWordPosition="3864">c models (Pharaoh uses its own ordering approach), the same models were used: MLE and lexical weighting channel models, target LM, and phrase and word count. Model weights were also trained following Och (2003). 5. Results We begin with a broad brush comparison of systems in Table 5.1. Throughout this section, treelet and phrase sizes are measured in terms of MTUs, not words. By default, all systems (includ</context>
C00-2109==>P97-1003!=<context citStr="Collins, 1997" endWordPosition="1023" position="6011" startWordPosition="1022">scribed in (Uchimoto et al., 1999). There have been a lot of proposals for statistical analysis, in many languages, in particular in English and Japanese (Magerman, 1995) (Sekine and Grishman, 1995) (Collins, 1997) (Ratnaparkhi, 1997) (K.Shirai et.al, 1998) (Fujio and Matsumoto, 1998) (Haruno et.al, 1997) (Ehara, 1998). One of the most advanced systems in English is proposed by Ratnaparkhi. It uses the Maximum </context>
E91-1033==>J88-3006!=<context citStr="[7]" endWordPosition="145" position="1095" startWordPosition="145"> recently. The proposed methods preferably address tailoring of explanations to the needs of their addressees, including, for instance, object descriptions 181 and presentation of taxonomic knowledge [7]. In addition, particular emphasis has been put on reactive explanation techniques for selecting an appropriate content according to contextual interpretation [6], and on the way of presenting explana</context>
W02-1029==>J92-4003!=<context citStr="Brown et al., 1992" endWordPosition="572" position="3801" startWordPosition="569">nd semantic resources, such as WordNet (Fellbaum, 1998), has allowed lexical semantic information to be leveraged to solve NLP tasks, including collocation discovery (Pearce, 2001), model estimation (Brown et al., 1992; Clark and Weir, 2001) and text classification (Baker and McCallum, 1998). Unfortunately, thesauri are expensive and timeconsuming to create manually, and tend to suffer from problems of bias, incons</context>
W06-1608==>J93-2004!=<context citStr="Marcus et al., 1993" endWordPosition="457" position="2990" startWordPosition="454">dependent on a synthetic ROOT node. We use the parsing approach described in (Corston-Oliver et al., 2006). The parser is trained on dependencies extracted from the English Penn Treebank version 3.0 (Marcus et al., 1993) by using the head-percolation rules of (Yamada and Matsumoto, 2003). Given a sentence x, the goal of the parser is to find the highest-scoring parse y among all possible parses y E Y: y = argmax s(</context>
I05-5009==>P01-1008!=<context citStr="Barzilay and McKeown, 2001" endWordPosition="350" position="2236" startWordPosition="347"> carry almost the same meaning. Some studies have constructed and evaluated hand-made rules (Takahashi et al., 2001; Ohtake and Yamamoto, 2001). Others have tried to extract paraphrases from corpora (Barzilay and McKeown, 2001; Lin and Pantel, 2001), which are very useful because they enable us to construct paraphrasing rules. In addition, we can construct an example-based or a Statistical Machine Translation (SMT)-like pa</context>
I05-5009==>P01-1008!=<context citStr="Barzilay and McKeown, 2001" endWordPosition="412" position="2684" startWordPosition="409"> utilizes paraphrasing examples. Thus, collecting paraphrased examples must be continued to achieve high-performance paraphrasing systems. Several methods of acquiring paraphrases have been proposed (Barzilay and McKeown, 2001; Shimohata and Sumita, 2002; Yamamoto, 2002). Some use parallel corpora as resources to obtain paraphrases, which seems a promising way to extract high-quality paraphrases. However, unlike translatio</context>
J95-1002==>J92-4007!=<context citStr="Moore and Pollack 1992" endWordPosition="2758" position="17523" startWordPosition="2755">rocedural relations. There is considerable debate in the field of discourse analysis concerning the relative importance of intentional structure and rhetorical relations (e.g., Grosz and Sidner 1986; Moore and Pollack 1992), most systems focusing on one or the other. The current study has conflated them, as the instructional texts have not tended to display the complex intentional structure common to persuasive texts an</context>
J95-1002==>J92-4007!=<context citStr="Moore and Pollack (1992)" endWordPosition="3420" position="21601" startWordPosition="3417">, Sequence, and Concurrent, which are used as abstractions to identify the lexical and grammatical manifestations of the procedural relations inherent in the process. They are termed informational by Moore and Pollack (1992) and subject matter by Mann and Thompson (1987) because they are based on semantic content rather than on intentional or presentational content.' This section will now provide specific definitions of </context>
W04-2603==>J92-4003!=<context citStr="Brown et al. (1992)" endWordPosition="519" position="3528" startWordPosition="516">ically and/or semantically interchangeable with a given lexeme. Others have constructed lexical similarity clusters using order-dependent co-occurrence statistics, particularly with N-gram modelssee Brown et al. (1992) for an example where words are sorted into exclusive classes based on bigram statistics. The occurrence statistics of bigrams do stabilize for frequent words given a training corpus of hundreds of mi</context>
J02-3004==>C94-2125!=<context citStr="Vanderwende 1994" endWordPosition="1413" position="8966" startWordPosition="1412">t contain detailed semantic information for each noun; a sequence of rules exploit a knowledge base to choose the correct interpretation for a given compound (Finin 1980; McDonald 1982; Leonard 1984; Vanderwende 1994). In what follows we develop a probabilistic model for the interpretation of nominalizations. We focus on nominalizations whose prenominal modifier is either the underlying subject or direct object of</context>
J02-3004==>C94-2125!=<context citStr="Vanderwende (1994)" endWordPosition="13584" position="84865" startWordPosition="13583">nts of hand-crafted knowledge, place emphasis on the recovery of relations other than nominalizations (see the examples in (1)), contain no quantitative evaluation (the exceptions are Leonard (1984), Vanderwende (1994), and Lauer (1995)), and generally assume that context dependence is either negligible or of little impact. Most symbolic approaches are limited to a specific domain because of the large effort involv</context>
J02-3004==>C94-2125!=<context citStr="Vanderwende (1994)" endWordPosition="13799" position="86291" startWordPosition="13798"> Linguistics Volume 28, Number 3 plausible interpretation. The approach was introduced by Leonard (1984), was based on a hand-crafted lexicon, and achieved an accuracy of 76.0% (on the training set). Vanderwende (1994) further developed a rule-based algorithm that does not rely on a hand-crafted lexicon but extracts the required semantic information from an on-line dictionary instead. The system achieved an accurac</context>
M98-1019==>A97-1029!=<context citStr="[3]" endWordPosition="2330" position="13388" startWordPosition="2330">iment 80.46 4) All training + 75/25 82.73 5) Add planet names 86.34 Table 5: Comparative Results RELATED WORK There have been several efforts to apply machine learning techniques to the same task [4] [3] [5] [2]. In this section, we will discuss a system which is one of the most advanced and which closely resembles our own [2]. A good review of most of the other systems can be found in their paper. T</context>
M98-1019==>A97-1029!=<context citStr="[3]" endWordPosition="2441" position="14035" startWordPosition="2441">named entity starts/ends at the current token. In contrast, our system has only one decision tree which produces probabilities of information about the named entity. In this regard, we are similar to [3], which also uses a probabilistic method in their N-gram based system. This is a crucial difference which also has important consequences. Because the system of [2] makes multiple decisions at each to</context>
P01-1060==>P99-1059!=<context citStr="Eisner and Satta (1999)" endWordPosition="1918" position="10627" startWordPosition="1915">ccording to headedness. Such an algorithm is shown in appendix B. This procedure gives worst case time and space complexity which is proportional to the fifth power of the length of the sentence. See Eisner and Satta (1999) for discussion and an algorithm with time and space requirements proportional to the fourth power of the length of the input sentence in the worst case. In practical experience with broad-coverage co</context>
H05-1050==>P05-1001!=<context citStr="Ando and Zhang (2005)" endWordPosition="731" position="4524" startWordPosition="728">classifiers. These vectors are detailed abstract representations of the words or documents. They can be clustered, or all their bits can be included as potentially relevant features in another task. 2Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. Both their work and ours try to transfer knowledge to a target pr</context>
P97-1059==>J94-3001!=<context citStr="Kaplan and Kay, 1994" endWordPosition="445" position="2861" startWordPosition="442">, 1995), in order to significantly improve tagging accuracy2. These rules may include long-distance dependencies not handled by HMM taggers, and can conveniently be expressed by the replace operator (Kaplan and Kay, 1994; Karttunen, 1995; Kempe and Karttunen, 1996).  further steps of text analysis, e.g. light parsing or extraction of noun phrases or other phrases (Ait-Mokhtar and Chanod, 1997). These compositions en</context>
N06-1041==>P05-1046!=<context citStr="Grenager et al. (2005)" endWordPosition="951" position="5928" startWordPosition="948">on of the legal tags for each word. Such dictionaries are large and embody a great deal of lexical knowledge. A prototype list, in contrast, is extremely compact. 3 Tasks and Related Work: Extraction Grenager et al. (2005) presents an unsupervised approach to an information extraction task, called CLASSIFIEDS here, which involves segmenting classified advertisements into topical sections (see figure 1(c)). Labels in th</context>
N06-1041==>P05-1046!=<context citStr="Grenager et al., 2005" endWordPosition="1152" position="7249" startWordPosition="1149">eld is not present in the original annotation, but added to model boundaries (see Section 5.3). The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure. In section 5.3, we discuss an approach to this task which does not require customization of model structure, but rat</context>
N06-1041==>P05-1046!=<context citStr="Grenager et al., 2005" endWordPosition="3761" position="22291" startWordPosition="3758">ng knowledge for 3,362 word types, they do achieve a higher accuracy of 88.1%. Setting Accuracy BASE 46.4 PROTO 53.7 PROTO+SIM 71.5 PROTO+SIM+BOUND 74.1 Figure 7: Results on test set for ads data in (Grenager et al., 2005). 5.2 Chinese POS Tagging We also tested our POS induction system on the Chinese POS data in the Chinese Treebank (Ircs, 2002). The model is wholly unmodified from the English version except that the </context>
N06-1041==>P05-1046!=<context citStr="Grenager et al. (2005)" endWordPosition="3963" position="23487" startWordPosition="3960">heless, the addition of distributional similarity features does reduce the error rate by 35% from BASE. 5.3 Information Field Segmentation We tested our framework on the CLASSIFIEDS data described in Grenager et al. (2005) under conditions similar to POS tagging. An important characteristic of this domain (see figure 1(a)) is that the hidden labels tend to be sticky, in that fields tend to consist of runs of the same</context>
N06-1041==>P05-1046!=<context citStr="Grenager et al., 2005" endWordPosition="4240" position="25142" startWordPosition="4237">tion observed transition structure: (a) labeled data, (b) BASE(c) BASE+PROTO+SIM+BOUND (after post-processing) figure 8), though we did change the similarity function (see below). On the test set of (Grenager et al., 2005), BASE scored an accuracy of 46.4%, comparable to Grenager et al. (2005)s unsupervised HMM baseline. Adding the prototype list (see figure 2) without distributional features yielded a slightly improv</context>
P04-1015==>P02-1036!=<context citStr="Geman and Johnson, 2002" endWordPosition="376" position="2358" startWordPosition="373"> can presumably become extremely large. Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model. presupposes that there is an existing baseline</context>
W06-3114==>W06-3119!=<context citStr="Zollmann and Venugopal, 2006" endWordPosition="765" position="4980" startWordPosition="762">sentences of length larger than 40 words. Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary. 103 ID Participant cmu Carnegie Mellon University, USA (Zollmann and Venugopal, 2006) lcc Language Computer Corporation, USA (Olteanu et al., 2006b) ms Microsoft, USA (Menezes et al., 2006) nrc National Research Council, Canada (Johnson et al., 2006) ntt Nippon Telegraph and Telephone</context>
A00-2022==>P99-1061!=<context citStr="Kiefer, Krieger, Carroll, &amp; Malouf, 1999" endWordPosition="592" position="3957" startWordPosition="587">nt, though, as wide-coverage HPSG grammars are starting to be deployed in practical applications for example for 'deep' analysis in the VerbMobil speech-to-speech translation system (Wahlster, 1997; Kiefer, Krieger, Carroll, &amp; Malouf, 1999).1 In this paper we answer the question by demonstrating that (a) subsumption- and equivalence-based feature structure packing is applicable to large HPSG grammars, and (b) average complexity and tim</context>
A00-2022==>P99-1061!=<context citStr="Kiefer et al. (1999)" endWordPosition="5212" position="32232" startWordPosition="5209">istributed disjunctions of feature structure fragments. Although the approach may have potential, the shifting of complex accounting into the unification algorithm is at variance with the findings of Kiefer et al. (1999), who report large speed-ups from the elimination of disjunction processing during unification. Unfortunately, the reported evaluation measures and lack of discussion of parser control issues are insu</context>
H90-1070==>A88-1014!=<context citStr="[7]" endWordPosition="2020" position="13807" startWordPosition="2020">as1 dictionaryl comic1 column2 blurb1 cataloguel calendarl bulletinl brochurel biographyl art iclel bibliography1 constitute-ion-id The basic semantic hierarchy acts as a sensedisambiguated thesaurus [7], under the assumption that in the absence of more specific knowledge word senses will tend to share semantic constraints with the most closely related words. Note that derivative lexical entries, suc</context>
P03-1012==>H91-1026!=<context citStr="Gale and Church, 1991" endWordPosition="195" position="1308" startWordPosition="192">egmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as 2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to p</context>
P03-1012==>H91-1026!=<context citStr="Gale and Church, 1991" endWordPosition="2786" position="15925" startWordPosition="2783">ents, as well as our sampling method used in training. 4.1 Initial Alignment We produce an initial alignment using the same algorithm described in Section 3, except we maximize summed 02 link scores (Gale and Church, 1991), rather than alignment probability. This produces a reasonable one-to-one word alignment that we can refine using our probability model. 4.2 Alignment Sampling Our use of the one-to-one constraint an</context>
P98-2182==>W97-0313!=<context citStr="Riloff and Shepherd (1997)" endWordPosition="278" position="1887" startWordPosition="275">oving its quality. Extracting semantic information from word co-occurrence statistics has been effective, particularly for sense disambiguation (Schiitze, 1992; Gale et al., 1992; Yarowsky, 1995). In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons. Generically, their algorithm can be outlined as f</context>
N06-2017==>P03-1069!=<context citStr="Lapata (2003)" endWordPosition="382" position="2515" startWordPosition="381">ational Transportation Safety Board database (ACCS). The same data and similar methods were used by Barzilay and Lee (2004) to compare their probabilistic approach for ordering sentences with that of Lapata (2003). This paper discusses how the Centering-based metrics of coherence employed by Karamanis et al. can be evaluated on the data prepared by Barzilay and Lapata. This is the first time that Centering is </context>
E03-1040==>C00-2108!=<context citStr="Walde, 2000" endWordPosition="244" position="1646" startWordPosition="243">roles (Riloff and Schmelzenbach, 1998; Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (Dorr and Jones, 1996; Lapata and Brew, 1999; Schulte im Walde, 2000; Merlo and Stevenson, 2001). Our work aims to extend the applicability of the latter, by developing a general feature space for automatic verb classification. Specifically, Merlo and Stevenson (2001)</context>
E03-1040==>C00-2108!=<context citStr="Walde (2000)" endWordPosition="4356" position="26131" startWordPosition="4355">ive similarity measure over the selectional preference of a verb for two slots that alternate, indicating that other formulations of slot similarity may be useful to pursue in future work. Schulte im Walde (2000) and Schulte im Walde and Brew (2002) achieved promising results using unsupervised clustering of verbs in English and German, respectively, according to syntactic frame statistics. Our feature space </context>
P98-1108==>C94-1032!=<context citStr="Nagata 1994" endWordPosition="167" position="1226" startWordPosition="166">ntroduction Recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (Church 1988; Cutting et. al. 1992; Charniak et. al. 1993; Brill 1994; Nagata 1994; Yamamoto 1996). Morphological analysis on Japanese, however, is more complex because, unlike European languages, no spaces are inserted between words. In fact, even native Japanese speakers place wo</context>
W03-0418==>P93-1024!=NO CONTEXT
H01-1067==>A00-2043!=<context citStr="[19]" endWordPosition="3790" position="25290" startWordPosition="3790">and coverage of semantic interpretation for randomly sampled texts in the two domains we consider. While recall was rather low (57% for MED, 31% for IT), precision peaked at 97% and 94%, respectively [19]. &amp;quot;Heavy&amp;quot; Semantics. We can deal with intricate semantic phenomena for which we have provided the first empirical evaluation data available at all. This relates to the resolution of metonymies, where </context>
J00-3004==>J95-4004!=<context citStr="Brill 1995" endWordPosition="1686" position="10845" startWordPosition="1685">xt is processed. Hockenmaier and Brew (1998) present an algorithm, based on Palmer's (1997) experiments, that applies a symbolic machine learning techniquetransformation-based error-driven learning (Brill 1995)to the problem of Chinese word segmentation. Using a set of rule templates and four distinct initial-state annotators, Palmer concludes that the learning technique works well. Hockenmaier and Brew in</context>
H91-1044==>H90-1056!=<context citStr="[6]" endWordPosition="2304" position="14837" startWordPosition="2304">tatistical natural language applications employ backing-off estimation techniques[10][5] to handle low-frequency events, Pearl uses a very simple estimation technique, reluctantly attributed to Church[6]. This technique estimates the probability of an event by adding 0.5 to ev6The mutual information of a part-of-speech trigram, popipa, is defined to be .p(P,;:p:=..), where x is any part-of-speech. Se</context>
J03-3005==>N01-1013!=<context citStr="Clark and Weir 2001" endWordPosition="9612" position="60524" startWordPosition="9609">s distributed evenly across its conceptual classes and attempt to find the right level of generalization in a concept hierarchy, by discounting, for example, the contribution of very general classes (Clark and Weir 2001; McCarthy 2000; Li and Abe 1998). Other smoothing approaches such as discounting (Katz 1987) and distance-weighted averaging (Grishman and Sterling 1994; Dagan, Lee, and Pereira 1999) re-create count</context>
J03-3005==>N01-1013!=<context citStr="Clark and Weir 2001" endWordPosition="9750" position="61478" startWordPosition="9747"> the performance of Web counts on a standard task from the literature. 3.4 Pseudodisambiguation In the smoothing literature, re-created frequencies are typically evaluated using pseudodisambiguation (Clark and Weir 2001; Dagan, Lee, and Pereira 1999; Lee 1999; Pereira, Tishby, and Lee 1993; Prescher, Riezler, and Rooth 2000; Rooth et al. 1999). 476 Keller and Lapata Web Frequencies for Unseen Bigrams The aim of the </context>
J03-3005==>N01-1013!=<context citStr="Clark and Weir (2001)" endWordPosition="9904" position="62441" startWordPosition="9901"> the frequencies it re-creates for both types of bigrams. We evaluated our Web counts by applying the pseudodisambiguation procedure that Rooth et al. (1999), Prescher, Riezler, and Rooth (2000), and Clark and Weir (2001) employed for evaluating re-created verb-object bigram counts. In this procedure, the noun n from a verb-object bigram (v, n) that is seen in a given corpus is paired with a randomly chosen verb v' th</context>
J03-3005==>N01-1013!=<context citStr="Clark and Weir (2001)" endWordPosition="10096" position="63542" startWordPosition="10093">rect disambiguations is a measure of the quality of the re-created frequencies (or probabilities). In the following, we will first describe in some detail the experiments that Rooth et al. (1999) and Clark and Weir (2001) conducted. We will then discuss how we replicated their experiments using the Web as an alternative smoothing method. Rooth et al. (1999) used pseudodisambiguation to evaluate a class-based model tha</context>
P06-2082==>P00-1016!=<context citStr="Ngai and Yarowsky, 2000" endWordPosition="3680" position="23092" startWordPosition="3677">rks To date, many works on selective sampling were conducted in the field related to natural language processing (Fujii et al., 1998; Hwa, 2004; Kamm and Meyer, 2002; Riccardi and Hakkani-T ur, 2005; Ngai and Yarowsky, 2000; Banko and Brill, 2001; Engelson and Dagan, 1996). The basic concepts are the same and it is important to predict the training utility value of each candidate with high accuracy. The work most closel</context>
W06-3125==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="181" position="1346" startWordPosition="178">ture functions along with a translation model which is based on bilingual n-grams (de Gispert and Mari no, 2002). This translation model differs from the well known phrase-based translation approach (Koehn et al., 2003) in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This translation approa</context>
J05-1003==>A00-2018!=<context citStr="Charniak (2000)" endWordPosition="11708" position="69651" startWordPosition="11707">ows results for the method. The model of Collins (1999) was the base model; the ExpLoss model gave a 1.5% absolute improvement over this method. The method gives very similar accuracy to the model of Charniak (2000), which also uses a rich set of initial features in addition to Charniaks (1997) original model. The LogLoss method was too inefficient to run on the full data set. Instead we made some tests on a sm</context>
J05-1003==>A00-2018!=<context citStr="Charniak 2000" endWordPosition="12207" position="72672" startWordPosition="12206">evelopment data led to minor improvements in the results. Model &lt; 40 Words (2,245 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 87.5% 87.4% 1.00 62.1% 86.1% Collins 1999 88.5% 88.7% 0.92 66.7% 87.1% Charniak 2000 90.1% 90.1% 0.74 70.1% 89.6% ExpLoss 90.2% 90.4% 0.73 71.2% 90.2% Model &lt; 100 Words (2,416 sentences) LR LP CBs 0 CBs 2 CBs Charniak 1997 86.7% 86.6% 1.20 59.5% 83.2% Ratnaparkhi 1998 86.3% 87.5% 1.2</context>
J05-1003==>A00-2018!=<context citStr="Charniak (2000)" endWordPosition="13254" position="78910" startWordPosition="13253">ely infrequent features are being selected. Even so, there are still savings of a factor of almost 50 in the early stages of the method. 6. Related Work 6.1 History-Based Models with Complex Features Charniak (2000) describes a parser which incorporates additional features into a previously developed parser, that of Charniak (1997). The method gives substantial improvements over the original parser and results w</context>
J05-1003==>A00-2018!=<context citStr="Charniak (2000)" endWordPosition="13320" position="79335" startWordPosition="13319">ery close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods). Our features are in many ways similar to those of Charniak (2000). The model in Charniak (2000) is quite different, however. The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratnaparkhi [1997]). Ratnapar</context>
P06-2057==>W04-0803!=<context citStr="Litkowski, 2004" endWordPosition="202" position="1305" startWordPosition="201">ects in Information Extraction and Question Answering, and are believed to be applicable in other domains as well. Building SRL systems for English has been studied widely (Gildea and Jurafsky, 2002; Litkowski, 2004), inter alia. However, all these works rely on corpora that have been produced at the cost of a large effort by human annotators. For instance, the current FrameNet corpus (Baker et al., 1998) consist</context>
P06-2057==>W04-0803!=<context citStr="Litkowski, 2004" endWordPosition="1032" position="6488" startWordPosition="1031"> On a test set from FrameNet, we estimated that the system had a precision of 0.71 and a recall of 0.65 using a strict scoring method. The result is slightly lower than the best systems at Senseval3 (Litkowski, 2004), possibly because we used a larger set of frames, and we did not assume that the frame was known a priori. 2.2 Transferring the Annotation We produced a Swedish-language corpus annotated with FrameNe</context>
P06-2057==>W04-0803!=<context citStr="Litkowski, 2004" endWordPosition="4296" position="25683" startWordPosition="4295">old standard FE and has the correct label. Although the strict measures are more interesting, we include these figures for comparison with the systems participating in the Senseval-3 Restricted task (Litkowski, 2004). We include baseline scores for the argument bracketing and classification tasks, respectively. The bracketing baseline method considers nonpunctuation subtrees dependent of the target word. When the</context>
H05-1111==>W04-3213!=<context citStr="Swier and Stevenson (2004)" endWordPosition="1557" position="9705" startWordPosition="1553">em is to demonstrate the usefulness of predicate lexicons for the role labelling task. The primary way that we apply the knowledge in our lexicon is via a process we call frame matching, adapted from Swier and Stevenson (2004). The automatic frame matcher aligns arguments extracted from an automatically parsed sentence with the frames in VerbNet for the target verb in the sentence. The output of this process is a highly co</context>
H05-1111==>W04-3213!=<context citStr="Swier and Stevenson (2004)" endWordPosition="1938" position="12047" startWordPosition="1935">the extracted arguments. Thus, the frame matcher only chooses roles from frames that are the best syntactic matches with the extracted argument set. This is achieved by adopting the scoring method of Swier and Stevenson (2004), in which we compute the portion %Frame of frame slots that can be mapped to an extracted argument, and the portion %Sent of extracted arguments from the sentence that can be mapped to the frame. The</context>
H05-1111==>W04-3213!=<context citStr="Swier and Stevenson (2004)" endWordPosition="2790" position="17087" startWordPosition="2787">eme, Recipient, and Location, respectively. These are the most likely roles assigned by the frame matcher over our development data. For comparison, we also apply the iterative algorithm developed by Swier and Stevenson (2004), using the same bootstrapping parameters. The method uses backoff over three levels of specificity of probabilities. 5 Materials and Methods 5.1 The Target Verbs For ease of comparison, we use the sa</context>
H05-1111==>W04-3213!=<context citStr="Swier and Stevenson (2004)" endWordPosition="4865" position="29548" startWordPosition="4862">elated Work Most role labelling systems have required handlabelled training data. Two exceptions are the subcategorization frame based work of Atserias et al. (2001) and the bootstrapping labeller of Swier and Stevenson (2004), but both are evaluated on only a small number of verbs and arguments. In related unsupervised tasks, Riloff and colleagues have learned case frames for verbs (e.g., Riloff and Schmelzenbach, 1998)</context>
H05-1111==>W04-3213!=<context citStr="Swier and Stevenson (2004)" endWordPosition="4954" position="30089" startWordPosition="4951">r role labelling systems have also relied on the extraction of much more complex features or probability models than we adopt here. As a point of comparison, we apply the iterative backoff model from Swier and Stevenson (2004), trained on 20% of the BNC, with our frame matcher and test data. The backoff model achieves an F-measure of .63, slightly below the performance of .65 for our simplest probability model, which uses </context>
E03-1034==>E99-1005!=<context citStr="Lapata et al. (1999" endWordPosition="629" position="3962" startWordPosition="625">t) and evaluated (e.g., whether to use a task-based evaluation or not). Furthermore, previous work has almost exclusively focused on verbal selectional 27 preferences in English with the exception of Lapata et al. (1999, 2001), who look at adjectivenoun combinations, again for English. Verbs tend to impose stricter selectional preferences on their arguments than adjectives or nouns and thus provide a natural test be</context>
E03-1034==>E99-1005!=<context citStr="Lapata et al., 1999" endWordPosition="1043" position="6533" startWordPosition="1040">take a noun n as its object. The method being tested must reconstruct which of the unseen (vi, n) and (v2, n) is a valid verb-object combination. correlate with the model's predictions (Resnik, 1993; Lapata et al., 1999; Lapata et al., 2001). This approach seems more appropriate for languages for which annotated corpora with word senses are not available. It is more direct than disambiguation which relies on the ass</context>
E03-1034==>E99-1005!=<context citStr="Lapata et al., 1999" endWordPosition="3210" position="19048" startWordPosition="3207">e the methods introduced in Section 2, we first established an independent measure of how well a verb fits its arguments by eliciting judgments from human subjects (Resnik, 1993; Lapata et al., 2001; Lapata et al., 1999). In this section, we describe our method for assembling the set of experimental materials and collecting plausibility ratings for these stimuli. Materials and Design. As mentioned earlier, co-occurre</context>
E03-1034==>E99-1005!=<context citStr="Lapata et al. (1999)" endWordPosition="4922" position="29602" startWordPosition="4919">(i.e., SimC for objects, SelA for subjects). The more sophisticated class-based approaches do not always yield better results when compared to simple frequency-based models. This is in agreement with Lapata et al. (1999) who found that cooccurrence frequency is the best predictor of the plausibility of adjective-noun pairs. Model combination seems promising in that a better fit with experimental data is obtained. How</context>
E95-1022==>A92-1021!=<context citStr="Brill 1992" endWordPosition="347" position="2417" startWordPosition="346">3 tags or words (with some well-known exceptions, e.g. Cutting et al. 1992). Corpus-based information can be represented e.g. as neural networks (Eineborg and Gamback 1994; Schmid 1994), local rules (Brill 1992), or collocational matrices (Garside 1987). In the data-driven approach, no human effort is needed for rulewriting. However, considerable effort may be needed for determining a workable tag set (cf. C</context>
E95-1022==>A92-1021!=<context citStr="Brill 1992" endWordPosition="642" position="4302" startWordPosition="641">ysis of several languages, in particular English (Marshall 1983; Black et al. 1992; Church 1988; Cutting et al. 1992; de Marcken 1990; DeRose 1988; Hindle 1989; Merialdo 1994; Weischedel et al. 1993; Brill 1992; Samuelsson 1994; Eineborg and Gamback 1994, etc.). Interestingly, no significant improvement beyond the 97% &amp;quot;barrier&amp;quot; by means of purely data-driven systems has been reported so far. In terms of the</context>
W02-1506==>P02-1035!=<context citStr="Riezler et al. (2002)" endWordPosition="385" position="2419" startWordPosition="382">ny direct quotes and proper names, (2a). In addition, for evaluation and training purposes we also parsed a version of this corpus marked up with labeled brackets and part-of-speech tags, as in (2b). Riezler et al. (2002) report on our WSJ parsing experiments. (2) a. But since 1981, Kirk Horse Insurance Inc. of Lexington, Ky. has grabbed a 20% stake of the market. b. But since 1981, [NP-SBJ Kirk Horse Insurance Inc. o</context>
W06-1656==>X96-1029!=<context citStr="Grishman 1996" endWordPosition="224" position="1485" startWordPosition="223"> the performance of its pattern learning component, which uses a simpler and less powerful pattern language than URES. 1 Introduction Information Extraction (IE) (Riloff 1993; Cowie and Lehnert 1996; Grishman 1996; Grishman 1997; Kushmerick, Weld et al. 1997; Freitag 1998; Freitag and McCallum 1999; Soderland 1999) is the task of extracting factual assertions from text. Benjamin Rosenfeld Computer Science Depa</context>
W98-0317==>P95-1018!=<context citStr="Moser and Moore (1995" endWordPosition="1013" position="6564" startWordPosition="1010">ne dialogues. Based on the above, we think it important for dialogue generation to select and set appropriate cue phrases at the beginning of discourse segments that refer to goals or direct actions. Moser and Moore (1995a) and Moser and Moore (1995b) investigated the relationship between cue placement and selection. They showed that the cue phrases are selected and distinguished depending on their placement. Somewhat</context>
W98-0317==>P95-1018!=<context citStr="Moser and Moore, 1995" endWordPosition="1439" position="9013" startWordPosition="1436">1 cue phrases based on the classification of Japanese connectives (Ichikawa, 1978; Moriyama, 1997) and cue phrase classification in English (Grosz and Sidner, 1986; Cohen, 1984: Knott and Dale, 1994; Moser and Moore, 1995b), 20 cue phrases, which occurred total of 848 times, were classified into three classes: changeover, such as soredeha, deha (&amp;quot;now&amp;quot; &amp;quot;now then&amp;quot; in English), conjunctive, such as sor;.de, de (-and&amp;quot;, -a</context>
W98-0317==>P95-1018!=<context citStr="Moser and Moore, 1995" endWordPosition="4680" position="28573" startWordPosition="4677">er-segment &amp;quot;sequence&amp;quot; relation. Elhadad and McKeown (1990), on the other hand, has presented a model for distinguishing connectives, which link two propositions, using some pragmatic constraints. In (Moser and Moore, 1995a; Moser and Moore, 1995b), the relationship between placement and selection of cue phrases was investigated using the core:contributor relations among units within a segment (Moser and Moore, 1995a).</context>
H05-1110==>A00-1002!=<context citStr="Hajic et al., 2000" endWordPosition="907" position="5591" startWordPosition="904"> MT work on related languages which explores language similarity in an opposite way: by using dictionaries and tools for both languages, and assuming that a near word-for-word approach is reasonable (Hajic et al., 2000). 4Much of recent MT research focuses on pairs of languages which are not related, such as English-Chinese, English-Arabic, etc. 3 Description of the Problem Let us assume that we have a group of rela</context>
W06-1627==>N03-1016!=<context citStr="Klein and Manning, 2003" endWordPosition="400" position="2520" startWordPosition="397">tatistical (monolingual) parsers (Charniak et al., 1998; Collins, 1999), and recent work has developed heuristics that are admissible for A* search, guaranteeing that the optimal parse will be found (Klein and Manning, 2003). We extend this type of outside probability estimate to include both word translation and n-gram language model probabilities. These measures have been used to guide search in word- or phrase-based M</context>
J96-2002==>E91-1023!=<context citStr="Reinhard and Gibbon 1991" endWordPosition="10869" position="66106" startWordPosition="10866">haracter of lexical phonology and morphophonology makes DATR a very suitable medium of representation (Bleiching 1992, 1994; Cahill 1993b; Gibbon 1990, 1992; Gibbon and Bleiching 1991; Reinhard 1990; Reinhard and Gibbon 1991). 28 It is straightforward to add extra DATR code so as to derive &lt;gender&gt; = feminine when &lt;gender_is_feminine&gt; is true and &lt;gender&gt; = masculine when &lt;gender_is_feminine&gt; is false, or conversely. 29 C</context>
J96-2002==>E91-1023!=<context citStr="Reinhard and Gibbon (1991)" endWordPosition="18303" position="111881" startWordPosition="18300">e natural to deal with the choice orthographically with a DAT R-coded FST of the kind discussed in Section 4.3, above, or morphophonologically using the kind of phonological representation adopted by Reinhard and Gibbon (1991), for example. Domenig and ten Hacken actually cite this latter paper in connection with German umlaut and suggest that the -s/-es alternation might be handled in the same way. However, they go on to </context>
J98-1005==>A92-1021!=<context citStr="Brill 1992" endWordPosition="2079" position="13713" startWordPosition="2078">restrict the test to senses within a single part of speech to focus the work on the harder part of the disambiguation problemthe accuracy of simple stochastic part-of-speech taggers such as Brill's (Brill 1992) suggests that distinguishing among senses with different parts of speech can readily be accomplished. The data set we use is identical to that of Leacock, Chodorow and Miller (this volume) with two e</context>
J82-1003==>P81-1036!=<context citStr="Thompson 1981" endWordPosition="10933" position="68048" startWordPosition="10932">nt of plurals in Schubert, 1982). 5. Parsing Phrase structure grammars are relatively easy to parse. The most advanced parser for Gazdar-style grammars that we are aware of is Thompson's chartparser (Thompson 1981), which provides for slash categories and coordination, but does not (as of this writing) generate logical translations. We have implemented two small parser-translators for preliminary experimentatio</context>
C96-2160==>P95-1013!=<context citStr="Kasper et al., 1995" endWordPosition="1616" position="9260" startWordPosition="1613">eps Figure 2: An example of DCP's execution daughter, such as signs tagged [1] and [2] in Figure 3. Kasper et al. presented an idea similar to this off-line raising in their work on HPSG-TAG compiler(Kasper et al., 1995). The difference is that our algorithm is based on substitution, not adjoining. Furthermore, it is not clear in their work how off-line raising is used to improve efficiency of parsing. Before giving </context>
W06-2503==>P04-1036!=<context citStr="McCarthy et al., 2004" endWordPosition="354" position="2149" startWordPosition="351">udicial trial is established or disproved Figure 1: The senses of evidence in WordNet to exploit the natural skew of the data and focus on finding the first (predominant) sense from a sample of text (McCarthy et al., 2004). Further contextual WSD may be required, but the technique provides a useful unsupervised back-off method. The other major problem for WSD is the granularity of the sense inventory since a pre-existi</context>
W06-2503==>P04-1036!=<context citStr="McCarthy et al., 2004" endWordPosition="1166" position="6995" startWordPosition="1163">to beat on an allwords task (Snyder and Palmer, 2004). We contrast the performance of first sense heuristics i) from SemCor (Miller et al., 1993) and ii) derived automatically from the sNC following (McCarthy et al., 2004) and also iii) an upper-bound first sense heuristic extracted from the test data. The paper is organised as follows. In the next section we describe some related work. In section 3 we describe the two</context>
W06-2503==>P04-1036!=<context citStr="McCarthy et al. (2004)" endWordPosition="3205" position="19100" startWordPosition="3202">where local context is not sufficient. Disambiguation is performed using the first sense heuristic from i) SemCor (Semcor FS) ii) automatic rankings from the BNC produced using the method proposed by McCarthy et al. (2004) (Auto FS) and iii) an upper-bound first sense heuristic from the SEVAL-2 ENG LEX data itself (SEVAL-2 FS). This represents how well the method would perform if we knew the first sense. The results ar</context>
W06-2503==>P04-1036!=<context citStr="McCarthy et al., 2004" endWordPosition="4628" position="27139" startWordPosition="4625">e lack of a plug and play application for feeding disambiguated data, we hope to examine the benefits on some lexical acquisition tasks that might feed into an application, for example sense ranking (McCarthy et al., 2004) or selectional preference acquisition. At this stage we have only experimented with nouns, we hope to go on relating senses in other parts-of-speech, particularly verbs since they have very fine-grai</context>
I05-2008==>P99-1042!=<context citStr="Hirschman et al. (1999)" endWordPosition="351" position="2265" startWordPosition="348">get materials. Although questions for second grade students also require world knowledge, it is expected that the questions become simpler and are resolved without tangled techniques. 2 Related Works Hirschman et al. (1999) and Charniak et al. (2000) proposed systems to solve Reading Comprehension. Hirschman et al. (1999) developed Deep Read, which is a system to select sentences in the text which include answers to</context>
P06-1109==>A97-1014!=<context citStr="Skut et al. 1997" endWordPosition="3230" position="19138" startWordPosition="3227">compared to U-DOP's f-score of 78.5% on the same data (Bod 2006). We next tested UML-DOP on two additional domains which were also used in Klein and Manning (2004) and Bod (2006): the German NEGRA10 (Skut et al. 1997) and the Chinese CTB10 (Xue et al. 2002) both containing 2200+ sentences &lt;_ 10 words after removing punctuation. Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Mann</context>
P95-1009==>J92-3003!=<context citStr="Russel et al., 1992" endWordPosition="281" position="1913" startWordPosition="278">ification are used instead of logical truth and consistency. There are three main advantages to Young and Rounds' work compared with other approaches to default unification (Houma, 1990; Bouma, 1992; Russel et al., 1992) which justify choosing it as a starting point for this work. The first is the separation of definite and default information, where Young and Rounds are more distinct than the other. The second is th</context>
C90-1012==>C86-1016!=<context citStr="[1, 2, 4]" endWordPosition="2053" position="12502" startWordPosition="2051">arks Some of the important features of the Generalized LR Parser/Compiler have been highlighted. More detailed descriptions can be found in its user's manual [9]. Unlike most other available software [1, 2, 4], the Generalized LR Parser/Compiler v8-4 is designed specifically to be used in practical natural language systems, sacrificing perhaps some of the linguistic and theoretical elegancy. The system has</context>
W04-2601==>J97-3005!=<context citStr="Kehler and Shieber 1997" endWordPosition="249" position="1738" startWordPosition="246">ull semantic interpretation of a text.2 Naturally, semantic ellipsis is important only in truly knowledge-rich ap1 Examples of NLP efforts to resolve syntactic ellipsis include Hobbs and Kehler 1997; Kehler and Shieber 1997; and Lappin 1992, among many others. 2 Some of the types of semantic underspecification treated here are described in the literature (e.g., Pustejovsky 1995) in theoretical terms, not as heuristic al</context>
N06-4004==>J03-1002!=<context citStr="Brown et al., 1993" endWordPosition="168" position="1201" startWordPosition="165">llections of translated documents. This can be be done at various levels. At the finest level, this involves the alignment of words and phrases within two sentences that are known to be translations (Brown et al., 1993; Och and Ney, 2003; Vogel et al., 1996; Deng and Byrne, 2005). Another task is the identification and alignment of sentence-level segments within document pairs that are known to be translations (Gal</context>
N06-4004==>J03-1002!=<context citStr="Brown et al., 1993" endWordPosition="949" position="6063" startWordPosition="946">s usually can serve as a starting point of sentence alignment. Alignment quality can be further improved when the chunking procedure is based on translation lexicons from IBM Model-1 alignment model (Brown et al., 1993). The MTTK toolkit also generates alignment score for each chunk pair, that can be utilized in post processing, for example in filtering out aligned segments of dubious quality. 2.2 Word and Phrase Al</context>
N06-4004==>J03-1002!=<context citStr="Brown et al., 1993" endWordPosition="1027" position="6562" startWordPosition="1024">hunk alignment procedures, statistical word and phrase alignment models can be estimated with EM algorithms. MTTK provides implementations of various alignment, models including IBM Model-1, Model-2 (Brown et al., 1993), HMM-based word-to-word alignment model (Vogel et al., 1996; Och and Ney, 2003) and HMM-based word-to-phrase alignment model (Deng and Byrne, 2005). After model parameters are estimated, the Viterbi </context>
N04-1014==>C00-1007!=<context citStr="Bangalore and Rambow, 2000" endWordPosition="302" position="2162" startWordPosition="299">ct F49620-00- 1-0337 and ARDA contract MDA904-02-C-0450. summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003). It is useful to understand </context>
P05-3014==>H93-1061!=<context citStr="Miller et al., 1993" endWordPosition="735" position="4702" startWordPosition="732">ated in real time. SENSELEARNER is attempting to learn general semantic models for various word categories, starting with a relatively small sense-annotated corpus. We base our experiments on SemCor (Miller et al., 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers. The input to the disambiguation algorithm consists of raw text. The output is a text wit</context>
C04-1115==>P98-2127!=<context citStr="Lin, 1998" endWordPosition="406" position="2826" startWordPosition="405"> Still, despite a wide variety of feature weighting methods existing in machine learning, these methods are poorly explored in application to lexical acquisition. There have been a few studies (e.g., Lin, 1998; Ciaramita, 2002; Alfonseca and Manandhar, 2002) where word representations are modified through this or that kind of feature weighting. But in these studies it is performed only as a standard pre-pr</context>
C04-1183==>C02-1002!=<context citStr="Tufis 2002" endWordPosition="263" position="1916" startWordPosition="262"> lexical level aligning techniques to establish word correspondences between the 4 languages. Though considerable progress has been made in this field (Dunning 1993, Dagan et al., 1993, Melamed 1998, Tufis 2002), this task remains difficult. The 75% accuracy of the best system for the translation spotting task, in the last Arcade campaign (Langlais and Vronis, 2000), showed that there was room for improveme</context>
P00-1058==>P96-1025!=<context citStr="Collins, 1996" endWordPosition="3309" position="19302" startWordPosition="3308"> the viability of TAG as a framework for statistical parsing. With LR &lt; 40 words &lt; 2 CB &lt; 100 words 0 CB &lt; 2 CB LP CB 0 CB LR LP CB (Magerman, 1995) 84.6 84.9 1.26 56.6 81.4 84.0 84.3 1.46 54.0 78.8 (Collins, 1996) 85.8 86.3 1.14 59.9 83.6 85.3 85.7 1.32 57.2 80.8 present model 86.9 86.6 1.09 63.2 84.3 86.2 85.8 1.29 60.4 81.8 (Collins, 1997) 88.1 88.6 0.91 66.5 86.9 87.5 88.1 1.07 63.9 84.6 (Charniak, 2000) 90</context>
E87-1022==>C86-1021!=<context citStr="Nagao and Tsujii, 1986" endWordPosition="2600" position="15916" startWordPosition="2597">tion relation As we have explained in section 1, the translation relation between languages is defined by attuning the grammars to each other. In this way complex structural transfer (as discussed in Nagao and Tsujii, 1986) can be avoided, but in some cases the dependency between the grammars may complicate individual grammars. Category mismatch is one of these translation problems, e.g. the graag/like case, where a Dut</context>
E87-1022==>C86-1021!=<context citStr="Nagao and Tsujii, 1986" endWordPosition="6090" position="36604" startWordPosition="6087">s and semi-auxiliaries, at least in the languages Rosetta deals with (Dutch, English and Spanish). In translation systems dealing with Japanese and English these phenomena occur more frequently. (cf. Nagao and Tsujii, 1986). Partial isomorphy of subgrammars Isomorphy between grammars of different languages must be defined in terms of isomorphy between the subgrammars of these languages. It should be noted that it is not</context>
W02-2030==>P00-1061!=<context citStr="Riezler et al., 2000" endWordPosition="286" position="1935" startWordPosition="283">ers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models a.k.a. Stochastic Unification Based Grammars (Johnson et al., 1999; Riezler et al., 2000). Here we also use log linear models to estimate conditional probabilities of sentence analyses. Since feature selection is almost prohibitive for these models, because of high computational costs, we</context>
W04-0509==>W03-1310!=<context citStr="Niu et al., 2003" endWordPosition="427" position="2575" startWordPosition="424">rdial infarction I: thrombolysis C:  O: mortality Our work in the project is to extend the keyword retrieval to a system that can answer questions expressed in natural language. In our earlier work (Niu et al., 2003), we showed that current technologies for factoid question answering (QA) are not adequate for clinical questions, whose answers must often be obtained by synthesizing relevant context. To adapt to th</context>
H92-1081==>H90-1036!=<context citStr="[2,3,5]" endWordPosition="223" position="1458" startWordPosition="223">cabulary sizes (V) of 5K or more words. This implementation also is incompatible with or intractable for many forms of LM, such as recursive or trigram models. A stack decoder [1,7,16] with fast match[2,3,5] is used here to overcome the limitations of the original decoder structure. Previous work focused on a 1K word task, Resource Management (RM)[18], which could be handled adequately with the TS decode</context>
H92-1081==>H90-1036!=<context citStr="[2,3,5]" endWordPosition="660" position="4240" startWordPosition="660"> (FM) to find a small number of potential successor words, evaluate the log-likelihood of these successors with the detailed matchs (DM), and insert the most promising new theories back onto the stack[2,3,5] This paradigm requires that theories of different lengths 399 be compared. Therefore, this system maintains a &amp;quot;leastupper-bound so-far&amp;quot; (lubsf) of all previously computed theory output log-likelihood</context>
W03-1107==>W02-1904!=<context citStr="Lai et al., 2002" endWordPosition="876" position="5563" startWordPosition="873">et al.(2001) studied QA and knowledge acquisition for definition type questions. Approaches by seeking any answer text in the pages of FAQs or newsgroups appeared in some studies(Hamada et al., 2002; Lai et al., 2002). Automatic QA systems in a support center of organizations was addressed in a study by Kurohashi et al.(2000). However, most of the previous studies targeting QA address fact type or definition type </context>
C04-1010==>A00-2018!=<context citStr="Charniak (2000)" endWordPosition="344" position="2234" startWordPosition="342">ish, the Penn Treebank (Marcus et al., 1993), is annotated primarily with constituent analysis. On the other hand, the best available parsers trained on the Penn Treebank, those of Collins (1997) and Charniak (2000), use statistical models for disambiguation that make crucial use of dependency relations. Moreover, the deterministic dependency parser of Yamada and Matsumoto (2003), when trained on the Penn Treeba</context>
C04-1010==>A00-2018!=<context citStr="Charniak, 2000" endWordPosition="2449" position="14440" startWordPosition="2448">ances that are equally distant to the test instance. This is different from the original IB1 algorithm, as described in Aha et al. (1991). used for training and section 23 for testing (Collins, 1999; Charniak, 2000). The data has been converted to dependency trees using head rules (Magerman, 1995; Collins, 1996). We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modific</context>
C04-1010==>A00-2018!=<context citStr="Charniak (2000)" endWordPosition="3183" position="18648" startWordPosition="3182"> .01 level; McNemars test.) Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3), Charniak (2000), and Yamada and Matsumoto (2003).5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministi</context>
C04-1010==>A00-2018!=<context citStr="Charniak (2000)" endWordPosition="3615" position="21249" startWordPosition="3614">d with a labeled attachment score of 84.4% for Model 2 with our B set, which is of about the same size as the set used by Buchholz, although the labels are not the same. In another study, Blaheta and Charniak (2000) report an F-measure of 98.9% for the assignment of Penn Treebank grammatical role labels (our G set) to phrases that were correctly parsed by the parser described in Charniak (2000). If null labels (</context>
W06-2912==>J02-1005!=<context citStr="Johnson 2002" endWordPosition="2563" position="15079" startWordPosition="2562">t should be kept in mind that while the probabilities of all parse trees generated by DOP sum up to 1, these probabilities do not converge to the &amp;quot;true&amp;quot; probabilities if the corpus grows to infinity (Johnson 2002). In fact, in Bod et al. (2003) we showed that the most probable parse tree as defined above has a tendency to be constructed by the shortest derivation (consisting of the fewest and thus largest subt</context>
W06-2912==>J02-1005!=<context citStr="Johnson (2002)" endWordPosition="2682" position="15791" startWordPosition="2681">d to Zuidema (2006) for a theoretical comparison of existing estimators for DOP. 1 As in Bod (2003) and Goodman (2003: 136), we additionally use a correction factor to redress DOP's bias discussed in Johnson (2002). 88 most appropriate one. For a subtle discussion on this issue, see Clark (2001) or Klein (2005). Step 3: Compute the most probable parse tree for each WSJ10 string While Goodman's reduction method </context>
P96-1042==>J93-1005!=<context citStr="Hindle and Rooth, 1993" endWordPosition="5158" position="32281" startWordPosition="5154">f the ambiguous word in question w, and f is a feature of occurrences of w. Common features are words in the context of w or morphological attributes of it.  In prepositional-phrase (PP) attachment (Hindle and Rooth, 1993): P(alf), where a is a possible attachment, such as an attachment to a head verb or noun, and f is a feature, or a combination of features, of the attachment. Corn325 mon features are the words involv</context>
C04-1170==>W04-0202!=<context citStr="Benamara et al. 2004" endWordPosition="323" position="2153" startWordPosition="320"> know-how of the responder. Cooperative know-how involves several forms of responses that include relaxations, intensional calculus, expression of restrictions, of warnings and conditional responses (Benamara et al. 2004). This simple example shows that, if a direct response cannot be found, several forms of knowledge and reasoning schemas need to be used and that the NL form of the response requires adequate and subt</context>
C04-1170==>W04-0202!=<context citStr="Benamara et al. 2004" endWordPosition="567" position="3660" startWordPosition="563">based on a detailed analysis of QA pairs, and on annotation tasks, from which a model emerged, suitable for NLG. 2 The WEBCOOP project and its environment This research is part of the WEBCOOP system (Benamara et al. 2004), that makes a heavy use of a rich domain ontology, which is a concept ontology structured by means of several semantic relations (Cruse 1986). To better understand the perspective in which this work </context>
C04-1170==>W04-0202!=<context citStr="Benamara et al. 2004" endWordPosition="790" position="5194" startWordPosition="786">gual (French and English) thesaurus of tourism and leisure activities (www.iztzg.hr/indokibiblioteka/THESAUR.PDF) which includes 2800 French terms. We manually integrated these ontologies in WEBCOOP (Benamara et al. 2004a) and added ourselves most properties by hand. We also revised slightly the ontology by removing concepts which are either too specific (i.e. too low level), such as some basic aspects of ecology or </context>
C04-1170==>W04-0202!=<context citStr="Benamara et al. 2004" endWordPosition="1030" position="6733" startWordPosition="1027">cing facts, which operate at the concept level, not at the lexicalization one: part-of(journey, trip). As shall be seen below, this ontology is futher annotated for lexicalization tasks. Finally, in (Benamara et al. 2004), we define a conceptual metrics that determines the distance between two concepts, evaluated in terms of distance in the hierarchy between the two concepts and property differences (nature of propert</context>
C04-1170==>W04-0202!=<context citStr="Benamara et al 2004" endWordPosition="3892" position="23831" startWordPosition="3889"> some elaboration if some concepts in the response are complex or quite remote from the original question focus. The implementation relies on the conceptual metrics, also used for concept relaxation (Benamara et al 2004a) and also on manually marked really complex concepts in the ontology. The lexicalization programme is simply able to propose additional lexicalizations, structured by the aggregation procedure, e.g.</context>
J96-3001==>P84-1075!=<context citStr="Shieber 1984" endWordPosition="101" position="765" startWordPosition="100">g for efficient processing for the analysis or synthesis of sentences using such grammars. 1. Introduction Formalisms equivalent to, or based on, unification grammars of the type exemplified by PATR (Shieber 1984) are very widely used in computational linguistics (Alshawi 1992; van Noord et al. 1990; Briscoe et al. 1987; Bobrow, Ingria, and Stallard 1991, etc.) A unification-based formalism has many well-known</context>
W06-1630==>J93-1006!=<context citStr="Kay and Roscheisen, 1993" endWordPosition="833" position="5273" startWordPosition="830">Rapp, 1995; Tanaka and Iwasaki, 1996). Recently, a Pearson correlation method was proposed to mine word pairs from comparable corpora (Tao and Zhai, 2005); this idea is similar to the method used in (Kay and Roscheisen, 1993) for sentence alignment. In our work, we adopt the method proposed in (Tao and Zhai, 2005) and apply it to the problem of transliteration; note that (Tao and Zhai, 2005) compares several different met</context>
N01-1029==>A00-2018!=<context citStr="Charniak, 2000" endWordPosition="3594" position="20157" startWordPosition="3593"> Treebank (PTB) (Marcus et al., 1995) using sections 2122 for development decisions and tested on sections 2324. The second set was trained on the BLLIP WSJ Corpus (BWC), which is a machine-parsed (Charniak, 2000) version of (a selection of) the ACL/DCI corpus, very similar to the selection made for the WSJ0/1 CSR corpus. As the training set, we used the BWC minus the WSJ0/1 dfiles and efiles intended for </context>
C02-1118==>P98-1071!=<context citStr="Gahl (1998)" endWordPosition="1062" position="6850" startWordPosition="1061"> of 43%. All three authors suppose the set of possible categories is relatively small and known in advance. Briscoe and Carroll (1997) also have a predefined set but it contains 160 different frames. Gahl (1998) presents an RE-based extraction tool that creates subcorpora of the BNC containing different subcategorization frames for verbs, nouns and adjectives. The resulting subcorpora can be used to determin</context>
W00-1425==>P98-2242!=<context citStr="Cheng, 1998" endWordPosition="886" position="5642" startWordPosition="885">s (e.g. cause). Semantic parataxis and hypotaxis feature in relationbased coherence and they depend on the text planner to put the related facts next to each other in order to perform a combination. (Cheng, 1998) describes interactions that need to be taken into account in aggregation. Firstly, complex embedded components like non-restrictive clauses may interrupt the semantic connection or syntactic similari</context>
W00-1425==>P98-2242!=<context citStr="Cheng, 1998" endWordPosition="2146" position="13341" startWordPosition="2145">ation about the referent. Bad embeddings are all those left, for example, if there is no available syntactic slot for the embedded part. Since semantic parataxis has a higher priority than embedding (Cheng, 1998), a good embedding should be less preferred than using a conjunct relation, but it should be preferred over a center continuation for it to happen. To decide the interaction between an embedding and a</context>
W01-0513==>P97-1004!=<context citStr="Jacquemin, et al., 1997" endWordPosition="949" position="6522" startWordPosition="946">wledge-driven Strategies Some researchers start with words and propose MWU induction methods that make use of parts of speech, lexicons, syntax or other linguistic structure (Justeson and Katz, 1995; Jacquemin, et al., 1997; Daille, 1996). For example, Justeson and Katz indicated that the patterns NOUN NOUN and ADJ NOUN are very typical of MWUs. Daille also suggests that in French, technical MWUs follow patterns such as</context>
E06-1034==>A00-1031!=<context citStr="Brants, 2000" endWordPosition="1431" position="8716" startWordPosition="1430"> focusing only on the variationflagged positions, we expect the tagger decisions to be more often correct than incorrect. We use two off-the-shelf taggers for correction, the Markov model tagger TnT (Brants, 2000) and the Decision Tree Tagger (Schmid, 1997), which we will abbreviate as DTT. Both taggers use probabilistic contextual and lexical information to disambiguate a tag at a particular corpus position. </context>
P97-1037==>P96-1021!=<context citStr="Wu, 1996" endWordPosition="564" position="3465" startWordPosition="563"> this section, we introduce a monotone HMAI based alignment and an associated DP based search algorithm for translation. Another approach to statistical machine translation using DP was presented in (Wu, 1996). The notational convention will be as follows. We use the symbol Pr(.) to denote general 289 Source Language Text 1 Transformation Transformation 1 Target Language Text Figure I: Architecture of the </context>
C90-2027==>P84-1075!=<context citStr="[13]" endWordPosition="325" position="2183" startWordPosition="325">supported by an LGF grant from the Land Baden-Wiirttemberg. For valuable comments on an earlier draft of this paper I am indebted to Christian Rohrer and Tobias Goeser. LFG [11], DCG [12] and PATR-11 [13]. Nevertheless, WACSG weakness concepts may also be implemented in monostratal formalisms as e.g. if PSG [14]. WACSG is dedicated to syntactical robustness, and not to morphosyntactic (spelling correc</context>
P05-2022==>J03-1002!=<context citStr="Och and Ney (2003)" endWordPosition="2933" position="18517" startWordPosition="2930">as well as to cope with the problem of non-correspondance across languages. That is why we chose the linguistic knowledge as the main information source. 5 Precision, recall and f-measure reported by Och and Ney (2003) for the intersection of IBM-4 Viterbi alignments from both translation directions. 131 7 Conclusion We have presented an efficient method for aligning words in English/French parallel corpora. It mak</context>
N03-1027==>J98-4004!=<context citStr="Johnson (1998)" endWordPosition="2796" position="16295" startWordPosition="2795">orporated the next word. Let k be the number of parses that have incorporated the next word, and let p  be the best probability from among that set. Then the probability of a parse must be above 2See Johnson (1998) for a presentation of the transform/detransform paradigm in parsing. 4 Empirical trials The parsing models were trained and tested on treebanks from the Penn Treebank II. For the Wall St. Journal por</context>
J05-3002==>N03-1004!=<context citStr="Chu-Carroll et al. 2003" endWordPosition="223" position="1568" startWordPosition="220">ation for applications such as summarization and question answering (Mani and Bloedorn 1997; Radev and McKeown 1998; Radev, Prager, and Samn 2000; Clarke, Cormack, and Lynam 2001; Dumais et al. 2002; Chu-Carroll et al. 2003). Clearly, it would be highly desirable to have a mechanism that could identify common information among multiple related documents and fuse it into a coherent text. In this article, we present a meth</context>
I05-3016==>J02-3003!=<context citStr="Miltsakaki, 2002" endWordPosition="2500" position="14732" startWordPosition="2498">M2 S Total overt 47 8 45 14 5 57 10 2 19 207 *pro* 11 17 48 4 10 65 15 21 81 272 58 25 93 18 15 122 25 23 100 479 These data are consistent with the observations made by Miltsakaki in her 2002 paper (Miltsakaki, 2002). Taking a main clause and all its dependent clauses as a unit, she found that there were different mechanisms needed to account for (1) topic continuity from unit to unit (intersentential), and (2) f</context>
P06-1071==>W03-1020!=<context citStr="Zhou et al., 2003" endWordPosition="149" position="1083" startWordPosition="146">tification demonstrate the benefits of the progressive feature selection (PFS) algorithm: the PFS algorithm maintains the same accuracy performance as previous CME feature selection algorithms (e.g., Zhou et al., 2003) when the same feature spaces are used. When additional features and their combinations are used, the PFS gives 17.66% relative improvement over the previously reported best result in edit region iden</context>
P06-1071==>W03-1020!=<context citStr="Zhou et al., 2003" endWordPosition="251" position="1730" startWordPosition="248">deling has received a great amount of attention within natural language processing community for the past decade (e.g., Berger et al., 1996; Reynar and Ratnaparkhi, 1997; Koeling, 2000; Malouf, 2002; Zhou et al., 2003; Riezler and Vasserman, 2004). One of the main advantages of CME modeling is the ability to incorporate a variety of features in a uniform framework with a sound mathematical foundation. Recent impro</context>
P06-1071==>W03-1020!=<context citStr="Zhou et al., 2003" endWordPosition="407" position="2717" startWordPosition="404"> propose a novel progressive feature selection (PFS) algorithm that addresses the feature space size limitation. The algorithm is implemented on top of the Selective Gain Computation (SGC) algorithm (Zhou et al., 2003), which offers fast training and high quality models. Theoretically, the new algorithm is able to explore an unlimited amount of features. Because of the improved capability of the CME algorithm, we a</context>
P06-1071==>W03-1020!=<context citStr="Zhou et al., 2003" endWordPosition="1020" position="6129" startWordPosition="1017">re selection algorithms are required to produce efficient and high quality models. This leads to a good amount of work in this area (Ratnaparkhi et al., 1994; Berger et al., 1996; Pietra et al, 1997; Zhou et al., 2003; Riezler and Vasserman, 2004) In the most basic approach, such as Ratnaparkhi et al. (1994) and Berger et al. (1996), training starts with a uniform distribution over all values of y and an empty fea</context>
P06-1071==>W03-1020!=<context citStr="Zhou et al., 2003" endWordPosition="1161" position="6949" startWordPosition="1158">very feature at each selection stage, and the computation of a parameter using Newtons method becomes expensive, considering that it has to be repeated many times. The idea behind the SGC algorithm (Zhou et al., 2003) is to use the gains computed in the previous step as approximate upper bounds for the subsequent steps. The gain for a feature needs to be re-computed only when the feature reaches the top of a prior</context>
P96-1018==>C90-3044!=<context citStr="Sato and Nagao, 1990" endWordPosition="221" position="1575" startWordPosition="218"> knowledge extraction (Kupiec, 1993; Matsumoto et al., 1993; Smadja et al., 1996; Dagan and Church, 1994; Kumano and Hirakawa, 1994; Haruno et al., 1996), machine translation (Brown and others, 1993; Sato and Nagao, 1990; Kaji et al., 1992) and information retrieval (Sato, 1992)). Most of these works assume voluminous aligned corpora. Many methods have been proposed to align bilingual corpora. One of the major approa</context>
P96-1018==>C90-3044!=<context citStr="Sato and Nagao, 1990" endWordPosition="4561" position="28619" startWordPosition="4558">ces are necessary for higher level applications such as well-grained translatiOn template acquisition (Matsumoto et al., 1993; Smadja et al., 1996; Haruno et al., 1996) and example-based translation (Sato and Nagao, 1990). Our method performs accurate alignment for such use by combining the detailed word correspondences: statistically acquired word correspondences and those from a bilingual dictionary of general use. </context>
P06-1126==>P04-1007!=<context citStr="Roark et al., 2004" endWordPosition="441" position="2845" startWordPosition="438">native training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004). To the best of our knowledge, it has not been applied to language model pruning. In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation. It </context>
C04-1154==>P03-2041!=<context citStr="Eisner, 2003" endWordPosition="355" position="2494" startWordPosition="354">ach cannot deal with complex or compound sentences. Other researchers (Imamura, 2001) also use phrasealignment in parsing but in DOT the translation fragments are already in the form of parse-trees. (Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. (Gildea, 2003) performs tree-totree</context>
C04-1154==>P03-2041!=<context citStr="Eisner, 2003" endWordPosition="698" position="4693" startWordPosition="697">2001) focus on using alignments to help resolve parsing ambiguities. As we wish to develop an alignment process for use in MT rather than parsing, this makes their approaches unsuitable for our use. (Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. He uses dynamic programming to break tree pairs into pairs of aligned elementary trees</context>
J94-3010==>E91-1023!=<context citStr="Reinhard and Gibbon 1991" endWordPosition="1915" position="13122" startWordPosition="1912">n the face of it, such benefits should extend beyond syntaxto phonology for example. Although there have been some valuable efforts to exploit inheritance and type hierarchies within phonology (e.g. Reinhard and Gibbon 1991), the potential of typed feature structures for this area has barely been scratched so far. In this section, we present a brief overview of HPSG (Pollard and Sag 1987), a constraint-based grammar form</context>
W05-0907==>W04-1013!=<context citStr="Lin, 2004" endWordPosition="1578" position="9201" startWordPosition="1577">arity metric between pairs of summaries if we consider only one model in the computation. There are different kinds of ROUGE metrics such as ROUGE-W, ROUGE-L, ROUGE1, ROUGE-2, ROUGE-3, ROUGE-4, etc. (Lin, 2004b). Each of these metrics has been applied over summaries with three preprocessing options: with stemming and stopword removal (type c); only with stopwords removal (type b); or without any kind of pr</context>
W05-0907==>W04-1013!=<context citStr="Lin, 2004" endWordPosition="3159" position="18357" startWordPosition="3158">. This suggests that there is some stylistic component in models that systems are not capturing in the topic-oriented task. 5 Related work The methodology which is closest to our framework is ORANGE (Lin, 2004a), which evaluates a similarity metric using the average ranks obtained by reference items within a baseline set. As in our framework, ORANGE performs an automatic meta-evaluation, there is no need f</context>
P95-1011==>C92-1034!=<context citStr="Vijay-Shanker &amp; Schabes (1992)" endWordPosition="314" position="2046" startWordPosition="311">nguage (LKRL) DATR (Evans &amp; Gazdar, 1989a; Evans &amp; Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an LTAG. The issue of efficient representation for LTAG' is discussed by Vijay-Shanker &amp; Schabes (1992), who lAs with all fully lexicalized grammar formalisms, there is really no conceptual distinction to be drawn in LTAG between the lexicon and the grammar: the grammatical rules are just lexical prope</context>
P95-1011==>C92-1034!=<context citStr="Vijay-Shanker &amp; Schabes (1992)" endWordPosition="3905" position="23549" startWordPosition="3902">y making multiple instances inherit from some common rule specification, but in our current treatment such instances would require different rule names. 6 Comparison with related work As noted above, Vijay-Shanker &amp; Schabes (1992) have also proposed an inheritance-based approach to this problem. They use monotonic inheritance to build up partial descriptions of trees: each description is a finite set of dominance, immediate do</context>
P98-1004==>P97-1063!=<context citStr="Melamed (1997" endWordPosition="169" position="1209" startWordPosition="168"> much progress have been made in the area of bilingual alignment for the support of tasks such as machine translation, machine-aided translation, bilingual lexicography and terminology. For instance, Melamed (1997a) reports that his word-to-word model for translational equivalence produced lexicon entries with 99% precision and 46% recall when trained on 13 million words of the Hansard corpus, where recall was</context>
P98-1004==>P97-1063!=<context citStr="Melamed (1997" endWordPosition="669" position="4357" startWordPosition="668">ve been based on the probabilistic translation models first proposed by Brown et al. (1988, 1990), especially Model 1 and Model 2. These models explicitly exclude multi-word units from consideration. Melamed (1997b), however, proposes a method for the recognition of multiword compounds in bitexts that is based on the predictive value of a translation model. A trial translation model that treat certain multi-wo</context>
P98-1004==>P97-1063!=<context citStr="Melamed (1997" endWordPosition="1630" position="10301" startWordPosition="1629">exical unit in one half of the bitext corresponds to at most one lexical unit in the other half. This can be seen as a generalization of the one-to-one assumption for word-to-word translation used by Melamed (1997a) and is exploited for the same purpose, i.e. to exclude large numbers of candidate alignments, when good initial alignments have been found. 2. Open class and closed class lexical units are usually </context>
P98-1004==>P97-1063!=<context citStr="Melamed (1997" endWordPosition="2255" position="14104" startWordPosition="2254">tracting program described in Merkel et al. (1994). 4.4 Basic operation The basic algorithm combines the K-vec approach, described by Fung and Church (1993), with the greedy word-to-word algorithm of Melamed (1997a). In addition, open class expressions are handled separately from closed class expressions, and sentences consisting of a single expression are handled in the manner of Tiedemann (1997). The algorit</context>
H89-2012==>J88-1003!=<context citStr="DeRose (1988)" endWordPosition="933" position="5692" startWordPosition="932">ting problem because of the possible confusion with the infinitive marker to. We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988) or DeRose (1988), and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to/in and verbs associated with a following infinitive</context>
W06-2925==>C96-1058!=<context citStr="Eisner (1996" endWordPosition="144" position="1004" startWordPosition="143">oNLL-X Shared Task on multilingual dependency parsing (Buchholz et al., 2006), for 13 different languages. Our system is a bottom-up projective dependency parser, based on the cubic-time algorithm by Eisner (1996; 2000). The parser uses a learning function that scores all possible labeled dependencies. This function is trained globally with online Perceptron, by parsing training sentences and correcting its p</context>
W06-2925==>C96-1058!=<context citStr="Eisner (1996" endWordPosition="618" position="3702" startWordPosition="617">ins the dependency. As described next, at scoring time y just contains the dependencies found between h and m. 2.2 Parsing Algorithm We use the cubic-time algorithm for dependency parsing proposed by Eisner (1996; 2000). This parsing algorithm assumes that trees are projective, that is, dependencies never cross in a tree. While this assumption clearly does not hold in the CoNLL-X data (only Chinese trees are </context>
W04-0822==>W02-1002!=<context citStr="Klein and Manning (2002)" endWordPosition="946" position="5983" startWordPosition="942">is model to be the most accurate classifier in a comparative study on a subset of Senseval-2 English lexical sample data. The second voting model, a maximum entropy model (Jaynes, 1978), was built as Klein and Manning (2002) found that it yielded higher accuracy than naive Bayes in a subsequent comparison of WSD performance. However, note that a different subset of either Senseval-1 or Senseval-2 English lexical sample d</context>
W06-1633==>P04-1018!=<context citStr="Luo et al., 2004" endWordPosition="215" position="1417" startWordPosition="212">ence to an entity1. Most of the previous coreference resolution methods have similar classification phases, implemented either as decision trees (Soon et al., 2001) or as maximum entropy classifiers (Luo et al., 2004). Moreover, these methods employ similar feature sets. The clusterization phase is different across current approaches. For example, there are several linking decisions for clusterization. (Soon et al</context>
W06-1633==>P04-1018!=<context citStr="Luo et al., 2004" endWordPosition="308" position="2042" startWordPosition="305">dent 1This definition was introduced in (NIST, 2003). candidate referent. Both these clustering decisions are locally optimized. In contrast, globally optimized clustering decisions were reported in (Luo et al., 2004) and (DaumeIII and Marcu, 2005a), where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization (LaSO) framework (DaumeII</context>
W06-1633==>P04-1018!=<context citStr="Luo et al., 2004" endWordPosition="527" position="3403" startWordPosition="524">n Min-Cut and a different way of stopping the cut2. Moreover, we have slightly modified the Min-Cut procedures. BESTCUT replaces the bottom-up search in a tree representation (as it was performed in (Luo et al., 2004)) with the top-down problem of obtaining the best partitioning of a graph. We start by assuming that all mentions refer to a single entity; the graph cut splits the mentions into subgraphs and the spl</context>
W06-1633==>P04-1018!=<context citStr="Luo et al., 2004" endWordPosition="723" position="4654" startWordPosition="720">mum entropy model for coreference classification. The experiments conducted on MUC and ACE data indicate state-of-the-art results when compared with the methods reported in (Ng and Cardie, 2002) and (Luo et al., 2004). The remainder of the paper is organized as follows. In Section 2 we describe the coreference resolution method that uses the BESTCUT clusterization; Section 3 describes the approach we have implemen</context>
W06-1633==>P04-1018!=<context citStr="Luo et al., 2004" endWordPosition="1193" position="7473" startWordPosition="1190">ntion mj.  gm,m,C P(C|mi, mj) = (1) Z(mi, mj) where 9k(mi, mj, C) is a feature and Ak is its weight; Z(mi, mj) is a normalizing factor. We created the training examples in the same way as (Luo et al., 2004), by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files. 2.2 Feature Representation We duplicated the statistic</context>
N06-2010==>P03-1070!=<context citStr="Nakano et al., 2003" endWordPosition="2375" position="14531" startWordPosition="2372">topic and sentence segmentation (e.g., (Shriberg et al., 2000)). We are aware of no equivalent work showing statistically significant improvement on unconstrained speech using hand gesture features. (Nakano et al., 2003) shows that body posture predicts turn boundaries, but does not show that these features improve performance beyond a text-only system. (Chen et al., 2004) shows that gesture may improve sentence segm</context>
C04-1047==>J93-2003!=<context citStr="Brown et al., 1993" endWordPosition="2104" position="12161" startWordPosition="2101">whether it accepts/rejects the top MT output in the systems N-best list by using the systems N-best. For scoring MT outputs, the proposed RSCM uses a score based on a translation model called IBM4 (Brown et al., 1993) (TM-score) and a score based on a language model for the translation target language (LM-score). As Akiba et al. (2002) reported, the products of TM-scores and LM-scores are statistical variables. Ev</context>
W06-3207==>P01-1063!=<context citStr="Snover and Brent, 2001" endWordPosition="5126" position="32266" startWordPosition="5123">th the proposed algorithm for learning phonology. Several algorithms have been proposed for automatically inducing morphological relations, like those assumed by the present learner (Goldsmith, 2001; Snover and Brent, 2001). The task of uncovering morphological relations is complicated by allomorphic alternations that obscure the underlying identity of related morphemes. While these algorithms are very promising, their </context>
E95-1024==>C94-1078!=<context citStr="KOnig, 1994" endWordPosition="1430" position="9195" startWordPosition="1429">, the problem of goaldirectedness is not solved. 2.2 Empty Heads Empty or displaced heads present the principal goaldirectedness problem for any head-driven generation approach (Shieber et al., 1990; KOnig, 1994; Gerdemann and Hinrichs, in press), where empty head refers not just to a construction in which the head has an empty phonology, but to any construction in which the head is partially unspecified. Si</context>
C94-2165==>C92-3141!=<context citStr="Mitjushin (1992)" endWordPosition="1435" position="8477" startWordPosition="1434">rom Table 1.1, A h captures local constraints: that prepositions are usually followed by a noun phrase, that 'and' usually is used as a noun co-ordinator (indicated by the high value for 'and-&gt;the'). Mitjushin (1992) has proposed similar links on a higher syntactic level, using a rule-based approach. We have deliberately tried to avoid talking about word-classes since it is misleading at this level of analysis. H</context>
J96-2002==>E93-1026!=<context citStr="Kilgarriff (1993)" endWordPosition="11964" position="72687" startWordPosition="11963">a. Thus, for example, Evans, Gazdar, and Weir (1995) report the use of DATR to formulate a lexical rule for Wh-questions in LTAG, inter alia. 34 Evaluable paths are not essential in this domain: thus Kilgarriff (1993) does not employ them in his DATR analysis of verbal alternations in the context of an H PSG lexicon, although he does use the standard encoding of argument lists. 35 Since our purpose here is exposit</context>
P96-1011==>E89-1002!=<context citStr="Hepple &amp; Morrill, 1989" endWordPosition="450" position="2923" startWordPosition="447">in only a small set of circumstances. Although similar work has been attempted in the past, with varying degrees of success (Karttunen, 1986; Wittenburg, 1986; Pareschi &amp; Steedman, 1987; Bouma, 1989; Hepple &amp; Morrill, 1989; Ki5nig, 1989; Vijay-Shanker &amp; Weir, 1990; Hepple, 1990; Moortgat, 1990; Hendriks, 1993; Niv, 1994), this appears to be the first full normal-form result for a categorial formalism having more than c</context>
P96-1011==>E89-1002!=<context citStr="Hepple &amp; Morrill, 1989" endWordPosition="1511" position="9342" startWordPosition="1508">ntically distinct. (Karttunen, 1986) suggests enforcing that property directlyby comparing each new analysis semantically with existing analyses in the cell, and refusing to add it if redundantbut (Hepple &amp; Morrill, 1989) observe briefly that this is inefficient for large charts.3 The following sections show how to obtain effectively the same result without doing any semantic interpretation or comparison at all. 4 A N</context>
P96-1011==>E89-1002!=<context citStr="Hepple &amp; Morrill, 1989" endWordPosition="3535" position="20912" startWordPosition="3532">urious ambiguity in CCG. 5 Extending the Approach to &amp;quot;Restricted&amp;quot; CCG The &amp;quot;pure&amp;quot; CCG of 4 is a fiction. Real CCG grammars can and do choose a subset of the possible rules. else a = 83 # 7 NF(0) -y 4(Hepple &amp; Morrill, 1989; Hepple, 1990; Hendriks, 1993) appear to share this view of semantic equivalence. Unlike (Karttunen, 1986), they try to eliminate only parses whose denotations (or at least A-terms) are systematicall</context>
P96-1011==>E89-1002!=<context citStr="Hepple &amp; Morrill, 1989" endWordPosition="5135" position="30127" startWordPosition="5132">ple constraints serve to eliminate all spurious ambiguity. It turns out that all spurious ambiguity arises from associative &amp;quot;chains&amp;quot; such as A/B B/C C or A/B/C C/D D/E\F/G G/H. (Wit8 5 tenburg, 1987; Hepple &amp; Morrill, 1989) anticipate this result, at least for some fragments of CCG, but leave the proof to future work. These normal-form results for pure CCG lead directly to useful parsers for real, restricted CCG grammar</context>
P93-1020==>J88-2006!=<context citStr="Webber, 1988" endWordPosition="271" position="1825" startWordPosition="270">rosz and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Grosz, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Grosz and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing syste</context>
P05-1007==>C02-1015!=<context citStr="Shaw, 2002" endWordPosition="373" position="2431" startWordPosition="372">gregation does. Further, we ran a controlled data collection in order to provide a more solid empirical base for aggregation rules than what is normally found in the literature, e.g. (Dalianis, 1996; Shaw, 2002). As regards NL interfaces for ITSs, research on the next generation of ITSs (Evens et al., 1993; Litman et al., 2004; Graesser et al., 2005) explores NL as one of the keys to bridging the gap between</context>
P05-1007==>C02-1015!=<context citStr="Shaw, 2002" endWordPosition="1403" position="8691" startWordPosition="1402">ConsultRu by DIAG-orig, DIAG-NLP1 and DIAG-NLP2 2.1 DIAG-NLP1: Syntactic aggregation DIAG-NLP11 (i) introduces syntactic aggregation (Dalianis, 1996; Huang and Fiedler, 1996; Reape and Mellish, 1998; Shaw, 2002) and what we call structural aggregation, namely, grouping parts according to the structure of the system; (ii) generates some referring expressions; (iii) models a few rhetorical relations; and (iv) </context>
P05-1007==>C02-1015!=<context citStr="Shaw, 2002" endWordPosition="4174" position="25392" startWordPosition="4173">r, 1996; Horacek, 2002), aggregation rules and heuristics are shown to be plausible, but are not based on any hard evidence. Even where corpus work is used (Dalianis, 1996; Harvey and Carberry, 1998; Shaw, 2002), the results are not completely convincing because we do not know for certain the content to be communicated from which these texts supposedly have been aggregated. Therefore, positing empirically ba</context>
P02-1030==>P93-1024!=<context citStr="Pereira et al., 1993" endWordPosition="746" position="4682" startWordPosition="743"> and syntactic information from the words surrounding the target term, which is then converted into a vector-space representation of the contexts that each target term appears in (Brown et al., 1992; Pereira et al., 1993; Ruge, 1997; Lin, 1998b). Other systems take the whole document as the context and consider term co-occurrence at the document level (Crouch, 1988; Sanderson and Croft, 1999). Once these contexts hav</context>
W06-0202==>P03-1054!=<context citStr="Klein and Manning, 2003" endWordPosition="2413" position="14822" startWordPosition="2410">dency Patterns Three dependency parsers were used for these experiments: MINIPAR3 (Lin, 1999), the Machinese Syntax4 parser from Connexor Oy (Tapanainen and J arvinen, 1997) and the Stanford5 parser (Klein and Manning, 2003). These three parsers represent a cross-section of approaches to producing dependency analyses: MINIPAR uses a constituency grammar internally before converting the result to a dependency tree, Machin</context>
W06-0202==>P03-1054!=<context citStr="Klein and Manning, 2003" endWordPosition="4606" position="28076" startWordPosition="4603">ext which can be useful for IE. For practical applications this approach relies on the ability to accurately generate dependency analyses. The results presented here suggest that the Stanford parser (Klein and Manning, 2003) is capable of generating analyses for almost all sentences within corpora from two very different domains. Bunescu and Mooney (2005) have also demonstrated that dependency graphs can be produced usin</context>
P01-1022==>A97-1001!=<context citStr="Stolcke and Shriberg 1996" endWordPosition="227" position="1551" startWordPosition="224">ore critical good language modeling becomes. Research in language modeling has heavily favored statistical approaches (Cohen 1995, Ward 1995, Hu et al. 1996, Iyer and Ostendorf 1997, Bellegarda 1999, Stolcke and Shriberg 1996) while handcoded finite-state or context-free language models dominate the commercial sector (Nuance 2001, SpeechWorks 2001, TellMe 2001, BeVocal 2001, HeyAnita 2001, W3C 2001). The difference revolve</context>
N03-1021==>P85-1015!=<context citStr="Johnson (1985)" endWordPosition="2113" position="11591" startWordPosition="2112"> to know their positions in the input multitext, but not their internal structure. However, items with discontinuities need to remember all their boundaries, not just the outermost ones. Expanding on Johnson (1985), we define a discontinuous span (or dspan, for short) as a list of zero or more intervals a = r,;. .; r,m), where  the I, are left boundaries and the ri are right boundaries between word positions i</context>
J90-2003==>C86-1016!=<context citStr="Kartunnen 1986" endWordPosition="10021" position="61446" startWordPosition="10020">nbounded dependency. 4. &amp;quot;Quantifier binding&amp;quot; in clausal comparatives; we discuss this in Section 4.1. All these forms of long-range dependency are handled by application of &amp;quot;threading&amp;quot; (Pereira 1983; Kartunnen 1986). Properly speaking, each distinct type of dependency ought to be associated with a distinct feature, which will be present in relevant constituents; by unifying features in different constituents aga</context>
P96-1038==>H92-1089!=<context citStr="Hirschberg and Grosz, 1992" endWordPosition="842" position="5718" startWordPosition="839">er discourse structure itself can be empirically determined in a reliable manner, a prerequisite to investigating linguistic cues to its existence. An intention-based theory of discourse was used in (Hirschberg and Grosz, 1992; Grosz and Hirschberg, 1992) to identify intonational correlates of discourse structure in news stories read by a professional speaker. Discourse structural elements were determined by experts in the</context>
P96-1038==>H92-1089!=<context citStr="Hirschberg and Grosz (1992)" endWordPosition="1488" position="10012" startWordPosition="1485">r DISCOURSE SEGMENT PURPOSES (DSPs) underlying the discourse and relations between DSPs. Two methods of discourse segmentation were employed by subjects who had expertise in the G&amp;S theory. Following Hirschberg and Grosz (1992), three subjects labeled from text alone (group T) and three labeled from text and speech (group S). Other than this difference in input modality, all subjects received identical written instructions.</context>
P96-1038==>H92-1089!=<context citStr="Hirschberg and Grosz (1992)" endWordPosition="1695" position="11300" startWordPosition="1692"> (S) 18% 14% 49% 80% SPON (N=552) Text alone (T) 13% 10% 40% 61% Text &amp; Speech (S) 15% 13% 54% 81% Note that group T and group S segmentations differ significantly, in contrast to earlier findings of Hirschberg and Grosz (1992) on a corpus of readaloud news stories and in support of informal findings of Swerts (1995). Table 1 shows that group S produced significantly more consensus boundaries for both read (p&lt;.001, x=58.8, </context>
P96-1038==>H92-1089!=<context citStr="Hirschberg and Grosz, 1992" endWordPosition="2044" position="13516" startWordPosition="2041">For comparative purposes, we explored several measures proposed in the literature, namely, COCHRAN'S Q and the KAPPA (K) COEFFICIENT (Siegel and Castellan, 1988). Cochran's Q, originally proposed in (Hirschberg and Grosz, 1992) to measure the likelihood that similarity among labelings was due to chance, was not useful in the current study; all tests of similarity using this metric (pairwise, or comparing all labelers) gave </context>
W98-0310==>E93-1031!=<context citStr="Lascarides &amp; Oberlander (1994)" endWordPosition="1908" position="11865" startWordPosition="1905">wise the temporal relation between John's shyness and his change of behavior cannot be derived. Bear in mind that temporal relations are by-products of rhetorical relations within the SDRT framework. Lascarides &amp; Oberlander (1994) propose an account that takes more into account the interaction between world knowledge and the underlying rhetorical relations. However, they do not consider the effect different syntactic variants </context>
W03-0432==>A97-1029!=<context citStr="Bikel et al., 1997" endWordPosition="790" position="4947" startWordPosition="787">ith words without accurate case information, and various workarounds have been exploited. Most commonly, feature-based classifiers use a set of capitalisation features and a sentence-initial feature (Bikel et al., 1997). Chieu and Ng used global information such as the occurrence of the same word with other capitalisation in the same document (Chieu and Ng, 2002a), and have also used a mixed-case classifier to teach</context>
W06-1631==>W98-1005!=<context citStr="Stalls and Knight (1998)" endWordPosition="1445" position="9337" startWordPosition="1442"> and are transliterated into probable Arabic forms. In contrast, we aim to identify foreign words as a within Arabic text which is made difficult by the absence of such easily perceptible difference. Stalls and Knight (1998) describe research to determine the original foreign word from its Arabic version; this is known as back transliteration. However, rather than using automatic methods to identify foreign words, they u</context>
P06-1040==>A00-2030!=<context citStr="Miller et al. (2000)" endWordPosition="1070" position="6327" startWordPosition="1067"> Riloff and Jones (1999) use a mutual bootstrapping technique that can find patterns automatically, but the bootstrapping requires an initial seed of manually chosen examples for each class of words. Miller et al. (2000) propose an approach to relation extraction that was evaluated in the Seventh Message Understanding Conference (MUC7). Their algorithm requires labeled examples of each relation. Similarly, Zelenko et</context>
H05-1035==>A00-2018!=<context citStr="Charniak, 2000" endWordPosition="638" position="3986" startWordPosition="636">biguity is based on a Support Vector Machines learner (Cortes and Vapnik, 1995). The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions. Some of these features were proven efficient for semantic information labeling (Gildea and Jurafsky, 2002). The feature set also</context>
H94-1009==>H92-1073!=<context citStr="[3]" endWordPosition="1532" position="9853" startWordPosition="1532">ata from subjects reading selected articles from the Wall Street Journal. The prompting texts for the WSJ-based tests came from the pre-defined evaluation test text pools specified in the WSJO corpus [3] which consists of articles from the Wall Street Journal published during the years 1987-1989. Typical tests used 10 subjects reading 20-40 sentences each. Each test had equal numbers of male and fema</context>
W02-2002==>W95-0107!=<context citStr="Ramshaw and Marcus, 1995" endWordPosition="464" position="2895" startWordPosition="461">hod, a very efficient method which has been successfully attempted for other NLP tasks like PP-attachment disambiguation (Brill and Resnik, 1994), part-of-speech tagging (Brill, 1995), text chunking (Ramshaw and Marcus, 1995), dialog act tagging (Samuel et al., 1988) ellipsis resolution (Hardt, 1998) and spelling correction (Magnu and Brill, 1997). Our second approach is a simple decision tree induction scheme. In the nex</context>
I05-5012==>P99-1071!=<context citStr="Barzilay et al., 1999" endWordPosition="465" position="3126" startWordPosition="462">es of generating non-verbatim sentences. This approach treats 1These are available publically at http://www.reliefweb.com. 2Paraphrase here includes sentences generated in an Information Fusion task (Barzilay et al., 1999). 88 sentence generation as a search problem. Given a set of words (taken from some set of sentences to paraphrase), we search for the most likely sequence given some language model. Intuitively, we w</context>
I05-5012==>P99-1071!=<context citStr="Barzilay et al., 1999" endWordPosition="1871" position="11839" startWordPosition="1867">elief workers distributed food to the hungry. and The UN workers requested medicine and blankets., the task is to generate a single sentence that contains material from these two sentences. As in (Barzilay et al., 1999), we assume that the sentences stem from the same event and thus, references can be fused together. Imagine also that bigram frequencies have been collected from a relevant UN Humanitarian corpus. Fig</context>
I05-5012==>P99-1071!=<context citStr="Barzilay et al., 1999" endWordPosition="2834" position="17527" startWordPosition="2830">ing for the best word sequence and dependency tree structure concurrently. Summarization researchers have also studied the problem of generating non-verbatim sentences: see (Jing and McKeown, 1999), (Barzilay et al., 1999) and more recently (Daum e III and Marcu, 2004). Jing uses a HMM for learning alignments between summary and source sentences. Daume III also provides a mechanism for sub-sentential alignment but allo</context>
I05-5012==>P99-1071!=<context citStr="Barzilay et al., 1999" endWordPosition="3331" position="20703" startWordPosition="3327">ated-verb-relations The corresponding recall metric is defined as: recall countsource-text-verb-relations The data for our evaluation cases is taken from the information fusion data collected by (Barzilay et al., 1999). This data is made up of news articles that have first been grouped by topic, and then component topic sentences further clustered by similarity of event. We use 100 sentence clusters and on average </context>
C96-2139==>C90-2049!=<context citStr="Nagao, 1990" endWordPosition="961" position="6657" startWordPosition="960">n on synonyms extracted from an on-line thesaurus dictionary and information on word sense and structural disambiguation extracted from an example base, such as one described ill (Uramoto, 1991) and (Nagao, 1990), may be added to the context. model. 2.2 Refinement of the context model in the first step, a syntactic parser may not always generate a single unified parse tree for each sentence in the source text</context>
P06-2061==>J93-2003!=<context citStr="Brown et al., 1993" endWordPosition="407" position="2663" startWordPosition="404">4), and researchers involved in the TransTalk project (Dymetman et al., 1994; Brousseau et al., 1995). In (Brown et al., 1994), the authors proposed a method to integrate the IBM translation model 2 (Brown et al., 1993) with an ASR system. The main idea was to design a language model (LM) to combine the trigram language model probability with the translation probability for each target word. They reported a perplexi</context>
P06-2061==>J93-2003!=<context citStr="Brown et al., 1993" endWordPosition="2070" position="12494" startWordPosition="2067">5 16.6 18.2 Phrase 18.8 20.3 -based better MT system, and generating a larger N-best list from the ASR word graphs. We rescore the ASR N-best lists with the standard HMM (Vogel et al., 1996) and IBM (Brown et al., 1993) MT models. The development and evaluation sets Nbest lists sizes are sufficiently large to achieve almost the best possible results, on average 1738 hypotheses per each source sentence are extracted </context>
P06-2061==>J93-2003!=<context citStr="Brown et al., 1993" endWordPosition="3618" position="21512" startWordPosition="3615">of the recognized words based on the source language text, and it seems that the single word based MT models are more suitable than phrase-based model in this task. 4.7 Fertility-Based Transducer In (Brown et al., 1993), three alignment models are described that include fertility models, these are IBM Models 3, 4, and 5. The fertility-based alignment models have a more complicated structure than the simple IBM Model</context>
H93-1036==>J92-4003!=<context citStr="Brown et al., 1992" endWordPosition="921" position="5977" startWordPosition="918">onaries and corpora are imperfect sources of knowledge, so we still employ human effort to check the results of our semiautomatic algorithms. This is in contrast to purely statistical systems (e.g., [Brown et al., 1992]), which are difficult to inspect and modify. There has been considerable use in the NLP community of both WordNet (e.g., [Lehman el al., 1992; Resnik, 1992]) and LDOCE (e.g..., [Liddy el al., 1992; </context>
J94-4003==>J93-1007!=<context citStr="Smadja 1993" endWordPosition="536" position="3607" startWordPosition="535">relations between verbs or nouns and their arguments and modifiers) for various purposes has received growing attention in recent research (Church and Hanks 1990; Zernik and Jacobs 1990; Hindle 1990; Smadja 1993). More specifically, two recent works have suggested using statistical data on lexical relations for resolving ambiguity of prepositional phrase attachment (Hindle and Rooth 1991) and pronoun referenc</context>
J94-4003==>J93-1007!=<context citStr="Smadja 1993" endWordPosition="9899" position="60880" startWordPosition="9898">n a local context were used recently for monolingual word sense disambiguation (Gale, Church, and Yarowsky 1992b, 1993; Schiitze 1992, 1993) (see Section 7 for more details and Church and Hanks 1990; Smadja 1993, for other applications of these statistics). It is possible to apply these methods using statistics of the target language and thus incorporate them within the framework proposed here for target wor</context>
C04-1202==>C96-2166!=<context citStr="Zechner 1996" endWordPosition="967" position="5933" startWordPosition="966">System Architecture In addition to the traditional way of extracting the highest ranked sentences in a document to compose a summary as in (Edmundson 1969; Lin 1999; Kupiec et al. 1995; Brandow 1995; Zechner 1996), we embedded a machine learning mechanism in our system. The system architecture is shown in Figure 1 where the GEP module is highlighted. In the training stage, each of the training documents is pas</context>
C04-1202==>C96-2166!=<context citStr="Zechner 1996" endWordPosition="1561" position="9387" startWordPosition="1560">sk, we designed three baseline methods, namely lead-based, randomly-selected, and random-lead-based, to generate summaries for performance comparison, which were also adopted by (Brandow et al. 1995; Zechner 1996; Radev et al. 2003). The baseline methods are detailed as Extractive Summary Figure 1: System Architecture 2 CMPLG corpus is composed of 183 documents from the Computation and Language (cmp-lg) colle</context>
W05-0836==>N04-1022!=<context citStr="Kumar and Byrne, 2004" endWordPosition="1288" position="7985" startWordPosition="1285">lternative decision rules. 2.2 The Minimum Bayes Risk Decision Rule The Minimum Bayes Risk Decision Rule as proposed by (Mangu et al., 2000) for the Word Error Rate Metric in speech recognition, and (Kumar and Byrne, 2004) when applied to translation, changes the decision rule in (2) to select the translation that has the lowest expected loss E[Loss(e, r)], which can be estimated by considering a weighted Loss between </context>
W05-0836==>N04-1022!=<context citStr="Kumar and Byrne, 2004" endWordPosition="1358" position="8409" startWordPosition="1355">and the elements of the n-best list, the approximation to E, as described in (Mangu et al., 2000). The resulting decision rule is: 1: transl(f) = arg min Loss(e,e')p(e'Jf) . eEGen(f) e,EGen(f) (4) (Kumar and Byrne, 2004) explicitly consider selecting both e and a, an alignment between the English and French sentences. Under a phrase based translation model (Koehn et al., 2003; Marcu and Wong, 2002), this distinction </context>
W05-0836==>N04-1022!=<context citStr="Kumar and Byrne (2004)" endWordPosition="2676" position="16085" startWordPosition="2673">nslation lattice. Selecting a particular path means in fact selecting the pair (e, s), where s is a segmentation of the the source sentence f into phrases and alignments onto their translations in e. Kumar and Byrne (2004) represent this decision explicitly, since the Loss metrics considered in their work evaluate alignment information as well as lexical (word) level output. When considering lexical scores as we do her</context>
W05-0836==>N04-1022!=<context citStr="Kumar and Byrne (2004)" endWordPosition="3479" position="20811" startWordPosition="3475">e corpus. To use the BLEU metric in the candidate pairwise loss calculation in (4), we need to make a decision regarding cases where higher order n-grams matches are not found between two candidates. Kumar and Byrne (2004) suggest that if any n-grams are not matched then the pairwise BLEU score is set to zero. As an alternative we first estimate corpuswide n-gram counts on the development set. When the pairwise counts </context>
E03-1006==>C02-1036!=<context citStr="Gamon et al. (2002" endWordPosition="360" position="2429" startWordPosition="357">unt for French. The purpose of this paper is to focus on the adaptation of Amalgam to French. Discussions about the general architecture of the system can be found in Corston-Oliver et al. (2002) and Gamon et al. (2002b). 1 Overview of German Amalgam Amalgam takes as its input a logical form graph, i.e., a sentence-level dependency graph with fixed lexical choices for content words. This graph represents the predic</context>
E03-1006==>C02-1036!=<context citStr="Gamon et al., 2002" endWordPosition="493" position="3330" startWordPosition="490">egation, insertion of punctuation, morphological inflection, and capitalization, an output string is read off the leaf nodes. The contexts for most of these linguistic operations are machine-learned (Gamon et al., 2002a). Figure 1 lists the eight stages in German Amalgam: the label ML denotes that the operation is applied in machinelearned contexts, and the label Proc indicates that the operation is procedural or d</context>
E03-1006==>C02-1036!=<context citStr="Gamon et al., 2002" endWordPosition="669" position="4506" startWordPosition="666"> (ML) Stage 9 Inflectional generation (Proc) Figure 1 The stages of German Amalgam All machine-learned components employ decision trees for classification and for probability distribution estimation (Gamon et al., 2002b). The decision trees are built with the WinMine toolkit (Chickering, 2002). There are a total of twenty-one decision trees in the German system. The complexity of the decision trees varies with the </context>
E03-1006==>C02-1036!=<context citStr="Gamon et al. 2002" endWordPosition="1260" position="8192" startWordPosition="1257">obability distributions involved in ordering (see Ringger et al. (in preparation) for a detailed discussion of different approaches to constituent ordering). Extraposition, which is common in German (Gamon et al. 2002c), is rare in the French technical software manuals: there were too few examples of extraposition in the French data to train an extraposition model for Stage 6. Stage 7 (clean-up) uses language-spec</context>
J94-3001==>J92-1003!=<context citStr="Ritchie (1992)" endWordPosition="18983" position="113267" startWordPosition="18982">ss of the regular relations, those that permit identity prefixes and suffixes of unbounded length to surround any nonidentity correspondences. It is interesting to note that for much the same reason, Ritchie (1992) also made crucial use of two-level boundary-context rules to prove that the relations denoted by Koskenniemi's (1985) two-level grammars are also coextensive with the regular relations. Moreover, put</context>
J94-3001==>J92-1003!=<context citStr="Ritchie (1992)" endWordPosition="19862" position="119157" startWordPosition="19861">ere used to translate this notation into the equivalent regular relations and corresponding transducers, and thus to create a compiler for a more intuitive and more tractable two-level rule notation. Ritchie (1992) summarizes aspects of this analysis as presented by Kaplan (1988). Ritchie et al. (1992) describe a program that interprets this notation by introducing and manipulating labels assigned to the states</context>
J94-3001==>J92-1003!=<context citStr="Ritchie (1992)" endWordPosition="23348" position="139419" startWordPosition="23347">n [e:#/d (Em*0 # Em*o) :#] at the beginning of the four-level cascade and compose its inverse at the end. As we mentioned before, the two-level grammars with boundary-context rules are the ones that Ritchie (1992) showed were complete for the regular relations. In reasoning about these systems, it is important to keep clearly in mind the distinction between the outer and inner relations. Ritchie (1992), for ex</context>
J05-1005==>A92-1014!=<context citStr="Sekine et al. 1992" endWordPosition="1972" position="13079" startWordPosition="1969">ning Linguistic Requirements During the last years, various stochastic approaches to linguistic requirements acquisition have been proposed (Basili, Pazienza, and Velardi 1992; Hindle and Rooth 1993; Sekine et al. 1992; Grishman and Sterling 1994; Framis 1995; Dagan, Marcus, and Markovitch 1995; Resnik 1997; Dagan, Lee, and Pereira 1998; Marques, Lopes, and Coelho 2000; Ciaramita and Johnson 2000). In general, they</context>
J05-1005==>A92-1014!=<context citStr="Sekine et al. 1992" endWordPosition="2380" position="15757" startWordPosition="2377">d Pereira 1998; Marques, Lopes, and Coelho 2000). Finally, it can be identified as the head or the dependent role within a binary grammatical relationship such as subject, direct object, or modifier (Sekine et al. 1992; Grishman and Sterling 1994; Framis 1995). In section 4, we pay special attention to the grammatical characterization of syntactic positions. As far as cond is concerned, various types of information</context>
J05-1005==>A92-1014!=<context citStr="Sekine et al. 1992" endWordPosition="3044" position="19989" startWordPosition="3041">oaches, the linguistic conditions used to characterize requirements may be situated at the lexical level (Dagan, Lee, and Pereira 1998; Dagan, Marcus, and Markovitch 1995; Grishman and Sterling 1994; Sekine et al. 1992). A pair like ((right, approve), law) matches those expressions containing a form of lemma law (e.g., law, laws, Law, Laws) appearing to the right of the verb approve (to be more precise, to the right</context>
J05-1005==>A92-1014!=<context citStr="Sekine et al. (1992)" endWordPosition="4053" position="26792" startWordPosition="4050">o be smoothed (generalized). Computations involving similar words minimize the data sparseness problem to a certain extent. Lexical methods provided with similarity-based generalizations are found in Sekine et al. (1992), Grishman and Sterling (1994), and Dagan, Lee, and Pereira (1998). Later, in section 8.3.4, we use a lexical method with similarity-based generalization to solve syntactic attachments. The results ob</context>
J05-1005==>A92-1014!=<context citStr="Sekine et al. 1992" endWordPosition="14326" position="93453" startWordPosition="14323">eing able to be instantiated by two different functions: both a direct object and a subject (section 6.1). Most works on attachment resolution use as test data only phrase sequences of type vpnppp (Sekine et al. 1992; Hindle and Rooth 1993; Ratnaparkhi, Reymar, and Roukos 1994; Collins and Brooks 1995; Li and Abe 1998; Niemann 1998; Grishman and Sterling 1994). These approaches consider each sequence selected for</context>
J98-1006==>J94-4003!=<context citStr="Dagan and Itai 1994" endWordPosition="175" position="1209" startWordPosition="172">proaches that rely on definitions (Vdronis and Ide 1990; Wilks et al. 1993) to corpus-based approaches that use only word cooccurrence frequencies extracted from large textual corpora (Schtitze 1995; Dagan and Itai 1994). We have drawn on these two traditions, using corpus-based co-occurrence and the lexical knowledge base that is embodied in the WordNet lexicon. The two traditions complement each other. Corpus-based</context>
J98-1006==>J94-4003!=<context citStr="Dagan and Itai (1994)" endWordPosition="633" position="4330" startWordPosition="630">ned from manually tagged training materials. 2. Corpus-based Statistical Sense Identification Work on automatic sense identification from the 1950s onward has been well summarized by Hirst (1987) and Dagan and Itai (1994). The discussion below is limited to work that is closely related to our research. 2.1 Some Recent Work Hearst (1991) represents local context with a shallow syntactic parse in which the context is se</context>
J98-1006==>J94-4003!=<context citStr="Dagan and Itai (1994)" endWordPosition="4000" position="24775" startWordPosition="3997">fy these situations and allow it to respond do not know rather than forcing a decision. What is needed is a measure of the difference in the probabilities of the two senses. Following the approach of Dagan and Itai (1994), we use the log of the ratio of the probabilities In(pi/p2) for this purpose. Based on this value, a threshold e can be set to control when the classifier selects the most probable sense. For example</context>
M93-1004==>M93-1005!=<context citStr="[5]" endWordPosition="3544" position="23028" startWordPosition="3544">different extraction tasks, quantitative criteria were developed in support of MUC-3 that enable comparison in terms of superficial features of the texts, template definition, and template fill rules [5]. Comparison of the complexity of the terrorist task with the naval task in light of these criteria shows at least an order-of-magnitude increase for several of the criteria. Once allowances are made </context>
P06-2022==>W02-1028!=<context citStr="Thelen and Riloff (2002)" endWordPosition="5050" position="30515" startWordPosition="5047"> nonevents), only that the presence of certain features make an event interpretation more or less likely. This justifies our probabilistic Bayesian approach, which performs well given its simplicity. Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. For events, only 27% of the 1,000 learned words are correct. Their experiments w</context>
W06-2105==>W02-0802!=<context citStr="Litkowski (2002)" endWordPosition="288" position="2043" startWordPosition="287">antics which has sufficient coverage for NLP applications (Litkowski and Hargraves, 2005; SaintDizier, 2005). The automatic interpretation of prepositions in English has been tackled, for example, by Litkowski (2002), who presents handcrafted disambiguation rules, and OHara and Wiebe (2003), who propose a statistical approach based on collocations. However, in order to be applicable for semantic inference, the r</context>
E03-1022==>J93-1007!=<context citStr="Smadja (1993)" endWordPosition="553" position="3754" startWordPosition="552">n much addressed in the literature, in particular since the work of Church at al. (1991), and several statistical packages have been designed for this purpose (see for instance, the X tract system of Smadja (1993)). Although very effective, those systems suffer from the fundamental weakness that the measure of relatedness they use is essentially the linear proximity of two or more words. As pointed out above, </context>
H94-1027==>P91-1034!=<context citStr="[8]" endWordPosition="2842" position="17357" startWordPosition="2842"> bilingual data has attracted interest in several areas, including sentence alignment [10], [2], [11], [1], [4], word alignment [6], alignment of groups of words [3], [7], and statistical translation [8]. Of these, aligning groups of words is most similar to the work reported here, although we consider a greater variety of groups. Note that additional research using bilingual corpora is less related </context>
H94-1027==>P91-1034!=<context citStr="[8]" endWordPosition="2907" position="17781" startWordPosition="2907">for example, word sense disambiguation in the source language by examining different translations in the target [9], [8]. One line of research uses statistical techniques only for machine translation [8]. Brown a. al. use a stochastic language model based on the techniques used in speech recognition [19], combined with translation probabilities compiled on the aligned corpus in order to do sentence t</context>
C04-1134==>P98-1013!=<context citStr="Baker et al., 1998" endWordPosition="390" position="2738" startWordPosition="387">se, simulating the concept lexicon, suggested by cognitive scientists, of a bilingual person. Figure 1. BiFrameNet lexicon and example sentence induction The linguists-defined ontologies-FrameNet (Baker et al., 1998), HowNet (Dong and Dong, 2000), and bilingual dictionaries are the basis for the induction of the mapping. We automatically estimate the semantic transfer likelihoods between English FrameNet lexical </context>
C04-1134==>P98-1013!=<context citStr="Baker et al., 1998" endWordPosition="1030" position="6188" startWordPosition="1027">2.1. FrameNet and HowNet The Berkeley FrameNet database consists of frame-semantic descriptions of more than 7000 English lexical items, together with example sentences annotated with semantic roles (Baker et al., 1998). There is currently no frame semantic representation of Chinese. However, the Chinese HowNet (Dong and Dong 2000) represents a hierarchical view of lexical semantics in Chinese. FrameNet is a collect</context>
A00-1014==>P88-1015!=<context citStr="Whittaker and Stenton, 1988" endWordPosition="3154" position="21054" startWordPosition="3151">and 2) providing answers to well-formed queries (steps 9-11). 3.2.3 Strategy Selection Previous work has argued that initiative affects the degree of control an agent has in the dialogue interaction (Whittaker and Stenton, 1988; Walker and Whittaker, 1990; Chu-Carroll and Brown, 1998). Thus, a cooperative system may adopt different strategies to achieve the same goal depending on the initiative distribution. Since task init</context>
W96-0304==>J93-1007!=<context citStr="Smadja (1993)" endWordPosition="838" position="5507" startWordPosition="837">do not appear show typical features of sublanguage in that there are no domain-specific structures, and the vocabulary is not as restricted. Therefore, sublanguage techniques such as Sager (1981) and Smadja (1993) do not work. In situations where lexico-syntactic patterning is deficient, a lexicon with specified metonymic relations can be developed to yield accurate scoring of response content. We define meton</context>
E95-1035==>J91-1002!=<context citStr="Morris &amp; Hirst (1991)" endWordPosition="2572" position="15242" startWordPosition="2569">ntic link between the second and third sentences we prefer to connect the third sentence to the thread begun by the second.' The approach to representing semantic relationships we take is one used by Morris &amp; Hirst (1991) wherein the words in the lexicon are associated with each other in a thesauruslike fashion and given a rating according to how semantically &amp;quot;close&amp;quot; they are. We thus avoid relying on high-level infer</context>
J95-3001==>J88-3006!=<context citStr="Paris (1988)" endWordPosition="690" position="4500" startWordPosition="689">edge, termed a user model. This concept is described, for example, in Kobsa and Wahlster (1988, 1989), Chin (1989), Cohen and Jones (1989), Finin (1989), Lehman and Carbonell (1989), Monk (1989), and Paris (1988). The user model specifies information needed for efficient interaction with the conversational partner. Its purpose is to indicate what needs to be said to the user to enable the user to function eff</context>
J95-3001==>J88-3006!=<context citStr="Paris 1988" endWordPosition="16299" position="102137" startWordPosition="16298"> semantic model is inadequate, and our design attempts to avoid such inadequacies. A number of important contributions have been made in the area of user modeling (Kobsa and Wahlster 1988; Rich 1988; Paris 1988; McCoy 1988). One of the most comprehensive is the &amp;quot;General User Modeling Shell&amp;quot; (GUMS), described by Finin (1989). He lists five important features for user models: (1) Separate knowledge base. The </context>
P04-1085==>W04-2319!=<context citStr="Shriberg et al., 2004" endWordPosition="620" position="4102" startWordPosition="617">er and offer-acceptance, and their labeling is used in our work to determine who are the addressees in agreements and disagreements. The annotation of the corpus with adjacency pairs is described in (Shriberg et al., 2004; Dhillon et al., 2004). Seven of those meetings were segmented into spurts, defined as periods of speech that have no pauses greater than .5 second, and each spurt was labeled with one of the four ca</context>
P99-1009==>A97-1051!=<context citStr="Day et al., 1997" endWordPosition="183" position="1170" startWordPosition="180">ing tasks, including part-of-speech tagging (Brill, 1995; Ramshaw and Marcus, 1994), spelling correction (Mangu and Brill, 1997), word-sense disambiguation (Gale et al., 1992), message understanding (Day et al., 1997), discourse tagging (Samuel et al., 1998), accent restoration (Yarowsky, 1994), prepositional-phrase attachment (Brill and Resnik, 1994) and base noun phrase identification (Ramshaw and Marcus, In Pre</context>
H05-2004==>A00-2018!=<context citStr="Charniak, 2000" endWordPosition="567" position="3606" startWordPosition="566">ystem begins preprocessing raw text by using sentence segmentation tools (available at http://l2r.cs.uiuc.edu/cogcomp/tools.php). Next, sentences are analyzed by a state-of-the-art syntactic parser (Charniak, 2000) the output of which provides useful information for the main SRL module. The main SRL module consists of four stages: pruning, argument identification, argument classification, and inference. The fol</context>
J93-2004==>H91-1037!=<context citStr="Weischedel et al. 1991" endWordPosition="7167" position="45564" startWordPosition="7164">dy in print: a number of projects investigating stochastic parsing have used either the POS-tagged materials (Magerman and Marcus 1990; Brill et al. 1990; Brill 1991) or the skeletally parsed corpus (Weischedel et al. 1991; Pereira and Schabes 1992). The POS-tagged corpus has also been used to train a number of different POS taggers including Meteer, Schwartz, and Weischedel (1991), and the skeletally parsed corpus has</context>
J99-2004==>J94-4005!=<context citStr="Alshawi and Carter (1994)" endWordPosition="1357" position="9023" startWordPosition="1354">bank that is used to train the system. There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995). In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the </context>
P06-2042==>C04-1159!=<context citStr="Shitaoka et al., 2004" endWordPosition="556" position="3770" startWordPosition="552">ersions. In this study, we address the problem of ambiguous clause boundaries in dependency structure analysis in spontaneous speech. We treated the other problems in the same way as Shitaoka et al. (Shitaoka et al., 2004). For example, inversions are represented as dependency relationships going in the direction from right to left in the CSJ, and their direction was changed to that from left to right in our experiment</context>
P06-2042==>C04-1159!=<context citStr="Shitaoka et al., 2004" endWordPosition="2260" position="13961" startWordPosition="2257">clauses. For training, we used 168 talks excluding the test data to conduct the open test and all the 188 talks to conduct the closed test. First, we detected sentence boundaries by using the method (Shitaoka et al., 2004) and analyzed the dependency structure of each sentence by the method described in Section 3.1 without using information on quotations and inserted clauses. We obtained an F-measure of 85.9 for the se</context>
W04-2708==>J03-1002!=<context citStr="Och and Ney, 2003" endWordPosition="3306" position="21046" startWordPosition="3303">lity of the source dictionary, the frequencies of the translations in Czech and English monolingual corpora, and the correspondence of the Czech and English POS tags. Furthermore, by training GIZA++ (Och and Ney, 2003) translation model on the training part of the PCEDT extended by the manual dictionaries, we obtained a probabilistic Czech-English dictionary, more sensitive to the domain of financial news specific </context>
W00-1410==>J98-3004!=<context citStr="Mittal et al., 1998" endWordPosition="425" position="2697" startWordPosition="422">ient clear information about the system in the available literature, and/or by means of personal contact with the developers. The system chosen was the Caption Generation System (Mittal et al., 1995; Mittal et al., 1998)3. This system was chosen because, as well as fulfilling the criteria above, it appeared to be a relatively simple pipeline, thus avoiding complex control issues, with individual modules performing th</context>
A00-2013==>W96-0213!=<context citStr="Ratnaparkhi, 1996" endWordPosition="186" position="1286" startWordPosition="185">thods: neural networks (Julian Benello and Anderson, 1989), HMM tagging (Merialdo, 1992), decision trees (Schmid, 1994), transformation-based error-driven learning (Brill, 1995), and maximum entropy (Ratnaparkhi, 1996), to select just a few. However different the methods were, English dominated in these tests. Unfortunately, English is a morphologically &amp;quot;impoverished&amp;quot; language: there are no complicated agreement re</context>
A00-2013==>W96-0213!=<context citStr="Ratnaparkhi, 1996" endWordPosition="2071" position="12940" startWordPosition="2069">dling unknown words. First, we use only information which may be obtained automatically from the manually annotated corpus (we call this method automatic). This is the way the Maximum Entropy tagger (Ratnaparkhi, 1996) runs if one uses the binary version from the website (see the comparison in Section 5). However, it is not unreasonable to assume that a larger independent dictionary exists which can help to obtain </context>
A00-2013==>W96-0213!=<context citStr="Ratnaparkhi, 1996" endWordPosition="2833" position="17584" startWordPosition="2832"> several taggers (HMM, Brill's Transformationbased Tagger, Ratnaparkhi's Maximum Entropy tagger, and the Daelemans et al.'s Memory-based Tagger) on Slovene. We have chosen the Maximum Entropy tagger (Ratnaparkhi, 1996) for a comparison with our universal tagger, since it achieved (by a small margin) the best overall result on Slovene as reported there (86.360% on all tokens) of taggers available to us (MBT, the bes</context>
H89-2008==>H89-2022!=<context citStr="[7]" endWordPosition="1268" position="8205" startWordPosition="1268">erpretation of the meaning of the sentence to the back end. Our natural language system, TINA, was specifically designed to meet these two needs. The basic design of TINA has been described elsewhere [7], and therefore will only be briefly mentioned here. Instead, we would like to focus on the issue of how to incorporate semantics into the parses. We have found that an enrichment of the parse tree wi</context>
H89-2008==>H89-2022!=<context citStr="[7]" endWordPosition="1879" position="12178" startWordPosition="1879">n fact, we decided to take the approach of only using these two parameters for semantic filtering, to see whether in fact that would be adequate. Their use in the gap mechanism is described elsewhere [7], but for clarification we will briefly review it here. Generators are nodes that enter their subparse into the Current-Focus slot. Activators move the Current-Focus into the Float-Object position, fo</context>
J05-3004==>P03-1023!=<context citStr="Yang et al. 2003" endWordPosition="1691" position="11264" startWordPosition="1688">rformance of these approaches on definite NPs is often substantially worse than on pronouns and/or named entities (Connolly, Burger, and Day 1997; Strube, Rapp, and Mueller 2002; Ng and Cardie 2002b; Yang et al. 2003). For example, for a coreference resolution algorithm on German texts, Strube, Rapp, and Mueller (2002) report an F-measure of 33.9% for definite NPs that contrasts with 82.8% for personal pronouns. S</context>
N06-2019==>H05-1030!=<context citStr="Johnson et al., 2004" endWordPosition="640" position="4300" startWordPosition="637">eaching speech data to more closely resemble text has been shown to improve accuracy with some text-based processing tasks (Rosenfeld et al., 1995). For our study, a state-of-the-art filler detector (Johnson et al., 2004) is employed to delete fillers prior to parsing. Results show parse accuracy improves significantly, suggesting disfluency filtering may have a broad role in enabling text-based processing of speech d</context>
N06-2019==>H05-1030!=<context citStr="Johnson et al., 2004" endWordPosition="1845" position="11952" startWordPosition="1842">contrast standard parsing with deleting disfluencies early (via oracle knowledge). Given our particular interest in fillers, we also report the effect of detecting them via a state-of-the-art system (Johnson et al., 2004). Results appear in Table 2. It is worth noting that since our text-trained parser never produces FILLER or EDITED constituents, the bracket-based metric penalizes it for each such constituent appeari</context>
N06-2019==>H05-1030!=<context citStr="Johnson et al., 2004" endWordPosition="1936" position="12551" startWordPosition="1933"> dependencies for these termi75 Table 2: F-scores parsing Switchboard when trained on WSJ. Edit word detection varies between parser and oracle, and filler word detection varies between none, system (Johnson et al., 2004), and oracle. Filler F, LB, and Dep are defined as in Table 1. Edits Fillers Filler F LB Dep oracle oracle 100.0 83.6 81.4 oracle detect 89.3 81.6 80.5 oracle none - 71.8 75.4 none oracle 100.0 76.3 7</context>
W03-1805==>P96-1041!=<context citStr="Chen and Goodman, 1996" endWordPosition="1744" position="10272" startWordPosition="1741">decomposed as Assuming only depends on the previous words, N-gram language models are commonly where ,, and Here each word only depends on the previous two words. Please refer to (Jelinek, 1990) and (Chen and Goodman, 1996) for more about N-gram models and associated smoothing methods. Now suppose we have a foreground corpus and a background corpus and have created a language model for each corpus. The simplest language</context>
W03-1805==>P96-1041!=<context citStr="Chen and Goodman, 1996" endWordPosition="2629" position="15551" startWordPosition="2626">gure 4: Procedure to find key-bigrams For this experiment we used unsmoothed count for calculating phraseness , where , and used the unigram model for calculating informativeness with Katz smoothing (Chen and Goodman, 1996)4 to handle zero occurrences. Figure 5 shows the extracted key-bigrams using this method. Comparing to Figure 2, you can see that those two methods extract almost identical ranked phrases. Note that w</context>
W06-1637==>P04-1014!=<context citStr="Clark and Curran, 2004" endWordPosition="1613" position="10577" startWordPosition="1610">particular interest to psycholinguistics, since it allows the construction of incremental derivations. CCG has also enjoyed the interest of the NLP community, with high-accuracy wide-coverage parsers(Clark and Curran, 2004; Hockenmaier and Steedman, 2002) and generators1 available (White and Baldridge, 2003). Words are associated with lexical categories which specify their subcategorization behaviour, eg. ((S[dcl]\NP)/</context>
C04-1095==>J94-1002!=<context citStr="Ostendorf and Veilleux, 1994" endWordPosition="332" position="2157" startWordPosition="328">ost of the literature on prosodic phrasing agrees that, next to syntactic features, the length of the prosodic phrases plays an important role (Nespor and Vogel, 1986; Bachenko and Fitzpatrick, 1990; Ostendorf and Veilleux, 1994). Prosodic phrases tend to be balanced, such that very short and very long phrases are less likely than phrases of intermediate length. The probability of a break therefore depends to some extent on t</context>
C04-1197==>W04-2416!=<context citStr="Hacioglu et al., 2004" endWordPosition="4328" position="25848" startWordPosition="4325">not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. For a fair comparison, our system was among the best at CoNLL-04, where the best system (Hacioglu et al., 2004) achieve a 69.49 F1 score. 6 Conclusion We show that linguistic information is useful for semantic role labeling, both in extracting features and Dist. Prec. Rec. F=1 Overall 100.00 70.07 63.07 66.39</context>
W02-1504==>W02-1510!=<context citStr="Suzuki, 2002" endWordPosition="970" position="6372" startWordPosition="969">ents are tested using monolingual regression files containing thousands of analyzed sentences; differences caused by the modification are examined manually by the linguist responsible for the change (Suzuki, 2002). This process serves as an initial screening to ensure that modifications to the analysis have the desired effect. 3 MSR-MT In this section we review the basics of the MSRMT translation system and it</context>
W02-1504==>W02-1510!=<context citStr="Suzuki (2002)" endWordPosition="1054" position="6878" startWordPosition="1053">r further details on the French and Spanish versions of the system. The overall architecture and basic component structure 2 LF as described here corresponds to the PAS representation of Campbell and Suzuki (2002). are the same for both the FE and SE versions of the system. 3.1 Overview MSR-MT uses the broad coverage analysis system described in Section 2, a large multi-purpose source-language dictionary, a le</context>
W02-0907==>A97-1052!=<context citStr="Briscoe and Carroll, 1997" endWordPosition="38" position="277" startWordPosition="35">e Workshop of the ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia, July 2002, pp. 51-58. Association for Computational Linguistics. with low frequency scFs. According to one account (Briscoe and Carroll, 1997) the majority of errors arise in SCF acquisition because of the statistical filtering process. Korhonen et al. (2000) investigated reasons for the poor performance of the statistical filters and repor</context>
W02-0907==>A97-1052!=<context citStr="Briscoe and Carroll (1997)" endWordPosition="318" position="2116" startWordPosition="315">imental evaluation and section 5 concludes with directions for future work. 2 Framework for SCF Acquisition 2.1 Hypothesis Generation We employ for hypothesis generation the SCF acquisition system of Briscoe and Carroll (1997). This system is capable of distinguishing 163 verbal scFs a superset of those found in the ANLT (Boguraev et al., 1987) and COMLEX Syntax dictionaries (Grishman et al., 1994) and returning relative f</context>
W02-0817==>W97-0201!=<context citStr="Ng, 1997" endWordPosition="817" position="5070" startWordPosition="816">es about 300 word types with 300-1000 tagged instances for each word, selected from a 17 million word corpus. Sense tagged corpora have thus been central to accurate WSD systems. Estimations made in (Ng, 1997) indicated that a high accuracy domain independent system for WSD would probably need a corpus of about 3.2 million sense tagged words. At a throughput of one word per minute (Edmonds, 2000), this wou</context>
P98-1068==>P97-1032!=<context citStr="Samuelsson and Voutilainen (1997)" endWordPosition="155" position="1225" startWordPosition="152">s is very important. Currently, there are two main methods for automatic part-of-speech tagging, namely, corpusbased and rule-based methods. The corpus-based method is popular for European languages. Samuelsson and Voutilainen (1997), however, show significantly higher achievement of a rulebased tagger than that of statistical taggers for English text. On the other hand, most Japanese taggers' are rule-based. In previous Japanese</context>
N04-3004==>M95-1012!=<context citStr="Aberdeen et al. 1995" endWordPosition="893" position="6186" startWordPosition="889">normalized, they are passed through a zoner using human-generated rules to identify source, date, and other information such as headline, or title, and content. The Alembic natural language analyzer (Aberdeen et al. 1995; Vilain and Day 1996) processes the zoned messages to identify paragraph, sentence, and word boundaries as well as part-of-speech tags. The messages then pass through the Alembic named entity recogni</context>
C00-2118==>E99-1007!=<context citStr="Stevenson and Merlo, 1999" endWordPosition="226" position="1572" startWordPosition="223">ers have investigated statistical corpusbased methods for lexical semantic classification from syntactic properties of verb usage (Aone and McKee, 1996; Lapata and Brew, 1999; Schulte im Walde, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; McCarthy, 2000). Corpus-based approaches to lexical semantic classification in particular have drawn on Levin's hypothesis (Levin, 1993) that verbs can be classified accordin</context>
P00-1017==>W99-0623!=<context citStr="Henderson and Brill, 1999" endWordPosition="2918" position="17317" startWordPosition="2915">combining the annotations from these different systems in order to increase the performance on all the GR types affected by those different existing systems. Various works (van Halteren et al., 1998; Henderson and Brill, 1999; Wilkes and Stevenson, 1998) on combining different systems exist. These works use one or both of two types of schemes. One is to have the different systems simply vote. However, this does not really</context>
N03-1003==>P93-1024!=<context citStr="Pereira et al., 1993" endWordPosition="1020" position="6719" startWordPosition="1017">d performance. 2 Related work Previous work on automated paraphrasing has considered different levels of paraphrase granularity. Learning synonyms via distributional similarity has been well-studied (Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Jacquemin (1999) and Barzilay and McKeown (2001) identify phraselevel paraphrases, while Lin and Pantel (2001) and Shinyama et al. (2002) acquire structural paraphras</context>
W06-1908==>C02-1042!=<context citStr="Hovy et al., 2002" endWordPosition="262" position="1733" startWordPosition="259">lected by the TREC (Text Retrieval Conference) QA track (Voorhees, 2004). Several QA systems have responded to these changes in the nature of the QA task by incorporating various knowledge resources (Hovy et al., 2002), handling of additional types of questions tapping into external data sources such as web, encyclopedia, and databases in order to find the answer candidates, which may then be located in the specifi</context>
W04-3213==>W03-1007!=<context citStr="Fleischman et al., 2003" endWordPosition="221" position="1412" startWordPosition="218">e of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., </context>
W04-3213==>W03-1007!=<context citStr="Fleischman et al., 2003" endWordPosition="831" position="5115" startWordPosition="828"> Role Sets Previous work has divided the semantic role labelling task into the identification of the arguments to be labelled, and the tagging of each argument with a role (Gildea and Jurafsky, 2002; Fleischman et al., 2003). Our algorithm addresses both these steps. Also, the unsupervised nature of the approach highlights an intermediate step of determining the set of possible roles for each argument. Because we need to</context>
J84-3001==>P83-1014!=<context citStr="Church 1983" endWordPosition="7603" position="46799" startWordPosition="7602"> that the cat that the dog chase ate died&amp;quot; (Langendoen 1975, Church 1981, Langendoen and Langsam 1984). They have also provided useful models in morphology (Kay 1983, Koskenniemi 1983) and phonology (Church 1983), 172 Computational Linguistics, Volume 10, Numbers 3-4, July-13cember 1984 C. Raymond Perrault On the Mathematical Properties of Linguistic Theories on the basis of sentences containing such that. Po</context>
P99-1032==>J99-2002!=<context citStr="Bruce and Wiebe, 1999" endWordPosition="3364" position="20094" startWordPosition="3361">ts of machine learning experiments, in which we develop probablistic classifiers to automatically perform the subjective and objective classification. In the method we use for developing classifiers (Bruce and Wiebe, 1999), a search is performed to find a probability model that captures important interdependencies among features. Because features can be dropped and added during search, the method also performs feature </context>
W03-1505==>E03-2009!=<context citStr="Maynard and Cunningham, 2003" endWordPosition="613" position="3835" startWordPosition="609">de-compliant). Following previous efforts to adapt the ANNIE information extraction system (the default IE system that comes with the GATE architecture) to different languages and applications (e.g. (Maynard and Cunningham, 2003; Maynard et al., 2002), we decided to investigate whether we could adapt ANNIE to an unknown language within a very limited period of time. There are two particularly important points to note about o</context>
C02-1050==>J93-2003!=<context citStr="Brown et al., 1993" endWordPosition="216" position="1539" startWordPosition="213">odel paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993). Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a s</context>
C02-1050==>J93-2003!=<context citStr="Brown et al. (1993)" endWordPosition="1070" position="6678" startWordPosition="1067">tive to the previous target word generated from the cept (j').  NULL Translation Model  p1 : A fixed probability of inserting a NULL word after determining each target word f. For details, refer to Brown et al. (1993). 2.2 Search Problem The search problem of statistical machine translation is to induce the maximum likely channel source sequence, e, given f and the model, P(f |e) = Ea P(f, a|e) and P(e). For the s</context>
P06-1006==>H05-1083!=<context citStr="Luo and Zitouni, 2005" endWordPosition="326" position="2167" startWordPosition="323"> used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005). However, such a solution has its limitation. The syntactic features have to be selected and defined manually, usually by linguistic intuition. Unfortunately, what kinds of syntactic information are </context>
P06-1006==>H05-1083!=<context citStr="Luo and Zitouni (2005)" endWordPosition="1149" position="7422" startWordPosition="1146">whether the candidate has an identical collocation pattern with the pronominal anaphor. Table 1: Feature set for the baseline pronoun resolution system salience measures have to be assigned manually. Luo and Zitouni (2005) proposed a coreference resolution approach which also explores the information from the syntactic parse trees. Different from Lappin and Leass (1994)s algorithm, they employed a maximum entropy base</context>
P06-1006==>H05-1083!=<context citStr="Luo and Zitouni (2005)" endWordPosition="4378" position="26992" startWordPosition="4375"> NORM+ Charniak00 79.2 82.7 82.3 S Simple Collins99 80.8 81.5 82.3 Table 6: Results using different parsers important for pronoun resolution. We also tested the flat syntactic feature set proposed in Luo and Zitouni (2005)s work. As described in Section 2, the feature set is inspired the binding theory, including those features like whether the candidate is c commanding the pronoun, and the counts of NP, VP, S n</context>
E06-1050==>C02-1150!=<context citStr="Li and Roth, 2002" endWordPosition="273" position="1687" startWordPosition="270">ngdom. The goal of answer typing is to determine whether a words semantic type is appropriate as an answer for a question. Many previous approaches to answer typing, e.g., (Ittycheriah et al., 2001; Li and Roth, 2002; Krishnan et al., 2005), employ a predefined set of answer types and use supervised learning or manually constructed rules to classify a question according to expected answer type. A disadvantage of </context>
E06-1050==>C02-1150!=<context citStr="Li &amp; Roth (2002)" endWordPosition="1025" position="6124" startWordPosition="1022">pervised learning. Ittycheriah, et al. (2001) describe a maximum entropy based question classification scheme to classify each question as having one of the MUC answer types. In a similar experiment, Li &amp; Roth (2002) train a question classifier based on a modified version of SNoW using a richer set of answer types than Ittycheriah et al. The LCC system (Harabagiu et al., 2003) combines fixed types with a novel lo</context>
W05-1504==>J98-2004!=<context citStr="Collins (1997)" endWordPosition="708" position="4395" startWordPosition="707">hension), including the phonological, morphological, syntactic, or referential (given/new) complexity of the intervening material (Gibson, 1998). In parsing, the most relevant previous work is due to Collins (1997), who considered three binary features of the intervening material: did it contain (a) any word tokens at all, (b) any verbs, (c) any commas or colons? Note that (b) is effective because it measures t</context>
W05-1504==>J98-2004!=<context citStr="Collins (1997)" endWordPosition="7316" position="41769" startWordPosition="7315">ectory of this search is plotted in Fig. 4.26 The graph shows that type-specific bounds can speed up the parser to a given level with less loss in accuracy. 5 Related Work As discussed in footnote 3, Collins (1997) and McDonald et al. (2005) considered the POS tags intervening between a head and child. These soft constraints were very helpful, perhaps in part because they helped capture the short dependency pre</context>
P94-1016==>H91-1036!=<context citStr="Moore and Dowding, 1991" endWordPosition="858" position="5631" startWordPosition="855"> gaps not licensed by any possible gap filler. In a pure bottom-up parser, syntactic gaps must be hypothesized between every pair of words and lead to many spurious phrases being built. Earlier work (Moore and Dowding, 1991) showed that over 80% of the edges built by a bottom-up parser using our grammar were in this class. Since these phrases are semantically incomplete, they are of no interest if they cannot be tied to </context>
P94-1016==>H91-1036!=<context citStr="Moore and Dowding, 1991" endWordPosition="2131" position="13446" startWordPosition="2128">dynamically at run time. Time is saved both because the predictions do not have to be generated at run time, and because the process of checking these static predictions is simpler. In previous work (Moore and Dowding, 1991), we compared limited left-context checking to some other methods for dealing with empty categories in a bottom-up parser. Standard grammar transformation techniques (Hoperoft and Ullman, 1980) can be</context>
P05-2008==>W02-1011!=<context citStr="Pang et al. (2002)" endWordPosition="283" position="1834" startWordPosition="280">es can be quite effective when applied to the sentiment analysis problem. Models such as Naive Bayes (NB), Maximum Entropy (ME) and Support Vector Machines (SVM) can determine the sentiment of texts. Pang et al. (2002) used a bagof-features framework (based on unigrams and bigrams) to train these models from a corpus of movie reviews labelled as positive or negative. The best accuracy achieved was 82.9%, using an S</context>
P05-2008==>W02-1011!=<context citStr="Pang et al. (2002)" endWordPosition="726" position="4578" startWordPosition="723">Mlight implementation of a Support Vector Machine classifier. The models were trained using unigram features, accounting for the presence of feature types in a document, rather than the frequency, as Pang et al. (2002) found that this is the most effective strategy for sentiment classification. When training and testing on the same set, the mean accuracy is determined using three-fold crossvalidation. In each case,</context>
P05-2008==>W02-1011!=<context citStr="Pang et al. (2002)" endWordPosition="1094" position="6850" startWordPosition="1091">erval 95%). 2.3 Domain Dependency We conducted an experiment to compare the accuracy when training a classifier on one domain (newswire articles or movie reviews from the Polarity 1.0 dataset used by Pang et al. (2002)) and testing on the other domain. In Figure 2, we see a clear indication that models trained on one domain do not perform as well on another domain. All differences are significant at a confidence in</context>
P05-2008==>W02-1011!=<context citStr="Pang et al. (2002)" endWordPosition="1252" position="7800" startWordPosition="1249"> sticking out surprise disappointed crying confused angry embarrassed Frequency 3.8739 2.4350 0.4961 0.1838 0.1357 0.0171 0.0146 0.0093 0.0075 0.0038 0.0007 views, following the same approach used by Pang et al. (2002) when they created the Polarity 1.0 dataset. The data source was the Internet Movie Review Database archive1 of movie reviews. The reviews were categorised as positive or negative using automatically </context>
W97-1105==>J94-3001!=<context citStr="Kaplan &amp; Kay, 1994" endWordPosition="507" position="3277" startWordPosition="504">ation relevant for phonology (Reinhard &amp; Gibbon, 1991). And there is a body of work on the use of finite state devices  closely related to regular expressions  for modelling phonological phenomena (Kaplan &amp; Kay, 1994) and for speech processing (cf. Kornai's 1Unlike regular database management systems, these include international and phonetic character sets and user-defined keystrokes for entering them, and a utili</context>
J04-4002==>C00-2123!=<context citStr="Tillmann and Ney 2000" endWordPosition="7132" position="42932" startWordPosition="7129">words for each target word position in the alignment template instantiation. Only these words are then hypothesized in the search. We call this selection of highly probable words observation pruning (Tillmann and Ney 2000). As a criterion for a word e at position i in the alignment template instantiation, we use  A(i,j)  p(e |fj) (28) A(i , j) (Ei,C(e))   J, j=0 Ei, In our experiments, we hypothesize only the five </context>
P04-1069==>J98-2005!=<context citStr="Chi and Geman, 1998" endWordPosition="1064" position="6425" startWordPosition="1061">urthermore, commonly used training algorithms for PCFGS/PPDAs always produce proper probability assignments, and many desired mathematical properties of these methods are based on such an assumption (Chi and Geman, 1998; S anchez and Benedi, 1997). We may therefore discard non-proper probability assignments in the current study. However, such probability assignments are outside the reach of the usual training algori</context>
P04-1069==>J98-2005!=<context citStr="Chi and Geman, 1998" endWordPosition="3169" position="17349" startWordPosition="3166">ation such that pg() = n n for each rule  = A  . For all nonterminals A, E=A pg() = E=A n n = n n = 1, which means that the PCFG (9,pg) is proper. Furthermore, it has been shown in (Chi and Geman, 1998; S anchez and Benedi, 1997) that a PCFG (9,pg) is consistent if pg was obtained by maximum-likelihood estimation using a set of derivations. Finally, since n &gt; 0 for each , also pg() &gt; 0 for each </context>
W06-1704==>J03-3005!=<context citStr="Keller and Lapata, 2003" endWordPosition="304" position="2006" startWordPosition="301">relatively minor language. There are currently about 10.8 million Catalan speakers, similar to Serbian (12), Greek (10.2), or Swedish (9.3). See http://www.upc.es/slt/alatac/cat/dades/catala-04.html (Keller and Lapata, 2003) using available search engines. This approach has a number of drawbacks, e.g. the data one looks for has to be known beforehand, and the queries have to consist of lexical material. In other words, i</context>
W06-1613==>W06-1312!=<context citStr="Teufel et al., 2006" endWordPosition="1101" position="7076" startWordPosition="1098">cheme for citations is based on empirical work in content citation analysis. It is designed for information retrieval applications such as improved citation indexing and better bibliometric measures (Teufel et al., 2006). Its 12 categories mark relationships with other works. Each citation is labelled with exactly one category. The following top-level four-way distinction applies:  Explicit statement of weakness  C</context>
P93-1019==>C92-1015!=<context citStr="Bird 1992" endWordPosition="3737" position="22627" startWordPosition="3736">ed from the more general adjectival rule, but pursuing this here would take us somewhat afield. 145 with feature-based filters was an important impetus. Second, researchers at Edinburgh (Calder 1988, Bird 1992) first suggested using FDLs in phonological and morphological description, and Bird 1992 suggests describing FA in FDL (without showing how they might be so characterized, howeverin particular, provi</context>
P93-1019==>C92-1015!=<context citStr="Bird 1992" endWordPosition="4100" position="24883" startWordPosition="4099">alism, and in which strides toward descriptions of morphotactics (Krieger 1993a, Riehemann 1993, &amp;quot;Cf. Reinhard and Gibbon 1991 for another sort of DATR-based allomorphy Gerdemann 1993) and phonology (Bird 1992) have been taken. This work is the first to show how allomorphy may be described here. The proposal here would allow one to describe segments using features, as well, but we have not explored this opp</context>
W96-0110==>P91-1017!=<context citStr="[4,5,6,7]" endWordPosition="400" position="2834" startWordPosition="400">and tuned from a training corpus. In such a way, the system can be easily scaled up and well trained based on the well-established theories. However, statistical approaches reported in the literature [4,5,6,7] usually use only surface level information, e.g., collocations and word associations, without taking structure information, such as syntax and thematic role, into consideration. In general, the struc</context>
W01-0712==>A88-1019!=<context citStr="Church, 1988" endWordPosition="642" position="4174" startWordPosition="641"> A specialised version of the chunking task is NP CHUNKING or baseNP identification in which the goal is to identify the base noun phrases. The first work on this topic was done back in the eighties (Church, 1988). The data set that has become standard for evaluation machine learning approaches is the one first used by Ramshaw and Marcus (1995). It consists of the same training and test data segments of the Pe</context>
P01-1022==>P96-1030!=<context citStr="Rayner and Carter 1996" endWordPosition="622" position="3941" startWordPosition="619">hich the language accepted by the approximation is a superset of language accepted by the original grammar. While we conceed that alternative techniques that are not sound (Black 1989, (Johnson 1998, Rayner and Carter 1996) may still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-b</context>
H05-1119==>P05-1055!=<context citStr="Chelba and Acero, 2005" endWordPosition="618" position="4051" startWordPosition="615">e, 2004) reports a substantial 50% improvement of FOM (Figure Of Merit) for a word-spotting task in voicemails. Improvements from using lattices were also reported by (Saraclar and Sproat, 2004) and (Chelba and Acero, 2005). To address the problem of domain independence, a subword-based approach is needed. In (Logan, 2002) the authors address the problem by indexing phonetic or word-fragment based transcriptions. Simila</context>
H05-1119==>P05-1055!=<context citStr="Chelba and Acero, 2005" endWordPosition="1210" position="7926" startWordPosition="1207">s follows. First, to increase recall we search recognition alternates based on lattices. Lattice oracle word-error rates1 are significantly lower than word-error rates of the best path. For example, (Chelba and Acero, 2005) reports a lattice oracle error rate of 22% for lecture recordings at a top-1 word-error rate of 45%2. To utilize recognizer scores in the lattices, we formulating the ranking problem as one of risk m</context>
H05-1119==>P05-1055!=<context citStr="Chelba and Acero, 2005" endWordPosition="5006" position="30893" startWordPosition="5003">osteriors (Eq. 4). However, posterior representations of lattices found in literature only include word (arc) posteriors, and some posterior-based systems simply ignore the node-posterior term, e.g. (Chelba and Acero, 2005). In Table 4, we evaluate the impact on accuracy when this term is ignored. (In this experiment, we bypassed the index-lookup step, thus the numbers are slightly different from Table 3.) We found that</context>
P06-1038==>P99-1016!=<context citStr="Caraballo (1999)" endWordPosition="912" position="5644" startWordPosition="910">tering in that space (Curran and Moens, 2002). Pereira (1993) and Lin (1998) use syntactic features in the vector definition. (Pantel and Lin, 2002) improves on the latter by clustering by committee. Caraballo (1999) uses conjunction and appositive annotations in the vector representation. 2We did not compare against methods that use richer syntactic information, both because they are supervised and because they </context>
N06-1013==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="154" position="1040" startWordPosition="151">ection of corresponding words between two sentences that are translations of each otheris usually an intermediate step of statistical machine translation (MT) (Brown et al., 1993; Och and Ney, 2003; Koehn et al., 2003), but also has been shown useful for other applications such as construction of bilingual lexicons, word-sense disambiguation, projection of resources, and crosslanguage information retrieval. Maximum</context>
N06-1013==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="1990" position="11575" startWordPosition="1987"> each system: (1) Intersection of both directions (Aligner(int)); (2) Union of both directions (Aligner(union)); and (3) The previously bestknown heuristic combination approach called growdiag-final (Koehn et al., 2003) (Aligner(gdf)). In our evaluation, we take A to be the set of alignment links for a set of sentences, S to be the set of sure alignment links, and P be the set of probable alignment links (in the gol</context>
N06-1013==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="4040" position="24420" startWordPosition="4037">he Xinhua portion of the Gigaword corpus. During decoding, the number of English phrases per FL phrase was limited to 100 and the distortion of phrases was limited by 4. Based on the observations in (Koehn et al., 2003), we also limited the phrase length to 3 for computational reasons. Alignment Chinese Arabic GIZA++(union) 22.66 41.72 GIZA++(gdf) 23.79 43.82 GIZA++(int) 23.97 42.76 ACME[2] 25.20 44.94 ACME[4] 25.59</context>
E06-3004==>W02-2024!=<context citStr="Sang, 2002" endWordPosition="206" position="1449" startWordPosition="205"> state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1. (Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate. </context>
E06-3004==>W02-2024!=<context citStr="Sang, 2002" endWordPosition="614" position="4016" startWordPosition="613">. In this scheme, tag B denotes the start of an entity, tag I continues the entity and tag O marks words that do not form part of an entity. This scheme was initially introduced in CoNLLs (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. Once all entities in the text are detected, they are passed for classification in a</context>
W06-0110==>P02-1062!=<context citStr="Collins, 2002" endWordPosition="280" position="1867" startWordPosition="279">l (Zhou and Su, 2000; Miller and Crystal, 1998), maximum entropy model (Borthwick, 1999), decision tree (Qin and Yuan, 2004), transformation-based learning (Black and Vasilakopoulos, 2002), boosting (Collins, 2002; Carreras et al., 2002), support vector machine (Takeuchi and Collier, 2002; Yu et al., 2004; Goh et al., 2003), memory-based learning (Sang, 2002). SVM has given high performance in various classifi</context>
W06-3312==>C94-2195!=<context citStr="Brill &amp; Resnik (1994)" endWordPosition="789" position="5091" startWordPosition="786">tachment points, calculated from query results of a webbased search engine. This system was evaluated on sentences from a weekly computer magazine, scoring 74% accuracy for both VP and NP attachment. Brill &amp; Resnik (1994) put transformationbased learning with added word-class information from WordNet to the task of PP attachment. Their system achieves 81.8% accuracy on sentences from the Penn Treebank Wall Street Jour</context>
H91-1067==>A88-1019!=<context citStr="[4]" endWordPosition="1695" position="10613" startWordPosition="1695"> fact of the matter  a natural clustering, but no systematic characterization of it is available, so an eyeball estimate must be used instead. infinitives, Church uses his statistical disambiguator ([4]) to distinguish between to as an infinitive marker and to as a preposition. Then he measures the mutual information between occurrences of the verb and occurrences of infinitives following within a c</context>
P95-1036==>C94-2149!=<context citStr="Doran et al., 1994" endWordPosition="1235" position="7460" startWordPosition="1232">ral means for generalization during the EBL process. 3 Overview of our approach to using EBL We are pursuing the EBL approach in the context of a wide-coverage grammar development system called XTAG (Doran et al., 1994). The XTAG system consists of a morphological analyzer, a part-ofspeech tagger, a wide-coverage LTAG English grammar, a predictive left-to-right Early-style parser for LTAG (Schabes, 1990) and an X-wi</context>
W04-2411==>C02-1146!=<context citStr="Tsang et al., 2002" endWordPosition="750" position="4844" startWordPosition="747">tic detection of such argument alternations is important to acquisition of verb lexical semantics (Dang et al., 2000; Dorr and Jones, 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002; Tsang et al., 2002), and moreover, may play a role in automatic processing of language for applied tasks, such as question-answering (Katz et al., 2001), information extraction (Riloff and Schmelzenbach, 1998), detectio</context>
E03-1037==>P98-1112!=<context citStr="Klavans and Kan, 1998" endWordPosition="166" position="1146" startWordPosition="162">verb information represents the core in supporting NLP-tasks such as word sense disambiguation (Dorr and Jones, 1996; Prescher et al., 2000), machine translation (Don, 1997), document classification (Klavans and Kan, 1998), and subcategorisation acquisition and filtering (Korhonen, 2002). A means to generalise over and predict common properties of verbs is captured by the constitution of verb classes. Levin (1993) has </context>
P06-2097==>C02-1122!=<context citStr="Kawahara and Kurohashi, 2002" endWordPosition="2367" position="15213" startWordPosition="2364">a surface form of verb but a case frame, which is assigned by case analysis. Case frames are automatically constructed from Web cooking texts (12 million sentences) by clustering similar verb usages (Kawahara and Kurohashi, 2002). An example of the automatically constructed case frame is shown in Table 3. For example,  (add salt) is assigned to ireru:1 (add) and  (carve with a knife) is assigned to case frame ireru:2 (car</context>
W06-3601==>N03-1019!=<context citStr="Kumar and Byrne, 2003" endWordPosition="1256" position="7711" startWordPosition="1252">rings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting s</context>
W95-0112==>J93-2004!=<context citStr="Marcus et al., 1993" endWordPosition="281" position="2045" startWordPosition="278">d with domain-specific tags, in contrast to general-purpose annotations such as part-of-speech tags or noun-phrase bracketing (e.g., the Brown Corpus [Francis and Kucera, 1982] and the Penn Treebank [Marcus et al., 1993]). Consequently, a new training corpus must be annotated for each domain. We have begun to explore the possibility of using an untagged corpus to automatically acquire conceptual patterns for informa</context>
H93-1018==>H90-1004!=<context citStr="[2, 3, 7, 8]" endWordPosition="1760" position="10808" startWordPosition="1757">the use of techniques similar to the beam search. Since the development of the exact algorithm, there have been several approximations developed that are much faster, with varying degrees of accuracy [2, 3, 7, 8]. The most recent algorithm [9] empirically retains the accuracy of the exact algorithm, while requiring little more computation than that of a simple 1-best search. The N-best Paradigm has the potent</context>
H93-1018==>H90-1004!=<context citStr="[8]" endWordPosition="2255" position="13734" startWordPosition="2255">used in the forward and backward directions were identical. So the estimate of the backward scores provided by the forward pass were exact. This method has also been used in a best-first stack search [8], in which it is very effective, since the forward-backward score for any theory covers the whole utterance. The forwardbackward score solves the primary problem with the bestfirst search, which is th</context>
H05-1098==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="164" position="1189" startWordPosition="161">al structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or phrases,  f1     fJ, and the process of translation involves selecting target phra</context>
H05-1098==>N03-1017!=<context citStr="Koehn et al. (2003)" endWordPosition="734" position="4733" startWordPosition="731">hn, and others: GIZA++ (Och and Ney, 2000) is used to obtain one-to-many word alignments in both directions, which are combined into a single set of refined alignments using the final-and method of Koehn et al. (2003); then those pairs of substrings that are exclusively aligned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: e</context>
H05-1098==>N03-1017!=<context citStr="Koehn et al., 2003" endWordPosition="958" position="5946" startWordPosition="955">where the i are features defined on rules. The basic model uses the following features, analogous to Pharaohs default feature set:  P( |) and P( |)  the lexical weights Pw( |) and Pw( |) (Koehn et al., 2003);1  a phrase penalty exp(1);  a word penalty exp(l), where l is the number of terminals in . The exceptions to the above are the two glue rules, which are the rules with left-hand side S in Figur</context>
H05-1098==>N03-1017!=<context citStr="Koehn et al. (2003)" endWordPosition="1126" position="6934" startWordPosition="1123"> rules formed out of an initial phrase. This distribution is then used to estimate the phrase translation probabilities. The lexical-weighting features are estimated using a method similar to that of Koehn et al. (2003). The language model is a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained using the SRI-LM toolkit (Stolcke, 2002). 1This feature uses word alignment information, wh</context>
E91-1031==>J89-4001!=<context citStr="Haas, 1989" endWordPosition="544" position="3380" startWordPosition="543">list and unification-based framework. CHART PARSING OF UNIFICATION GRAMMAR (UG). Parsing methods for context-free grammar can be extended to unification-based grammar formalisms (see Shieber, 1985 or Haas, 1989), and therefore they can in principle be used to parse CUG. A chart-parser scans a sentence from left to right, while entering items, representing (partial) derivations, in a chart. Assume that items </context>
E91-1031==>J89-4001!=<context citStr="Haas, 1989" endWordPosition="2904" position="17619" startWordPosition="2903">nto account as well). This multiplication may lead to a needlessly large left-corner table, which, if used in the prediction step, may in fact lead to sharp decreases in parsing performance (see also Haas, 1989, who encountered similar problems). Note that checking a left-corner table containing feature-structures is in general expensive, as unification, rather than identity-tests, have to be carried out. T</context>
E91-1031==>J89-4001!=<context citStr="Haas (1989)" endWordPosition="3522" position="21251" startWordPosition="3521">hart items, although the difference is less marked than in the case of German. In terms of parse times the two algorithms are almost equivalent. Comparing our results with those of Shieber (1985) and Haas (1989), we see that in all cases top-down filtering may reduce the size of the chart significantly. Whereas Haas (1989) found that top-down filtering never helps to actually decrease parse times in a bottom</context>
W04-1213==>W04-1221!=<context citStr="Settles, 2004" endWordPosition="1899" position="11696" startWordPosition="1898">. HMMs were employed by one system in isolation (Zhao, 2004) and by two systems in combination with SVMs (Zhou and Su, 2004; R ossler, 2004). Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al., 2004). It is somewhat surprising that Maximum Entropy Models were applied by only one system (Finkel et al., 2004), while it was the most </context>
W04-1213==>W04-1221!=<context citStr="Settles, 2004" endWordPosition="2267" position="13925" startWordPosition="2266">clearly examined. The top-ranked two systems incorporated information from gazetteers and employed abbreviation handling mechanisms, which were reported to give good effect. However, one participant (Settles, 2004) reported that their attempt to utilize gazetteers (together with other resources) had failed in gaining better overall performance. To overcome the shortage of training materials, several systems att</context>
W04-1213==>W04-1221!=<context citStr="Settles (2004)" endWordPosition="3279" position="19070" startWordPosition="3278">kel et al. (2004) explicitly pointed out the problem of abusing positive information with regard to using gazetteers, and utilized frequency information from BNC corpus to prevent such abusement. Settles (2004)s CRF system deserves special note in the sense that it achieved comparable performance to top ranked systems with a rather simple feature set. This fact may suggest that integration of information i</context>
W00-0207==>A97-1021!=<context citStr="Dorr, 1997" endWordPosition="270" position="1841" startWordPosition="269">tructural idiosyncrasies (Jackendoff, 1983; Jackendoff, 1990; Jackendoff, 1996). This representation has been used as the interlingua of several projects such as UN1TRAN (Dorr et at., 1993) and MILT (Dorr, 1997). An LCS is &amp;directed graph with a root. Each node is associated with certain information, including a type, a primitive and a field. The type of an LOS node is one of Event, State, Path, Manner, Prop</context>
J03-4003==>H91-1060!=<context citStr="Miller et al. (2000)" endWordPosition="17591" position="105015" startWordPosition="17588">k (2001) gives measurements of perplexity for a lexicalized PCFG. Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing. Miller et al. (2000) develop generative, lexicalized models for information extraction of relations. The approach enhances nonterminals in the parse trees to carry semantic labels and develops a probabilistic model that </context>
M92-1036==>H90-1005!=<context citStr="[3]" endWordPosition="366" position="2331" startWordPosition="366">, or New York University. They were not doing a lot of processing. They were doing the right processing. The second source of inspiration was Pereira's work on finite-state approximations of grammars [3], especially the speed of the implementation. Speed was the third source. It was simply too embarassing to have to report at the 1MUC-3 conference that it took TACITUS 36 hours to process 100 messages</context>
P01-1022==>H94-1010!=<context citStr="Hu et al. 1996" endWordPosition="217" position="1481" startWordPosition="214">he more varied the language that must be recognized, the more critical good language modeling becomes. Research in language modeling has heavily favored statistical approaches (Cohen 1995, Ward 1995, Hu et al. 1996, Iyer and Ostendorf 1997, Bellegarda 1999, Stolcke and Shriberg 1996) while handcoded finite-state or context-free language models dominate the commercial sector (Nuance 2001, SpeechWorks 2001, TellM</context>
P98-2184==>J93-2004!=<context citStr="Marcus et al. 1993" endWordPosition="537" position="3705" startWordPosition="534">tween various corpora and to find the causes of these differences, we analyzed 1122 psychological sentence production data (Connine et al. 1984), written discourse (Brown and WSJ from Penn Treebank - Marcus et al. 1993), and conversational data (Switchboard - Godfrey et al. 1992). We found that the subcategorization frequencies in each of these sources are different. We performed three experiments to (1) find the ca</context>
P95-1017==>P87-1022!=<context citStr="Brennan et al., 1987" endWordPosition="259" position="1785" startWordPosition="256">xists a perfect theory, it might not work well with noisy input, or it would not cover all the anaphoric phenomena. 'Walker (Walker, 1989) compares Brennan, Friedman and Pollard's centering approach (Brennan et al., 1987) with Hobbs' algorithm (Hobbs, 1976) on a theoretical basis. These requirements have motivated us to develop robust, extensible, and trainable anaphora resolution systems. Previously (Aone and McKee, </context>
W06-0705==>N03-1022!=<context citStr="Moldovan et al., 2003" endWordPosition="457" position="2939" startWordPosition="454"> in order to produce fine-grained, contextsensitive inferences that could be used to answer questions. Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al., 2003). Under this approach, information extracted from the scenario narrative is converted to logical axioms that can used in conjunction with a logic prover in order justify answers returned for questions</context>
P01-1010==>W97-0302!=<context citStr="Goodman (1997)" endWordPosition="1484" position="8847" startWordPosition="1483">p track of all subderivations at each edge in the chart (at least for such a large corpus as the WSJ). As in most other statistical parsing systems we therefore use the pruning technique described in Goodman (1997) and Collins (1999: 263-264) which assigns a score to each item in the chart equal to the product of the inside probability of the item and its prior probability. Any item with a score less than 105 </context>
W97-0201==>H93-1051!=<context citStr="Leacock et al., 1993" endWordPosition="1336" position="8119" startWordPosition="1333">in practical NLP tasks, and that resolving senses to the refined level of WORD NET is a worthwhile task to pursue. 3 The Effect of Training Corpus Size A number of past research work on WSD, such as (Leacock et al., 1993; Bruce and Wiebe, 1994; Mooney, 1996), were tested on a small number of words like &amp;quot;line&amp;quot; and &amp;quot;interest&amp;quot;. Similarly, (Yarowsky, 1995) tested his WSD algorithm on a dozen words. The sense-tagged corpu</context>
E91-1008==>P90-1014!=<context citStr="Fong (1990)" endWordPosition="844" position="5317" startWordPosition="843">d a device (actually, a Turing machine) assigning the same index to multiple occurences of the same R-expression (names); furthermore, a set of disjoint indices is associated with each item. Finally, Fong (1990) performs a combinatorial analysis of the paradigm of free indexation, as proposed in (Chomsky, 1981); he shows that free indexation gives rise to an exponential number of alternatives and argues for </context>
H92-1026==>H90-1055!=<context citStr="(3)" endWordPosition="1709" position="10320" startWordPosition="1709">ased unification grammar. The grammar is context-free but uses unification to express rule templates for the the context-free productions. For example, the rule template: [N P Det irNi :n unspec :n j (3) corresponds to three CFG productions where the second feature : n is either s, p, or : n. This rule template may elicit up to 7 non-terminals. The grammar has 21 features whose range of values maybe </context>
H92-1026==>H90-1055!=<context citStr="(3)" endWordPosition="2830" position="16846" startWordPosition="2830">14 c Syn) While a different order for these predictions is possible, we only experimented with this one. 6.1. Parameter Estimation We only have built a decision tree to the rule probability component (3) of the model. For the moment, we are using n-gram models with the usual deleted interpolation for smoothing for the other four components of the model. We have assigned bit strings to the syntactic a</context>
W05-0817==>J93-2003!=<context citStr="Al-Onaizan et al., 1999" endWordPosition="288" position="1895" startWordPosition="285">described in the (Brown et al. 1993) seminal paper. The first wide-spread and publicly available implementation of the IBM models was the GIZA program, which itself was part of the SMT toolkit EGYPT (Al-Onaizan et al., 1999). GIZA has been superseded by its recent extension GIZA++ (Och and Ney, 2000, 2003) publicly available2. We used the translation probabilities generated by GIZA++ for implementing a second aligner, ME</context>
J94-3010==>C90-3009!=<context citStr="Cahill 1990" endWordPosition="976" position="6806" startWordPosition="975">te natural (cf. Johnson [19881). Moreover, some recent thinking on the phonologyphonetics interface supports this view (Pierrehumbert 1990; Coleman 2 (Bach and Wheeler 1981; Wheeler 1981; Bird 1990; Cahill 1990; Coleman 1991; Scobbie 1991; Bird 1992; Walther 1992; Mastroianni 1993; Russell 1993) 456 Steven Bird and Ewan Klein Phonological Analysis in Typed Feature Systems 1992). However, it represents a fun</context>
W05-0606==>P98-1043!=<context citStr="Covington (1998)" endWordPosition="1592" position="9843" startWordPosition="1591">he modified model is shown in Figure 3. First, the original models assumption that an insertion followed by a deletion is the same as a substitution is problematic in the context of word similarity. Covington (1998) illustrates the problem with an example of Italian due and the Spanish dos, both of which mean two. While there is no doubt that the first two pairs of symbols should be aligned, there is no hi</context>
P98-2140==>J93-2004!=<context citStr="Marcus et al., 1993" endWordPosition="3597" position="20690" startWordPosition="3594">tter&amp;quot; approach works well for simple texts but is rather unreliable for texts with many proper names and abbreviations at the end of sentence as, for instance, the Wall Street Journal (WSJ) corpus ( (Marcus et al., 1993) ). One well-known trainable systems - SATZ - is described in (Palmer&amp;Hearst, 1997). It uses a neural network with two layers of hidden units. It was trained on the most probable parts-of-speech of th</context>
W05-0618==>W94-0319!=<context citStr="Reiter (1994)" endWordPosition="249" position="1649" startWordPosition="248">a generic pipeline architecture is GATE (Cunningham et al., 1997) which provides an infrastructure for building NLP applications. Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter &amp; Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking 136 and named entity recognition (e.g. Buchholz et al. (1999), </context>
P91-1001==>P88-1003!=<context citStr="[17]" endWordPosition="45" position="338" startWordPosition="45">X 78759 aone@mcc.com Abstract I present a semantic analysis of collectivedistributive ambiguity, and resolution of such ambiguity by model-based reasoning. This approach goes beyond Scha and Stallard [17], whose reasoning capability was limited to checking semantic types. My semantic analysis is based on Link [14, 13] and Roberts [15], where distributivity comes uniformly from a quantificational opera</context>
P91-1001==>P88-1003!=<context citStr="[17]" endWordPosition="643" position="4054" startWordPosition="643">amount x 5) (measure x CU) x' (i-part x' x) (pizza y) (amount y 4) (measure y slice) Y' (i-part y' y) =-- (eat e x' y') necessary knowledge and develop a reasoner, which goes beyond Scha and Stallard [17]. There is a special reading called a cumulative reading (cf. Scha [16]). (3) 500 students ate 1200 slices of pizza. The cumulative reading of (3) says &amp;quot;there were 500 students and each student ate so</context>
P91-1001==>P88-1003!=<context citStr="[17]" endWordPosition="3879" position="23306" startWordPosition="3879"> PhD thesis, The University of Texas at Austin, 1991. References 3 Conclusion The work described in this paper improves upon previous works on collective-distributive ambiguity (cf. Scha and Stallard [17], Gardiner et al. [7]), since they do not fully explore the necessary reasoning. I believe that the reasoning method described in this paper is general enough to solve collectivedistributive problems </context>
W05-0805==>C02-2022!=<context citStr="Cahill and Tiberius (2002)" endWordPosition="723" position="4863" startWordPosition="720">d 8, we discuss our results and draw some final conclusions. 2 Previous Research Some approaches to revealing sound correspondences require clean data whereas other methods can deal with noisy input. Cahill and Tiberius (2002) use a manually compiled cognate list of Dutch, English and German cognates and extract crosslinguistic phoneme correspondences. The results1 contain the counts of a certain German phoneme and their p</context>
W05-0805==>C02-2022!=<context citStr="Cahill and Tiberius (2002)" endWordPosition="4412" position="26858" startWordPosition="4409">s with a high probability whereas unlikely sound correspondences receive lower probabilities. Our approach differs from other approaches either in the method used or in the different linguistic task. Cahill and Tiberius (2002) is based on mere counts of phoneme correspondences; Kondrak (2003) generates Algonquian phoneme correspondences which are possible according to his translation models; Kondrak (2004) measures if two </context>
W05-0613==>J92-4007!=<context citStr="Moore and Pollack, 1992" endWordPosition="583" position="3823" startWordPosition="580">parsing. However, the results are trees of Rhetorical Structure Theory (RST) (Mann and Thompson, 1986), and the classifiers rely on well-formedness constraints on RST trees which are too restrictive (Moore and Pollack, 1992). Furthermore, RST does not offer an account of how compositional semantics gets augmented, nor does it model anaphora. It is also designed for monologue rather than dialogue, so it does not offer a p</context>
P98-1010==>H92-1022!=<context citStr="Brill (1992)" endWordPosition="3940" position="22921" startWordPosition="3939">mber of words in the training data, for particular parameter settings. 4 Related Work Two previous methods for learning local syntactic patterns follow the transformation-based paradigm introduced by Brill (1992). Vilain and Day (1996) identify (and classify) name phrases such as company names, locations, etc. Ramshaw and Marcus (1995) detect noun phrases, by classifying each word as being inside a phrase, ou</context>
H94-1014==>H94-1013!=<context citStr="[1,9]" endWordPosition="823" position="5012" startWordPosition="823">pproach has the advantage that it can be used either as a static or a dynamic model, and can easily leverage the techniques that have been developed for adaptive language modeling, particularly cache [1,9] and trigger [2, 3] models. One might raise the issue of recognition search cost for a model of mixtures at the sentence level, but in the N-best rescoring framework [10] the additional cost of the mi</context>
H94-1014==>H94-1013!=<context citStr="[1, 9]" endWordPosition="3434" position="21024" startWordPosition="3433"> weights can be adapted according to the likelihood of the respective mixture components in the previous utterance, .as in [8] for n-gram level mixture weights. Second, the dynamic n-gram cache model [1, 9] can easily be incorporated into the mixture language model. However, in the mixture model, it is possible to have component-dependent cache models, where each component cache would be updated after e</context>
W01-1308==>C90-3053!=<context citStr="Zelinsky-Wibbelt, 1990" endWordPosition="1288" position="8328" startWordPosition="1287">ositional concepts have less internal relations between attributes than nominal concepts. Instead verbs, prepositions and conjunctions have external relations to the parts of speech they interrelate (Zelinsky-Wibbelt, 1990). The extensional variations of verbal concepts explain that they are harder to learn, to remember, to produce and to comprehend. The contextdependent concepts of verbs, adjectives, prepositions, and</context>
W05-1509==>C00-2137!=<context citStr="Yeh, 2000" endWordPosition="4198" position="25274" startWordPosition="4197"> the one augmented both with finer-grained tags and representations of syntactic locality perform better than our comparison baseline H03, but only the latter is significantly better (p &lt; .01, using (Yeh, 2000)s randomised test). This indicates that while information projected from the lexical items is very important, only a combination of lexical semantics information and careful modelling of syntactic do</context>
J05-4002==>J98-2002!=<context citStr="Li and Abe 1998" endWordPosition="1293" position="8578" startWordPosition="1290">rb, the object, the preposition, and the prepositional object). However, a statistical model built on the basis of four lexical events must cope with extremely sparse data. One approach (Resnik 1993; Li and Abe 1998; Clark and Weir 2000) is to induce probability distributions over semantic classes rather than lexical items. For example, a cottage is a type of building and a brother is a type of person, and so th</context>
P05-1021==>P02-1060!=<context citStr="Zhou and Su, 2002" endWordPosition="2959" position="18807" startWordPosition="2956">., 2000). The recognition of NEs as well as their semantic categories was done by a HMM based NER, which was trained for the MUC NE task and obtained high F-scores of 96.9% (MUC-6) and 94.3% (MUC-7) (Zhou and Su, 2002). For each anaphor, the markables occurring within the current and previous two sentences were taken as the initial candidates. Those with mismatched number and gender agreements were filtered from th</context>
P98-2207==>C96-2154!=<context citStr="Sekine, 1996" endWordPosition="377" position="2441" startWordPosition="376">to segment each sentence automatically. Sekine proposed a method for selecting a suitable sentence from sentences which were extracted by a speech recognition system using statistical language model (Sekine, 1996). However, if the statistical model is used for extraction of sentence candidates, we will obtain higher recognition accuracy. Some initial studies of transcription of broadcast news proceed (Bakis et</context>
P03-1023==>M95-1005!=<context citStr="Vilain et al. 1995" endWordPosition="4144" position="24952" startWordPosition="4141"> 2 and 3, here we focus on whether a coreferential chain could be correctly identified. For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program (Vilain et al. 1995) for the coreference resolution task. Here the recall means the correct resolved chains over the whole coreferential chains in the data set, and precision means the correct resolved chains over the wh</context>
W06-1670==>A00-1031!=<context citStr="Brants, 2002" endWordPosition="3024" position="19068" startWordPosition="3023">rphological functions of the Wordnet library. The first sense feature (2) is the label predicted for xi by the baseline model, cf. Section 5.3. POS labels (4) were generated using Brants TnT tagger (Brants, 2002). POS features of the form pos;[0] extract the first character from the POS label, thus providing a simplified representation of the POS tag. Finally, word shape features (5) are regular expression-li</context>
W03-0910==>A00-2034!=<context citStr="McCarthy, 2000" endWordPosition="651" position="3992" startWordPosition="650"> `commerce' and `judgment'. Various attempts have been made to automatically cluster verbs into semantically meaningful classes, using the Levin class as a gold standard for evaluation (Gildea, 2002; McCarthy, 2000; Merlo and Stevenson, 2001; Schulte im Walde, 2000). In the next two sections, we provide background on VerbNet and PropBank which play central roles in the cluster methodology presented here. 2.1 Ve</context>
W04-1216==>W03-1305!=<context citStr="Lee et al 2003" endWordPosition="210" position="1420" startWordPosition="207">ms from MUC to the biomedical domain, such as (Fukuda et al 1998), (Proux et al 1998), (Nobata et al 2000), (Collier et al 2000), (Gaizauskas et al 2000), (Kazama et al 2002), (Takeuchi et al 2002), (Lee et al 2003) and (Zhou et al 2004). As opposed to rule-based systems, machine learning-based systems could train their models on labeled data. But due to the irregular forms of biomedical texts, people still need</context>
C94-2112==>C88-2110!=<context citStr="[22, 3]" endWordPosition="175" position="1203" startWordPosition="174">global coherence in the structure of the language lexicon, some researchers have moved towards more expressive semantic descriptions ([16, 1, 5, 10]), as well as more powerful methods of composition ([22, 3]). Some, however, have expressed reservations as to the general applicability of type-changing operations such as coercion, as well as the notion of a generative lexicon itself ([7]). In this paper, w</context>
W01-1203==>C00-1043!=<context citStr="[30]" endWordPosition="2805" position="18114" startWordPosition="2805">syntactic properties of a sentence are often very language specific and therefore dont map well to another language. Parse trees [1] and [12] are examples of our systems structure, whereas [18] and [30] represent the same question/answer pair in the more syntactically oriented structure of the Penn treebank5 (Marcus 1993). Question and answer in CONTEX format: [1] When was the Berlin Wall opened? [S</context>
W06-1413==>P02-1013!=<context citStr="Gardent, 2002" endWordPosition="132" position="924" startWordPosition="131">ght be amenable to this treatment. 1 Introduction Until recently, GRE algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible (e.g. (Dale, 1992; Gardent, 2002)) or almost as short as possible (e.g. (Dale and Reiter, 1995)). Since reductions in ambiguity are achieved by increases in length, there is a tension between these factors, and algorithms usually res</context>
W00-1403==>A00-2002!=<context citStr="Marcu et al. (2000)" endWordPosition="3965" position="24650" startWordPosition="3962">. When generating texts in language 0, the MGEN system generates a text plan in Lan: :guage.B, maps ikirktalanguage.0.,, and .then :proceeds further with the sentence planning and realization stages. Marcu et al. (2000) present and evaluate a discourse-tree rewriting algorithm that exploits machine learning methods in order to map Japanese discourse trees into discourse trees that resemble English-specific rendering</context>
W95-0105==>P91-1047!=<context citStr="[0, 1]" endWordPosition="966" position="6403" startWordPosition="965">ld conclude belong to the group of senses corresponding to the word grouping W. The goal is then to define a membership function co that takes si,j, wi, and W as its arguments and computes a value in [0, 1], representing the confidence with which one can state that sense si,3 belongs in sense grouping W'.4 Note that, in principle, nothing precludes the possibility that multiple senses of a word are incl</context>
C02-1077==>J97-4005!=<context citStr="Abney, 1997" endWordPosition="3047" position="17099" startWordPosition="3046"> but it is certainly a plausible one. 4 Log-linear Modelling of Attribute-Value Grammars Here we show how attribute-value grammars may be modelled using log-linear models. Abney gives fuller details (Abney, 1997). Let G be an attribute-value grammar, and D a set of sentences within the string-set defined by L(G). A log-linear model, M, consist of two components: a set of features, F and a set of weights, A. T</context>
W97-0309==>P94-1038!=<context citStr="Dagan, Pereira, and Lee, 1994" endWordPosition="4492" position="26000" startWordPosition="4488">probability mass is then filled in by the backoff model. We compared this to a trigram model that backed off to the m = 2 model in Table 6. This was handled by a slight variant of the Katz procedure (Dagan, Pereira, and Lee, 1994) in which the mixedorder model substituted for the backoff model. One advantage of this smoothing procedure is that it is straightforward to assess the performance of different backoff models. Becaus</context>
W96-0306==>J93-2002!=<context citStr="Brent, 1993" endWordPosition="357" position="2498" startWordPosition="356">tical techniques, such as bilingual alignment (Church and Hanks, 1990; Klavans and Tzoukermann, 1996; Wu and Xia, 1995), or extraction of syntactic constructions from online dictionaries and corpora (Brent, 1993; Dorr, Garman, and Weinberg, 1995). Others who have taken a more knowledge-based (interlingual) approach (Lonsdale, Mitamura, and Nyberg, 1996) do not provide a means for systematically deriving the </context>
P00-1053==>P98-1011!=<context citStr="Azzam, et al., 1998" endWordPosition="1573" position="9162" startWordPosition="1570">our units, the linear model was equally effective. Here, we compare VT to stack-based models of discourse structure based on Grosz and Sidner's (1986) (G&amp;S) focus spaces (e.g., Hahn and Strube, 1997; Azzam, et al., 1998). In these approaches, discourse segments are pushed on the stack as they are encountered in a linear traversal of the text. Before a dominating segment is pushed, subordinate segments that precede it</context>
W05-0832==>C04-1072!=<context citStr="Lin and Och, 2004" endWordPosition="2661" position="16170" startWordPosition="2658"> this experiment. Naturally, BLEU (Papineni et al., 2001) was the first choice metric, as it was well-matched to the target language evaluation function. ROUGE was a reimplementation of ROUGE-L from (Lin and Och, 2004). It computes an F-measure from precision and recall that are both based on the longest common subsequence of the hypothesis and reference strings. WER-G is a variation on traditional word error rate </context>
J05-1004==>P98-1046!=<context citStr="Dang et al. 1998" endWordPosition="1650" position="10904" startWordPosition="1647">many cases, the additional information that VerbNet provides for each class has caused it to subdivide, or use intersections of, Levins original classes, adding an additional level to the hierarchy (Dang et al. 1998). We are also extending the coverage by adding new classes (Korhonen and Briscoe 2004). Our objective with the Proposition Bank is not a theoretical account of how and why syntactic alternation takes </context>
P04-1023==>J03-1002!=<context citStr="Och and Ney, 2003" endWordPosition="1608" position="10396" startWordPosition="1605">ion of the corpus. 4 Experimental Design To perform our experiments with word-level alignements we modified GIZA++, an existing and freely available implementation of the IBM models and HMM variants (Och and Ney, 2003). Our modifications involved circumventing the E-step for sentences which had word-level alignments and incorporating these observed alignment statistics in the M-step. The observed and expected stati</context>
P04-1023==>J03-1002!=<context citStr="Och and Ney (2003)" endWordPosition="1696" position="10940" startWordPosition="1693">r to measure the accuracy of the predictions that the statistical translation models make under our various experimental settings, we choose the alignment error rate (AER) metric, which is defined in Och and Ney (2003). We also investigated whether improved AER leads to improved translation quality. We used the alignments created during our AER experiments as the input to a phrase-based decoder. We translated a tes</context>
P04-1023==>J03-1002!=<context citStr="Och and Ney (2003)" endWordPosition="2560" position="16292" startWordPosition="2557">rocess. For this experiment we allowed a bilingual dictionary to constrain which words can act as translations of each other during the initial estimates of translation probabilities (as described in Och and Ney (2003)). As can be seen in Table 3, using a dictionary reduces the AER when compared to using GIZA++ without a dictionary, but not as dramatically as integrating the word-alignments. We further tried combin</context>
P04-1023==>J03-1002!=<context citStr="Och and Ney (2003)" endWordPosition="3823" position="23742" startWordPosition="3820"> corpus). Nevertheless, the trend of decreased AER and increased Bleu score still holds. For each size of training corpus we tested we found better results using the word-aligned data. 6 Related Work Och and Ney (2003) is the most extensive analysis to date of how many different factors contribute towards improved alignments error rates, but the inclusion of word-alignments is not considered. Och and Ney do not giv</context>
P03-2013==>P98-1085!=<context citStr="Heine, 1998" endWordPosition="3503" position="22076" startWordPosition="3502">fying several different uses of definite descriptions (Vieira and Poesio, 2000; Bean and Riloff, 1999) is somewhat analogous to that for bare nouns. Determining definiteness of Japanese noun phrases (Heine, 1998; Bond et al., 1995; Murata and Nagao, 1993)8 is also relevant to ATN (which is definite in nature) recognition. 6 Future Directions We have proposed an ATN (hence zero adnominal) recognition algorith</context>
E03-1076==>P02-1040!=<context citStr="Papineni et al., 2002" endWordPosition="2865" position="17297" startWordPosition="2862"> NP/PP test set as in the previous section. Training and testing data was split consistently in the same way. The translation accuracy is measured against reference translations using the BLEU score [Papineni et al., 2002]. Table 2 displays the results. Somewhat surprisingly, the frequency based method leads to better translation quality than the more accurate methods that take advantage from knowledge from the parall</context>
H05-1112==>W04-2609!=<context citStr="Moldovan et al., 2004" endWordPosition="1089" position="6979" startWordPosition="1086">training and 20% for testing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al., 2004) and shown in Table 1. The genitives noun components were manually disambiguated with the corresponding WordNet 2.0 senses or the named entities if they are not in WordNet (e.g. names of persons, nam</context>
P98-2227==>J90-1004!=NO CONTEXT
C90-2024==>P88-1031!=<context citStr="[2]" endWordPosition="1038" position="6250" startWordPosition="1038">ame formalism for the representation of syntactic and semantic knowledge. 3. Type hierarchies and x+'-terms We have chosen to represent knowledge about words and trees with a unique formalism : terms [2]. 4J-terms are case frame structures which permit the description of types (in the sense of classical programming languages such as Pascal), i.e. sets of values. 41-terms are directed graphs (Figure 2</context>
C90-2024==>P88-1031!=<context citStr="[1, 2]" endWordPosition="1144" position="6895" startWordPosition="1143">ared. Simple types are defined in the signature which is a set partially ordered by the isa relation. This order is extended to 'P-terms by the unique operation used to manipulate them : unification [1, 2]. The unification of two simple types is 140 2 defined as the set of lower bounds of these two types (in the iss-a relation). describing respectively two sets ei and e2 of values, then unification of </context>
C00-2099==>P95-1037!=<context citStr="Magerman, 1995" endWordPosition="4461" position="25833" startWordPosition="4460">far, using, e.g., maximum-entropy or decision-tree models to extract relevant features of it; there is no difference in principle between our model and history-based parsing, see (Black et al., 1993; Magerman, 1995). The proposed treatment of string realization through the use of the S and M variables is also both truly novel and important. While phrase-structure grammars overemphasize word order by making the p</context>
W05-1006==>W03-0415!=<context citStr="Cederberg and Widdows (2003)" endWordPosition="998" position="6234" startWordPosition="995">racted for any member of this class, it could be applied to all members of the class. This technique can often mistakenly reason across an ambiguous middle-term, a situation that was improved upon by Cederberg and Widdows (2003), by combining pattern-based extraction with contextual filtering using latent semantic analysis. Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et </context>
W05-1006==>W03-0415!=<context citStr="Cederberg and Widdows (2003)" endWordPosition="3091" position="19027" startWordPosition="3088">ee of syntactic analysis would improve this situation. For extracting semantic relationships, Figure 5: Directed graph showing that life-events are usually ordered temporally when they occur together Cederberg and Widdows (2003) demonstrated that nounphrase chunking does this work very satisfactorily, while being much more tractable than full parsing. The mistaken pair middle and class shown in Table 1 is another of these mi</context>
W05-1006==>W03-0415!=<context citStr="Cederberg and Widdows, 2003" endWordPosition="3468" position="21383" startWordPosition="3465">s relationship is used in contexts that are typical uses of the words in question, or whether these uses appear to be anomalies such as rare senses or idioms. This technique was used successfully by (Cederberg and Widdows, 2003) to improve the accuracy of hyponymy extraction. It follows that it should be useful to tell the difference between regularly related words and idiomatically related words. To test this hypothesis, we</context>
